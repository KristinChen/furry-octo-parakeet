{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\thefuzz\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from dateutil import parser\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import glob\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tag import pos_tag\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "import spacy #numpy needs to be <1.20\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import datefinder\n",
    "from datetime import datetime\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thefuzz in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (0.19.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rtd91\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datefinder\n",
      "  Downloading datefinder-0.7.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from datefinder) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.4.2 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from datefinder) (2.8.1)\n",
      "Requirement already satisfied: regex>=2017.02.08 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from datefinder) (2022.3.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from python-dateutil>=2.4.2->datefinder) (1.12.0)\n",
      "Installing collected packages: datefinder\n",
      "Successfully installed datefinder-0.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rtd91\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721091408_phonescreening.pdf\n",
      "Bachelor of Engineering in Electronics and Communication Engineering\n",
      "821101104_phonescreening.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:402: DeprecationWarning: Flags not at the start of the expression '(?i)(school|college|' (truncated)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bachelor of Technology in Mechanical Engineering, Delhi, India       \n",
      "821101136_phonescreening.pdf\n",
      "Bachelor of Technology, Instrumentation and Control Engineering\n",
      "821110120.pdf\n",
      "Bachelors in Technology, Chemical Engineering with a minor in Pollution Control Engineering\n",
      "821110440_Mythri.pdf\n",
      "B S  Chemistry   Minor  Mathematics\n",
      "821110445_Qizhe.pdf\n",
      "821110447_rachan.pdf\n",
      "Bachelor of Technology in Mechanical Engineering    \n",
      "821110448_RAGHUVEERA.pdf\n",
      "Bachelor of Technology, Electronics and Communication Engineering, Vellore Institute of Technology\n",
      "821110464_Saida.pdf\n",
      "MS in Data Science Graduation     \n",
      "BS in Mathematics Graduation     \n",
      "821110509_Akash.pdf\n",
      "Bachelor of Technology in Chemical Engineering;             \n",
      "821110510_AKSHARA.pdf\n",
      "Bachelor of Technology in Computer Science,             \n",
      "821110511_Alexandra.pdf\n",
      "Bachelor of Science, Finance     –    \n",
      "821110527_.pdf\n",
      "M S , Information Technology and Management\n",
      "821110528_Wanting.pdf\n",
      "Bachelor of Arts in Applied Mathematics and Data Science          \n",
      "821111539_manaswini.pdf\n",
      "Bachelor of Technology in Computer Science Engineering\n",
      "821112919_mohit.pdf\n",
      "Bachelors of Management Studies (concentration in marketing)\n",
      "821112928_yechen.pdf\n",
      "B S  in Applied Mathematics (Specialization in Computing)\n",
      "821122313_Dimple.pdf\n",
      "821122315_Indupriya.pdf\n",
      "Bachelor of Engineering Electronics and Communications        \n",
      "821122316_James.pdf\n",
      "Bachelor of Science in Industrial Engineering, summa cum laude (Equiv        )     –    \n",
      "822010605_anya.pdf\n",
      "MATLAB\n",
      "822010628_ryan.pdf\n",
      "M S  Data Science\n",
      "B S  Applied Physics\n",
      "822011416_Mayank.pdf\n",
      "MS in, Data Analytics Engineering\n",
      "Bachelor of Engineering, Computer Science\n",
      "822011417_Abhinay.pdf\n",
      "Bachelor of Engineering in Information Technology,          \n",
      "822013101_Noah.pdf\n",
      "BS in Mathematics, Minor in Philosophy\n",
      "AbhilashRajaram.pdf\n",
      "Bachelors in Computer Science and Engineering;           \n",
      "AdamDinser.pdf\n",
      "BA\n",
      "AdeolaAdesoba.pdf\n",
      "AjayPrasathManoharan.pdf\n",
      "Bachelor of Engineering in Mechanical Engineering\n",
      "AkshayKajale.pdf\n",
      "Bachelor of Engineering in Computer Engineering, University of Pune, India (          )\n",
      "Alexander Friend.pdf\n",
      "Bachelor of Science in Data Science\n",
      "AmarJha.pdf\n",
      "AmolAgrawal.pdf\n",
      "Bachelors of Science in Computer Science,      \n",
      "AndrewLin.pdf\n",
      "Bachelor of Management   Finance\n",
      "AnirudhSuram.pdf\n",
      "MS in Data Science   University of Massachusetts Dartmouth\n",
      "Bachelor's In Computer Science  CVR College of Engineering , Hyderabad, India\n",
      "AnnapoornaGopalakrishhna.pdf\n",
      "AnttonWillbanks.pdf\n",
      "MS Suite\n",
      "AnyaPatel.pdf\n",
      "MATLAB\n",
      "ArchitPrem.pdf\n",
      "Bachelor of Technology, Major  Computer Science\n",
      "AshviSoni.pdf\n",
      "Bachelor of Engineering (B E ) in Computer Engineering\n",
      "BaiyuChen.pdf\n",
      "Bachelor of Science   Applied Mathematics and Statistics\n",
      "BhavikaChavda.pdf\n",
      "Bachelor of Computer Application,         \n",
      "BokyeungKim.pdf\n",
      "Bachelor of Science in Business Administration   keting\n",
      "BonTrinh.pdf\n",
      "BukaCakrawala.pdf\n",
      "C01-21111807_Andrew_Decker.pdf\n",
      "Bachelor of Science in Mathematical Economics Minor Data Science, Mathematics Department       \n",
      "C01-21120201_Jiayue_Fei.pdf\n",
      "CahyariniHariga.pdf\n",
      "CarlosPerez.pdf\n",
      "CassinEdwin.pdf\n",
      "Bachelor of Science, Computer Science\n",
      "ChengwiChen.pdf\n",
      "Bachelor of Arts in Economics\n",
      "ChetnaKhanna.pdf\n",
      "MS, Data Analytics Engineering\n",
      "BS, Mathematics (Ramjas College)\n",
      "DhruvParikh.pdf\n",
      "MS in Electrical Engineering (Machine Learning, Data Science, Reinforcement Learning, Computer Vision)\n",
      "DhwaniGandhi.pdf\n",
      "Bachelor of Engineering in Electronics & Telecommunication, University of Mumbai, India             \n",
      "DimpleMehra.pdf\n",
      "DivyaManku.pdf\n",
      "Bachelors  Electronics and Communication Engineering\n",
      "ElijahHall.pdf\n",
      "Bachelor of Science in Applied Mathematics\n",
      "EvanJones.pdf\n",
      "EvanJoyce.pdf\n",
      "B S  Information Science\n",
      "GaliSabyr.pdf\n",
      "B S  IN SOFTWARE ENGINEERING AND COMPUTER NETWORKING (EXPECTED GRADUATION     ) OVERALL\n",
      "GrishmaParajuli.pdf\n",
      "Bachelor of Science in Mathematics\n",
      "Hamza Abdelghani.pdf\n",
      "MS in Data Science\n",
      "IndupriyaKompiS.pdf\n",
      "Bachelor of Engineering Electronics and Communications        \n",
      "JairoCarreon.pdf\n",
      "Bachelor of Science in Computer Science and Engineering\n",
      "JamesRenierDomingo.pdf\n",
      "Bachelor of Science in Industrial Engineering, summa cum laude (Equiv        )     –    \n",
      "JialuXia.pdf\n",
      "Bachelor of Engineering in Information Engineering, Minor in Statistics\n",
      "JianboGu.pdf\n",
      "Bachelor in Civil Engineering with Minor in Mathematics,        \n",
      "JiehongLiu.pdf\n",
      "JimLiu.pdf\n",
      "M S  in Statistics\n",
      "B S  in Risk Management and Insurance\n",
      "JuliaXia.pdf\n",
      "M S  in Operation Research Expected    \n",
      "B A  in Math & Economics  t   –    \n",
      "KRISTINJIATINGCHEN_RESUME_06082021 (1).pdf\n",
      "Bachelor of Science in Business Administration, concentration in  keting,       \n",
      "KunjMithapara.pdf\n",
      "Bachelor of Engineering in Mechanical Engineering  Gujarat Technological University, India     –    \n",
      "LejianHe.pdf\n",
      "Bachelor of Science in Atmospheric Sciences (Graduated with Honor),              \n",
      "ManaswiniNagaraj.pdf\n",
      "Bachelor of Technology in Computer Science Engineering\n",
      "Manisha Patel Resume.pdf\n",
      "Bachelor of Technology in Information Technology,         \n",
      "MasenBachleda.pdf\n",
      "Bachelor of Science  Mathematics and Statistics\n",
      "MatthewHolcombe.pdf\n",
      "Bachelor of Science in Mathematics, Option in Data Science\n",
      "MayuriLalwani.pdf\n",
      "MAYURI LALWANI\n",
      "MikhailTokarev.pdf\n",
      "Bachelor of Electrical,electronics engineering and Computer Science\n",
      "MistereAbate.pdf\n",
      "BACHELOR OF SCIENCE IN COMPUTER SCIENCE\n",
      "MohammadZarei.pdf\n",
      "Bachelor of Science in Electrical and Computer Engineering;       \n",
      "MonaEslamijam.pdf\n",
      "NahianSiddique.pdf\n",
      "M S  in Electrical and Computer Engineering\n",
      "B S  in Electrical Engineering\n",
      "NidhiChovatiya.pdf\n",
      "NiksonPanigrahi.pdf\n",
      "Bachelor of Engineering, Computer Science \n",
      "PratikSatpute.pdf\n",
      "PraveenPandey.pdf\n",
      "Bachelor’s Degree, Computer Engineering  Focus  Artificial Intelligence\n",
      "Resume_Adedamola_Olawoye.pdf\n",
      "Bachelor of Science,  keting Concentration, Cum Laude,   (   )\n",
      "Resume_Aditya_Deshpande.pdf\n",
      "Bachelor of Science, Computer Science\n",
      "Resume_Ali_Majidian.pdf\n",
      "Bachelor of Science in Civil Engineering   (all   units)      out of         out of  \n",
      "Resume_Ankit_Hemant_Lade.pdf\n",
      "Resume_Anqi_Cheng.pdf\n",
      "Bachelor of Science in Computer Science, Minor in Mathematics\n",
      "Resume_Arbaaz_Mohideen.pdf\n",
      "Resume_Arunava_Ray.pdf\n",
      "Bachelor of Engineering in Mechanical Engineering (        )\n",
      "Resume_Ashutosh_Shinde.pdf\n",
      "Bachelor of Technology in Computer Science\n",
      "Resume_Bah_Yannick_Konan.pdf\n",
      "Bachelor of Science in Finance\n",
      "Resume_Bala_Harimani.pdf\n",
      "M S , Data Science and Business Analytics\n",
      "Resume_Bonface_Njuguna.pdf\n",
      "Resume_Chaeeun_Lim.pdf\n",
      "Bachelor of Science, Biochemistry\n",
      "Resume_Chantelle_Lim.pdf\n",
      "Bachelor of Science in Biomedical Engineering, Concentration in Biosystems and Signals\n",
      "Resume_Chitranjan_Joshi.pdf\n",
      "Bachelor of Engineering, Computer Science\n",
      "Resume_Diego_Burgos.pdf\n",
      "Resume_Ethan_Laurenceau.pdf\n",
      "Bachelor of Science in Mathematics Major       \n",
      "Resume_Girija_Bandaru.pdf\n",
      "MS in Business Analytics and Information Systems,                  \n",
      "Bachelor of Technology in Electronics & Communications Engineering,                  \n",
      "Resume_Harshitha_Prasad.pdf\n",
      "Resume_Harsh_Pundir.pdf\n",
      "Bachelor of Technology in Mechanical Engineering, Delhi, India       \n",
      "Resume_Hemanth_Kodakandla.pdf\n",
      "M S  Applied Data Science\n",
      "Resume_Isioma_Ochia.pdf\n",
      "Bachelor of Science, Chemical Engineering, cum laude,    \n",
      "Resume_Jay_Pandya.pdf\n",
      "Bachelor of Engineering in Mechanical Engineering      \n",
      "Resume_Jessica_Rega.pdf\n",
      "Bachelor of Science in Applied Mathematics\n",
      "Resume_Jishan_Ahmed.pdf\n",
      "M A  in Mathematics\n",
      "Resume_Kaleb_Shikur.pdf\n",
      "BSc Financial Engineering       \n",
      "Resume_Kanth_Juvadi.pdf\n",
      "Resume_Katherine_Pearson.pdf\n",
      "Bachelor of Science in Biochemistry\n",
      "Resume_Kruti_Allenki.pdf\n",
      "M S  Applied Data Science   Syracuse University           \n",
      "Resume_Kunjan_Devendra_Khatri.pdf\n",
      "Bachelor of Engineering in Computer Science           \n",
      "Resume_Leonel_Flores.pdf\n",
      "M S  in Computer Science\n",
      "B S  in Mathematics\n",
      "Resume_Maaz_Zahid_Shaikh.pdf\n",
      "Bachelor of Engineering in Computer Engineering, Pune University, India             \n",
      "Resume_Naina Grover.pdf\n",
      "Bachelors of Commerce (Finance & Accounting)\n",
      "Resume_Nasir_Sarkar.pdf\n",
      "Bachelor of Arts in Biology and Philosophy\n",
      "Resume_Natnael_Tsegaw.pdf\n",
      "Bachelor of Arts, Government and Politics; Minor in Professional Writing\n",
      "Resume_Navina_Sethi.pdf\n",
      "Resume_Rakshit_Sinha.pdf\n",
      "Bachelor of Engineering, Electrical and Electronics,       \n",
      "Resume_Reetika_Chaturvedi.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fae96a59175b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_resume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-fae96a59175b>\u001b[0m in \u001b[0;36mparse_resume\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[0mraw_txt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPDFSyntaxError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         return_dict = {'index': index,\n",
      "\u001b[1;32m<ipython-input-17-fae96a59175b>\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\high_level.py\u001b[0m in \u001b[0;36mextract_text\u001b[1;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[0mcaching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         ):\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mctm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mrender_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_resources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1024\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, streams)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnextobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mPSEOF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\psparser.py\u001b[0m in \u001b[0;36mnextobject\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnexttoken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPSLiteral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;31m# normal token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\psparser.py\u001b[0m in \u001b[0;36mnexttoken\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillbuf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nexttoken: %r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pdfminer\\psparser.py\u001b[0m in \u001b[0;36m_parse_main\u001b[1;34m(self, s, i)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_literal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34mb'-+'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_curtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def read_file(path):\n",
    "    txt = extract_text(path, codec='utf-8')\n",
    "    return txt.encode('utf-8').decode('utf-8')\n",
    "\n",
    "def read_file2(path):\n",
    "    txt_lst = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in range(len(pdf.pages)):\n",
    "            txt_lst.append(pdf.pages[p].extract_text())\n",
    "        txt = ' '.join(txt_lst)\n",
    "    return txt.encode('utf-8').decode('utf-8')\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    # verify type of input\n",
    "    if not isinstance(txt, str):\n",
    "        raise TypeError('Work with String Only')\n",
    "    # verify len(text)\n",
    "    if len(txt) < 1:\n",
    "        raise ValueError('Work with nonempty String Only')\n",
    "    #tokenize\n",
    "    words = nltk.word_tokenize(txt)\n",
    "\n",
    "    #remove stopwords and punctuations\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    simple_strings = [word for word in words if word not in stopwords_set if word not in punctuation]\n",
    "\n",
    "    #remove unwanted characters\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    res3 = [res.replace(\"ï\", \"i\") if \"ï\" in res else res for res in res3]\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    res3 = [res.encode('ascii', \"ignore\").decode() for res in res3]\n",
    "\n",
    "    return list(filter(None, res3)) #remove empty string from list #return tokens_txt\n",
    "\n",
    "# name -------------------------------------------------------------------------\n",
    "indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "                     \"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\",\n",
    "                     \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "                     \"Chabra\", \"Chadha\", \"Chakrabarti\", \"Chawla\", \"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\",\n",
    "                     \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "                     \"Joshi\", \"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\",\n",
    "                     \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "                     \"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\",\n",
    "                     \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "                     \"Mahajan\", \"Kumar\", \"Mani\", \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\",\n",
    "                     \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\",\n",
    "                     \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "                      \"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "                      \"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "                      \"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\",\n",
    "                      \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\",\n",
    "\n",
    "                      \"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\",\n",
    "                      \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "                      \"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\",\n",
    "                      \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\",\n",
    "\n",
    "                      \"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "# chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "# indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "\n",
    "us_lnames = nd.get_top_names(n=2000, use_first_names = False, country_alpha2='US')\n",
    "china_lnames = nd.get_top_names(n=2000, use_first_names = False, country_alpha2='CN')\n",
    "indian_lnames = nd.get_top_names(n=2000, use_first_names = False, country_alpha2='IN')\n",
    "\n",
    "ref_names = list(set(us_lnames['US'] + china_lnames['CN'] + chinese_last_names+ indian_lnames['IN']  + indian_last_names))\n",
    "\n",
    "def get_name(sentence_txt, tokens_txt): #-> list of person names\n",
    "    temp_person = []\n",
    "    nltk_tagged = pos_tag(tokens_txt[:11])\n",
    "    for k in range(10):\n",
    "        if nltk_tagged[k][1] == 'NNP' and nltk_tagged[k + 1][1] == 'NNP':\n",
    "            nltk_name = nltk_tagged[k][0] + ' ' + nltk_tagged[k + 1][0]\n",
    "            temp_person.append(nltk_name)\n",
    "            if len(temp_person) > 0:# Comment: stop as long as you got the nltk_name\n",
    "                return temp_person\n",
    "        if k > 1:\n",
    "            if (tokens_txt[k].lower() in indian_last_names) or (tokens_txt[k].lower() in chinese_last_names):\n",
    "                j = k - 1\n",
    "                return([tokens_txt[j] + \" \" + tokens_txt[k]]) #Comment: what if kristin (jiating) chen?\n",
    "\n",
    "    tagged = nlp(sentence_txt[0:200]) #Comment: assume names exist in the top section\n",
    "    for word in tagged.ents:\n",
    "        if word.label_ == \"PERSON\":\n",
    "            temp_person.append(word.text)\n",
    "        return temp_person\n",
    "\n",
    "\n",
    "def get_name_from_ner(sentence_txt):\n",
    "    temp_person = []\n",
    "    tagged = nlp(sentence_txt[0:200]) #Comment: assume names exist in the top section\n",
    "    for word in tagged.ents:\n",
    "        if word.label_ == \"PERSON\":\n",
    "            temp_person.append(word.text)\n",
    "        return temp_person\n",
    "    return list(set(temp_person))\n",
    "\n",
    "def get_name_from_pos(tokens_txt):\n",
    "    temp_person = []\n",
    "    nltk_tagged = pos_tag(tokens_txt[:11])\n",
    "    for k in range(10):\n",
    "        if nltk_tagged[k][1] == 'NNP' and nltk_tagged[k + 1][1] == 'NNP':\n",
    "            nltk_name = nltk_tagged[k][0] + ' ' + nltk_tagged[k + 1][0]\n",
    "            temp_person.append(nltk_name)\n",
    "            # if len(temp_person) > 0:  # Comment: stop as long as you got the nltk_name\n",
    "            #     return temp_person\n",
    "    return list(set(temp_person))\n",
    "\n",
    "def get_name_from_ref(tokens_txt):\n",
    "    temp_person = []\n",
    "    for k in range(10):\n",
    "        if k >= 1:\n",
    "            j = k - 1\n",
    "            if tokens_txt[k].capitalize() in ref_names:\n",
    "                if len(tokens_txt[j]) < 2 and j != 0:\n",
    "                    temp_person.append(tokens_txt[j-1] + \" \" + tokens_txt[j] + \" \" + tokens_txt[k])\n",
    "                else:\n",
    "                    temp_person.append(tokens_txt[j] + \" \" + tokens_txt[k])\n",
    "    return(list(set(temp_person)))\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "    'University'\n",
    "]\n",
    "\n",
    "replace_words = [\"|\",\"/\",\".\",\":\",\"-\"]\n",
    "\n",
    "def extract_education(txt): # Comment: this is not solid\n",
    "    edu=set()\n",
    "    p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)')\n",
    "    for m in re.finditer(p, txt):\n",
    "\n",
    "        for word in RESERVED_WORDS:\n",
    "            if word in m.group(2).lower():\n",
    "                edu.add(m.group(2))\n",
    "    return edu\n",
    "\n",
    "# -----------------------------\n",
    "def extract_phone_number(txt):\n",
    "    PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if txt.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "# ---------------------------- -\n",
    "def extract_email(txt):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "# -----------------------------------\n",
    "def extract_phd_degree(txt):\n",
    "    phd_set = set()\n",
    "    p = re.compile('(Ph\\.D|PhD|PhD\\.).*,|(Ph\\.D|PhD|PhD\\.).*\\\\n')\n",
    "    for m in re.finditer(p, txt):\n",
    "        candidate_txt = m.group().split(',')[0]\n",
    "        phd_set.add(' '.join(candidate_txt.split()))\n",
    "    return phd_set\n",
    "\n",
    "def extract_phd_degree_from_lines(txt):\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    phd_degrees_lst = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(Ph\\.D|PhD|PhD\\.)' ,line):\n",
    "            phd_degrees_lst.append(line.strip())\n",
    "    if len(phd_degrees_lst)>0:        \n",
    "        phd_deg = [' '.join(m.split()) for m in phd_degrees_lst][0]\n",
    "        phd_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",phd_deg)\n",
    "        phd_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",phd_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in phd_deg2:\n",
    "                phd_deg2 = phd_deg2.replace(punct,\" \")\n",
    "    else:\n",
    "        phd_deg2 = None\n",
    "    return phd_deg2\n",
    "\n",
    "# ------------------------------------\n",
    "neglect_words = [\n",
    "    'MSSQL',\n",
    "    'MS LAB',\n",
    "    'MSMQ',\n",
    "    'MS-DB',\n",
    "    'MS-Excel',\n",
    "    'MS-Access',\n",
    "    'MATLAB',\n",
    "    'MARY',\n",
    "    'MACHINE',\n",
    "    'MAPE',\n",
    "    'MSE',\n",
    "    'MARKOV',\n",
    "    'MAXIMIZATION',\n",
    "    'MATPLOTLIB',\n",
    "    'MATPLOT',\n",
    "    'MATPLOTLIB',\n",
    "    'MATRICES',\n",
    "    'MATRIX',\n",
    "    'MASSACHUSETTS',\n",
    "    'Mssql',\n",
    "    'Ms Lab',\n",
    "    'Msmq',\n",
    "    'Ms-db',\n",
    "    'Ms-excel',\n",
    "    'Ms-access',\n",
    "    'Matlab',\n",
    "    'Mary',\n",
    "    'Machine',\n",
    "    'Mape',\n",
    "    'Mse',\n",
    "    'Markov',\n",
    "    'Maximization',\n",
    "    'Matplotlib',\n",
    "    'Matplot',\n",
    "    'Matplotlib',\n",
    "    'Matrices',\n",
    "    'Matrix',\n",
    "    'Massachusetts'\n",
    "    'mssql',\n",
    "    'ms lab',\n",
    "    'msmq',\n",
    "    'ms-db',\n",
    "    'ms-excel',\n",
    "    'ms-access',\n",
    "    'matlab',\n",
    "    'mary',\n",
    "    'machine',\n",
    "    'mape',\n",
    "    'mse',\n",
    "    'markov',\n",
    "    'maximization',\n",
    "    'matplotlib',\n",
    "    'matplot',\n",
    "    'matplotlib',\n",
    "    'matrices',\n",
    "    'matrix',\n",
    "    'massachusetts'\n",
    "]\n",
    "\n",
    "def extract_master_degree(txt):\n",
    "    ms_set = set()\n",
    "    masters_set = set()\n",
    "    p = re.compile('((?i:Master)|MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech).*,|((?i:Master)|MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech).*\\\\n')  # greedy\n",
    "    for m in re.finditer(p, txt):\n",
    "        candidate_txt = m.group().split(',')[0]\n",
    "        for w in candidate_txt.split():\n",
    "            if w in neglect_words:\n",
    "                continue\n",
    "            else:\n",
    "                ms_set.add(' '.join(candidate_txt.split()))\n",
    "        if re.search('(?i:Master)', candidate_txt):\n",
    "            #print(candidate_txt)\n",
    "            masters_set.add(' '.join(candidate_txt.split()))\n",
    "    if len(masters_set) > 0:\n",
    "        return masters_set\n",
    "    else:\n",
    "        return ms_set\n",
    "\n",
    "def is_valid_date(txt):\n",
    "    return_lst = []\n",
    "    matches = datefinder.find_dates(txt)\n",
    "    for match in matches:\n",
    "        #print(match)\n",
    "        return_lst.append(match)\n",
    "    if len(return_lst):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_master_degree_from_lines(txt):\n",
    "    year =\"(\\d{4})\"\n",
    "    month = \"((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))\"\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    master_degrees_lst = []\n",
    "    ms_lst = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i:Master)' ,line):\n",
    "            master_degrees_lst.append(line.strip())\n",
    "        if re.match('MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech' ,line):\n",
    "            ms_lst.append(line.strip())\n",
    "    if len(master_degrees_lst) > 0:\n",
    "        # cleaned_master_degress_lst = []\n",
    "        # for d in master_degrees_lst:\n",
    "        #     cleaned_master_degress_lst.append(' '.join([m for m in d.split() if m != month or not m.isdigit()]))\n",
    "        mast_deg = [' '.join(m.split()) for m in master_degrees_lst][0]\n",
    "        mast_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_deg)\n",
    "        mast_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",mast_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_deg2:\n",
    "                mast_deg2 = mast_deg2.replace(punct,\" \")\n",
    "        return mast_deg2\n",
    "    elif len(ms_lst) > 0:\n",
    "        mast_deg = [' '.join(m.split()) for m in ms_lst][0]\n",
    "        mast_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_deg)\n",
    "        mast_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",mast_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_deg2:\n",
    "                mast_deg2 = mast_deg2.replace(punct,\" \")\n",
    "        print(mast_deg2)\n",
    "        return mast_deg2\n",
    "    else:\n",
    "        mast_deg2 = None\n",
    "        return mast_deg2\n",
    "# ---------------------------------\n",
    "# doesn't match: \\nBachelors of Management Studies (concentration in marketing)\n",
    "def extract_bachelor_degree(txt):\n",
    "    bachelors_set = set()\n",
    "    ba_set = set()\n",
    "    p = re.compile('((?i:Bachelor)|BS|B\\.S\\.|BA|B\\.A\\.|B\\.S\\.E|B\\.tech).*,|((?i:Bachelor)|BS|B\\.S\\.|BA|B\\.A\\.|B\\.S\\.E|B\\.tech).*\\\\n')  # greedy\n",
    "    for m in re.finditer(p, txt):\n",
    "        candidate_txt = m.group().split(',')[0] #MS Excel MS word...\n",
    "        #print(candidate_txt)\n",
    "        ba_set.add(' '.join(candidate_txt.split()))\n",
    "        if re.search('(?i:Bachelor)', candidate_txt):\n",
    "            bachelors_set.add(' '.join(candidate_txt.split()))\n",
    "        if len(bachelors_set) > 0:\n",
    "            return bachelors_set\n",
    "        else:\n",
    "            return ba_set\n",
    "\n",
    "def extract_bachelor_degree_from_lines(txt):\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    bachelors_set = []\n",
    "    ba_set = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i:Bachelor)' ,line):\n",
    "            bachelors_set.append(line.strip())\n",
    "        if re.match('BS|B\\.S\\.|BA|B\\.A\\.|B\\.S\\.E|B\\.tech' ,line):\n",
    "            ba_set.append(line.strip())\n",
    "\n",
    "    if len(bachelors_set) > 0:\n",
    "        bach_deg = [' '.join(m.split()) for m in bachelors_set][0]\n",
    "        bach_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",bach_deg)\n",
    "        bach_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",bach_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in bach_deg2:\n",
    "                bach_deg2 = bach_deg2.replace(punct,\" \")\n",
    "        print(bach_deg2)\n",
    "        return bach_deg2\n",
    "    elif len(ba_set) > 0:\n",
    "        bach_deg = [' '.join(m.split()) for m in ba_set][0]\n",
    "        bach_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",bach_deg)\n",
    "        bach_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",bach_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in bach_deg2:\n",
    "                bach_deg2 = bach_deg2.replace(punct,\" \")\n",
    "        print(bach_deg2)\n",
    "        return bach_deg2\n",
    "    else:\n",
    "        bach_deg2 = None\n",
    "        return bach_deg2\n",
    "\n",
    "# -----------------------------------------------------\n",
    "def extract_phd_school(txt):\n",
    "    p = re.compile(\n",
    "        '(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(Ph.D.|PhD|PhD.)')\n",
    "    for m in re.finditer(p, txt):\n",
    "        return m.group(0).split(\"\\n\")[0]\n",
    "\n",
    "def extract_ms_school(txt):\n",
    "    p = re.compile(\n",
    "        '(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(M.S.|M.S|M.Sc.|MS|Masters|(?i:Master))')\n",
    "    for m in re.finditer(p, txt):\n",
    "        return m.group(0).split(\"\\n\")[0]\n",
    "\n",
    "\n",
    "## Using regex for finding university nearest to Bachelor's degree\n",
    "def extract_bach_school(txt):\n",
    "    p = re.compile(\n",
    "        '(?i)(school|college|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(?:(?!(?i)(school|college|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)|(Bachelors|B.tech|Bachelor\\'s|B.A.|BS|B.Sc.|B.S|B.S.))[\\s\\S])*(Bachelors|B.tech|Bachelor\\'s|B.A.|BS|B.Sc.|B.S|B.S.)')\n",
    "    for m in re.finditer(p, txt):\n",
    "        return m.group(0).split(\"\\n\")[0]\n",
    "\n",
    "# date -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_master_date(txt1):\n",
    "    lines_txt = [l.strip() for l in txt1.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "\n",
    "    for index, line in enumerate(lines_txt):\n",
    "            if re.match('(?i:Master)' ,line):\n",
    "                masters_date_lst = []\n",
    "                start_idx = index\n",
    "                for i, l in enumerate(lines_txt[start_idx:start_idx + 8]):\n",
    "                    #print(i, l)\n",
    "                    if datefinder.find_dates(l):\n",
    "                        matches = datefinder.find_dates(l)\n",
    "                        for match in matches:\n",
    "                            #print(match)\n",
    "                            masters_date_lst.append(match)\n",
    "                        if len(masters_date_lst) > 0:\n",
    "                            return [d.strftime(\"%m/%Y\") for d in masters_date_lst]\n",
    "            elif re.match('MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech' ,line):\n",
    "                return_set = []\n",
    "                start_idx = index\n",
    "                for _, l in enumerate(lines_txt[start_idx:start_idx + 8]):\n",
    "                    if datefinder.find_dates(l):\n",
    "                        matches = datefinder.find_dates(l)\n",
    "                        for match in matches:\n",
    "                            #print(match)\n",
    "                            return_set.append(match)\n",
    "                        return [d.strftime(\"%m/%Y\") for d in return_set]\n",
    "\n",
    "def extract_bachelor_date(txt1):\n",
    "    lines_txt = [l.strip() for l in txt1.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i:Bachelor)', line):\n",
    "            bachelor_date_lst = []\n",
    "            start_idx = index\n",
    "            for i, l in enumerate(lines_txt[start_idx:start_idx + 8]):\n",
    "                #print(i, l)\n",
    "                if datefinder.find_dates(l):\n",
    "                    matches = datefinder.find_dates(l)\n",
    "                    for match in matches:\n",
    "                        #print(match)\n",
    "                        bachelor_date_lst.append(match)\n",
    "                    if len(bachelor_date_lst) > 0:\n",
    "                        return [d.strftime(\"%m/%Y\") for d in bachelor_date_lst]\n",
    "        elif re.match('BS|B\\.S\\.|BA|B\\.A\\.|B\\.S\\.E|B\\.tech' ,line):\n",
    "            return_set = []\n",
    "            start_idx = index\n",
    "            for _, l in enumerate(lines_txt[start_idx:start_idx + 8]):\n",
    "                if datefinder.find_dates(l):\n",
    "                    matches = datefinder.find_dates(l)\n",
    "                    for match in matches:\n",
    "                        #print(match)\n",
    "                        return_set.append(match)\n",
    "                    return [d.strftime(\"%m/%Y\") for d in return_set]\n",
    "\n",
    "def extract_phd_date(txt1):\n",
    "    lines_txt = [l.strip() for l in txt1.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    phd_date_lst = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(Ph\\.D|PhD|PhD\\.)' ,line):\n",
    "            start_idx = index\n",
    "            for i, l in enumerate(lines_txt[start_idx:start_idx + 8]):\n",
    "                #print(i, l)\n",
    "                if datefinder.find_dates(l):\n",
    "                    matches = datefinder.find_dates(l)\n",
    "                    for match in matches:\n",
    "                        #print(match)\n",
    "                        phd_date_lst.append(match)\n",
    "                    if len(phd_date_lst) > 0:\n",
    "                        return [d.strftime(\"%m/%Y\") for d in phd_date_lst]\n",
    "\n",
    "    return phd_date_lst\n",
    "\n",
    "\n",
    "\n",
    "# --------- aggregate -----------------------------\n",
    "def get_info(txt):\n",
    "    tokens_txt = preprocess_text(txt)\n",
    "    name = get_name(txt, tokens_txt)\n",
    "    name_ner = get_name_from_ner(txt)\n",
    "    name_pos = get_name_from_pos(tokens_txt)\n",
    "    name_ref = get_name_from_ref(tokens_txt)\n",
    "    # education = extract_education(txt)\n",
    "    phone_number = extract_phone_number(txt)\n",
    "    email = extract_email(txt)\n",
    "    master_degree = extract_master_degree(txt)\n",
    "    bachelor_degree = extract_bachelor_degree(txt)\n",
    "    phd_degree = extract_phd_degree(txt)\n",
    "    master_degree2 = extract_master_degree_from_lines(txt)\n",
    "    bachelor_degree2 = extract_bachelor_degree_from_lines(txt)\n",
    "    phd_degree2 = extract_phd_degree_from_lines(txt)\n",
    "\n",
    "    master_school = extract_ms_school(txt)\n",
    "    bachelor_school = extract_bach_school(txt)\n",
    "    phd_school = extract_phd_school(txt)\n",
    "    master_graduation_date = extract_master_date(txt)\n",
    "    bachelor_graduation_date = extract_bachelor_date(txt)\n",
    "    phd_graduation_date = extract_phd_date(txt)\n",
    "    return {\n",
    "        'name' : name,\n",
    "        'name_ner': name_ner,\n",
    "        'name_pos': name_pos,\n",
    "        'name_ref': name_ref,\n",
    "        'phone_number' : phone_number,\n",
    "        'email' : email,\n",
    "        'master_degree' : master_degree,\n",
    "        'bachelor_degree': bachelor_degree,\n",
    "        'phd_degree' : phd_degree,\n",
    "        'master_degree2': master_degree2,\n",
    "        'bachelor_degree2': bachelor_degree2,\n",
    "        'phd_degree2': phd_degree2,\n",
    "        'master_school': master_school,\n",
    "        'bachelor_school': bachelor_school,\n",
    "        'phd_school': phd_school,\n",
    "        'master_graduation_date': master_graduation_date,\n",
    "        'bachelor_graduation_date': bachelor_graduation_date,\n",
    "        'phd_graduation_date': phd_graduation_date\n",
    "    }\n",
    "\n",
    "def parse_resume(path):\n",
    "    index = os.path.basename(path)\n",
    "    print(index)\n",
    "    try:\n",
    "        raw_txt = read_file(path)\n",
    "    except pdfminer.pdfparser.PDFSyntaxError:\n",
    "        return_dict = {'index': index,\n",
    "                   'name': None,\n",
    "                   'name_ner': None,\n",
    "                   'name_pos': None,\n",
    "                   'name_ref': None,\n",
    "                   'phone_number': None,\n",
    "                   'email': None,\n",
    "                   'master_degree': None,\n",
    "                   'bachelor_degree': None,\n",
    "                   'phd_degree': None,\n",
    "                   'master_degree2': None,\n",
    "                   'bachelor_degree2': None,\n",
    "                   'phd_degree2': None,\n",
    "                   'master_school': None,\n",
    "                   'bachelor_school': None,\n",
    "                   'phd_school': None,\n",
    "                   'master_graduation_date': None,\n",
    "                   'bachelor_graduation_date': None,\n",
    "                   'phd_graduation_date': None\n",
    "                       }\n",
    "        return return_dict\n",
    "\n",
    "    if not isinstance(raw_txt, str):\n",
    "        return_dict = {   'index' : index,\n",
    "                          'name': None,\n",
    "                          'name_ner': None,\n",
    "                          'name_pos': None,\n",
    "                          'name_ref': None,\n",
    "                          'phone_number': None,\n",
    "                          'email': None,\n",
    "                          'master_degree': None,\n",
    "                          'bachelor_degree': None,\n",
    "                          'phd_degree': None,\n",
    "                          'master_degree2': None,\n",
    "                          'bachelor_degree2': None,\n",
    "                          'phd_degree2': None,\n",
    "                          'master_school': None,\n",
    "                          'bachelor_school': None,\n",
    "                          'phd_school': None,\n",
    "                          'master_graduation_date': None,\n",
    "                          'bachelor_graduation_date': None,\n",
    "                          'phd_graduation_date': None}\n",
    "        return return_dict\n",
    "    elif len(raw_txt) < 8:\n",
    "        return_dict = {   'index' : index,\n",
    "                          'name': None,\n",
    "                          'name_ner': None,\n",
    "                          'name_pos': None,\n",
    "                          'name_ref': None,\n",
    "                          'phone_number': None,\n",
    "                          'email': None,\n",
    "                          'master_degree': None,\n",
    "                          'bachelor_degree': None,\n",
    "                          'phd_degree': None,\n",
    "                          'master_degree2': None,\n",
    "                          'bachelor_degree2': None,\n",
    "                          'phd_degree2': None,\n",
    "                          'master_school': None,\n",
    "                          'bachelor_school': None,\n",
    "                          'phd_school': None,\n",
    "                          'master_graduation_date': None,\n",
    "                          'bachelor_graduation_date': None,\n",
    "                          'phd_graduation_date': None}\n",
    "        return return_dict\n",
    "    else:\n",
    "        info_dict = get_info(raw_txt)\n",
    "        return_dict = {**{'index': index}, **info_dict}\n",
    "        return return_dict\n",
    "\n",
    "# -------------- execution ------------------------------------------------------\n",
    "folder = glob.glob(r\"C:/Users/rtd91/Data/resume_samples/*\")\n",
    "# file_path = folder[11]\n",
    "# sample_txt = read_file(file_path)\n",
    "# sample_txt2 = read_file2(file_path)\n",
    "# tokens_txt = preprocess_text(sample_txt)\n",
    "#\n",
    "# get_name(sentence_txt=sample_txt, tokens_txt = tokens_txt)\n",
    "# get_name_from_ner(sample_txt)\n",
    "# get_name_from_pos(tokens_txt)\n",
    "# get_name_from_ref(tokens_txt)\n",
    "# extract_education(sample_txt[0:1000])\n",
    "# doc = nlp(sample_txt)\n",
    "# extract_phd_degree(sample_txt)\n",
    "# extract_phd_degree_from_lines(sample_txt)\n",
    "# extract_phd_school(sample_txt)\n",
    "# extract_phd_date(sample_txt)\n",
    "# extract_master_degree(sample_txt)\n",
    "# extract_master_degree_from_lines(sample_txt)\n",
    "# extract_ms_school(sample_txt)\n",
    "# extract_master_date(sample_txt)\n",
    "# extract_bachelor_degree(sample_txt)\n",
    "# extract_bachelor_degree_from_lines(sample_txt)\n",
    "# extract_bach_school(sample_txt)\n",
    "# extract_bachelor_date(sample_txt)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for f in folder:\n",
    "    return_dict = parse_resume(f)\n",
    "    df = df.append(return_dict, ignore_index = True)\n",
    "\n",
    "df.to_csv('matched_resumes.csv', index=False)\n",
    "\n",
    "# # --------- performance ------------------------------\n",
    "# gt_df = pd.read_csv('./Data/GroundTruth-Sheet2.csv', index_col=False)\n",
    "# gt_df.shape #182\n",
    "# gt_df.columns\n",
    "# gt_df.master_graduation_date\n",
    "# fuzz.token_sort_ratio(\"KRISTIN J CHEN\", \"KRISTIN JIATING CHEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Master of Science Information Systems | 3.54/4 GPA\"\n",
    "def extract_master_degree_from_lines(txt):\n",
    "    year =\"(\\d{4})\"\n",
    "    month = \"((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))\"\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    master_degrees_lst = []\n",
    "    ms_lst = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i:Master)' ,line):\n",
    "            master_degrees_lst.append(line.strip())\n",
    "        if re.match('MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech' ,line):\n",
    "            ms_lst.append(line.strip())\n",
    "    if len(master_degrees_lst) > 0:\n",
    "        # cleaned_master_degress_lst = []\n",
    "        # for d in master_degrees_lst:\n",
    "        #     cleaned_master_degress_lst.append(' '.join([m for m in d.split() if m != month or not m.isdigit()]))\n",
    "        print(master_degrees_lst)\n",
    "        mast_deg = [' '.join(m.split()) for m in master_degrees_lst][0]\n",
    "        print(mast_deg)\n",
    "        mast_deg1 = re.sub(\"GPA|.|Current|Cumulative|\\d+|–|-|((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))\", \" \", mast_deg)\n",
    "        print(mast_deg1)\n",
    "        return mast_deg1\n",
    "    elif len(ms_lst) > 0:\n",
    "        mast_deg = [' '.join(m.split()) for m in ms_lst][0]\n",
    "        mast_deg1 = re.sub(\"GPA|.|Current|Cumulative|\\d+|–|-|((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))\", \" \", mast_deg)\n",
    "        print(mast_deg1)\n",
    "        return mast_deg1\n",
    "    else:\n",
    "        mast_deg = None\n",
    "    return mast_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Master of Science Information Systems | 3.54/4 GPA']\n",
      "Master of Science Information Systems | 3.54/4 GPA\n",
      "                                                \n",
      "                                                \n"
     ]
    }
   ],
   "source": [
    "test = extract_master_degree_from_lines(sample_text)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_words = [\"|\",\"/\",\".\"]\n",
    "sample_txt = \"Master of Science Information Systems | 3.54/4 Cumulative Current GPA January 2021\" \n",
    "sample_txt1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",sample_txt)\n",
    "sample_txt2 = re.sub(\"Current|Cumulative|GPA\",\" \",sample_txt1)\n",
    "#sample_txt3 = re.sub(r'[\\w\\s]',\" \",sample_txt2)\n",
    "for punct in replace_words:\n",
    "    if punct in sample_txt2:\n",
    "        sample_txt2 = sample_txt2.replace(punct,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Master of Science Information Systems                  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ms_school_from_lines(txt):\n",
    "    \"\"\"p = re.compile(\n",
    "        '(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(M.S.|M.S|M.Sc.|MS|Masters|(?i:Master))')\n",
    "    \n",
    "    for m in re.finditer(p, txt):\n",
    "        mast_school = m.group(0).split(\"\\n\")[0]\n",
    "    \"\"\"\n",
    "    ms_school_lst = []\n",
    "    master_schools_lst = []\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        print(re.findall('(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(?i:Master)' ,line))\n",
    "        #master_schools_lst.append(line)\n",
    "        if re.match('(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech)' ,line):\n",
    "            ms_school_lst.append(line)\n",
    "    if len(master_schools_lst)>0:\n",
    "        mast_school = [' '.join(m.split()) for m in master_schools_lst][0]\n",
    "        mast_school1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_school)\n",
    "        mast_school2 = re.sub(\"Current|Cumulative|GPA|Master|of|Science|In|Statistics|â€“|present|Expected|Graduation|Graduating\",\" \",mast_school1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_school2:\n",
    "                mast_school2 = mast_school2.replace(punct,\" \")\n",
    "                return mast_school2\n",
    "    elif len(ms_school_lst)>0:\n",
    "        mast_school = [' '.join(m.split()) for m in ms_school_lst][0]\n",
    "        mast_school1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_school)\n",
    "        mast_school2 = re.sub(\"Current|Cumulative|GPA|Master|of|Science|In|Statistics|â€“|present|Expected|Graduation|Graduating\",\" \",mast_school1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_school2:\n",
    "                mast_school2 = mast_school2.replace(punct,\" \")\n",
    "                return mast_school2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bach_school_from_lines(txt):\n",
    "    \"\"\"p = re.compile(\n",
    "        '(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(M.S.|M.S|M.Sc.|MS|Masters|(?i:Master))')\n",
    "    \n",
    "    for m in re.finditer(p, txt):\n",
    "        mast_school = m.group(0).split(\"\\n\")[0]\n",
    "    \"\"\"\n",
    "    bach_lst = []\n",
    "    bach_schools_lst = []\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(?i:Bachelor)' ,line):    \n",
    "            bach_schools_lst.append(line)\n",
    "        if re.match('(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(BS|B\\.S\\.|BA|B\\.A\\.|B\\.S\\.E|B\\.tech)' ,line):\n",
    "            bach_schools_lst.append(line)\n",
    "    if len(bach_schools_lst)>0:\n",
    "        bach_school = [' '.join(m.split()) for m in bach_schools_lst][0]\n",
    "        bach_school1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",bach_school)\n",
    "        bach_school2 = re.sub(\"Current|Cumulative|GPA|Master|of|Science|In|Statistics|â€“|present|Expected|Graduation|Graduating\",\" \",bach_school1)\n",
    "        for punct in replace_words:\n",
    "            if punct in bach_school2:\n",
    "                bach_school2 = bach_school2.replace(punct,\" \")\n",
    "                return bach_school2\n",
    "    elif len(bach_lst)>0:\n",
    "        bach_school = [' '.join(m.split()) for m in bach_lst][0]\n",
    "        bach_school1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",bach_school)\n",
    "        bach_school2 = re.sub(\"Current|Cumulative|GPA|Master|of|Science|In|Statistics|â€“|present|Expected|Graduation|Graduating\",\" \",bach_school1)\n",
    "        for punct in replace_words:\n",
    "            if punct in bach_school2:\n",
    "                bach_school2 = bach_school2.replace(punct,\" \")\n",
    "                return bach_school2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phd_school_from_lines(txt):\n",
    "    \"\"\"p = re.compile(\n",
    "        '(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(M.S.|M.S|M.Sc.|MS|Masters|(?i:Master))')\n",
    "    \n",
    "    for m in re.finditer(p, txt):\n",
    "        mast_school = m.group(0).split(\"\\n\")[0]\n",
    "    \"\"\"\n",
    "    phd_lst = []\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i)(school|college|univers|academy|faculty|institute|faculdades|Schola|schule|lise|lyceum|lycee|polytechnic|kolej|ünivers|okul|University)(.|\\n)*(?i:(Ph\\.D|PhD|PhD\\.))' ,line):    \n",
    "            phd_lst.append(line)\n",
    "    if len(phd_lst)>0:\n",
    "        phd_school = [' '.join(m.split()) for m in phd_lst][0]\n",
    "        phd_school1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",phd_school)\n",
    "        phd_school2 = re.sub(\"Current|Cumulative|GPA|Master|of|Science|In|Statistics|â€“|present|Expected|Graduation|Graduating\",\" \",phd_school1)\n",
    "        for punct in replace_words:\n",
    "            if punct in phd_school2:\n",
    "                phd_school2 = phd_school2.replace(punct,\" \")\n",
    "                return phd_school2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = \"\"' \\n\\n \\n\\nTIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.com | 469-370-9437 | https://www.linkedin.com/in/tirthpatel24/ \\n\\nEDUCATION \\nThe University of Texas at Dallas, Dallas, TX \\nMaster of Science, Business Analytics \\nRelevant Coursework:  Statistics and Data Analysis, Data Visualization, Business Analytics with R, Database Foundations, Applied ML \\nNirma University, Ahmedabad, India \\nBachelor of Technology, Instrumentation and Control Engineering \\nStudy Abroad: Graduate Level Summer Course, Studies in Innovation and Entrepreneurship at Harvard Summer School (July 2016) \\n\\nMay 2021 \\nGPA: 3.69 \\n\\nMay 2018 \\n\\n \\n\\nTECHNICAL SKILLS AND CERTIFICATIONS \\nDatabases and Languages: \\nVisualization Tools: \\nAnalysis Tools: \\nModeling and Statistical Analysis:  Regression, Classification, Time Series Analysis, Optimization, Hypothesis Testing, A/B Testing, ETL \\n\\nMySQL, Microsoft SQL Server, MongoDB, Python, R \\nTableau, Microsoft Power BI (DAX), Google Data Studio \\nSAS, STATA, Google Analytics, Adobe Analytics, MS Excel (Pivot Tables, VLOOKUP, Macros) \\n\\n \\n\\nPROFESSIONAL EXPERIENCE \\nWeb Analyst, The University of Texas at Dallas | Dallas, TX \\n Jan 2021 – May 2021 \\n•  Analyzed web traffic behavior for 14 graduate programs using Google Analytics and recommended digital marketing campaigns \\n\\n \\n\\nthat increased  monthly active users by 15%. \\nImproved data collection efficiencies by troubleshooting roadblocks using Google Tag Manager that enhanced reporting quality. \\nIdentified trends and patterns of user behavior to optimize the conversion funnels and reduced exit rates by 10%. \\n\\n• \\n• \\n•  Defined and translated KPIs into meaningful insights for 35+ program managers and directors using Data Studio dashboards and \\n\\n• \\n\\ntrained them to self-serve their data needs. \\nPerformed user navigation  analysis using Google Page Analytics to provide content optimization recommendations, increasing \\naverage page value by 20%. \\n\\nData Analyst, TruSkills Solutions | Ahmedabad, India \\n• \\n\\n May 2018 – Jul 2019 \\nFacilitated the development of digital learning solutions by translating product features into functional requirements that allowed \\nfor creation and delivery of 1700 hours of interactive content. \\n\\nStreamlined the process of cleaning and pre-processing Excel files for data import using Python reducing manual efforts by 2x. \\nSpearheaded cross-functional planning sessions to propose new product features and improvements based on user feedback. \\n\\n•  Wrote complex SQL queries to extract data that serve as underpinnings of ad-hoc and recurring analysis. \\n•  Developed Tableau dashboards for stakeholders and clients to report user performance and behavior. \\n• \\n• \\nTeaching Assistant, Divyapath Science School | Ahmedabad, India \\n•  Delivered supplementary lectures to 150+ high school students to reinforce foundational concepts in Statistics and Calculus. \\n•  Held one-on-one mentoring sessions to help students struggling with coursework, achieving a 35% increase in class results. \\n• \\n\\nCollaborated with supervisors to design learning solutions to meet individual student needs, improving course evaluation by 24%. \\n\\n                              Jan 2017 – May 2018 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nACADEMIC PROJECTS \\nEmail Marketing Campaign Analysis (Python – Pandas, Tableau, MS Excel) \\nAug 2021 \\n•  Analyzed data on 9.5M+ emails sent to over 1M subscribers and their response actions to evaluate the campaign performance. \\n•  Discovered trends, patterns and presented recommendations to improve future campaigns and maximize profit from members. \\nE-commerce Sales Analysis (Google Analytics, Power BI, MS Excel) \\nMay 2020 \\n• \\nEstablished KPIs for Godiva Chocolatier by analyzing growth and marketing strategies and identified factors affecting the metrics. \\n• \\nCreated Power BI dashboards for YoY comparison of online sales and presented recommendations to improve performance. \\nSocio-Economic Impact of the Opioid Crisis (Tableau, MS Excel, Python – Pandas, Seaborn)                                                        May 2021 \\n•  Visualized DEA ARCOS dataset of the distribution of 500M opioid pills from 2008-2014 across the US to find insights and trends. \\n• \\nPredicting Responses to Marketing Campaign (Python – NumPy, Scikit-Learn, Pandas, Matplotlib) \\n• \\n• \\nUS Wildfires Data Analysis (MySQL, MongoDB)                                                                                                                                         Oct 2019 \\n•  Modeled a relational database from raw dataset of 1.88 million US wildfires and implemented the normalized database in MySQL. \\n• \\nPerformed data extraction and analysis to discover hidden insights using joins, nested conditions, and complex SQL queries. \\nPrediction of Credit Card Defaulters (MS Excel, R - dplyr, ggplot2, shiny) \\n•  Analyzed historical data of 30K credit card clients to determine most important factors resulting in default payments.  \\n• \\n\\nBuilt statistical models using ML algorithms for target variable prediction and improved accuracies by tuning hyperparameters. \\nPredicted with 86% accuracy if a customer will subscribe to term deposit in response to marketing campaign with kernelized SVM. \\n\\nEmployed various data mining techniques to analyze the impact on highly affected counties and provided measures for mitigation. \\n\\nImplemented various algorithms such as logistic regression, decision trees and neural networks to build a predictive model that \\nidentifies potential defaulters with an accuracy of 82%.  \\n\\n  Dec 2020  \\n\\nFeb 2020 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c'\"\"\n",
    "sample_txt1 = \"'SAIDA MUKTAR \\n\\nPhone: 443-833-6344 \\n\\nEmail: saidam1@umbc.edu \\n\\nEDUCATION \\nMS in Data Science     Graduation: August 2021 \\nGraduate Certificate in Cybersecurity Operations  \\nRelevant courses: Introduction to Data Science, Introduction to Data Analysis and Machine Learning, Systems and Information Integration, Ethical and \\nLegal Issues in Data Science, Data Management, Big Data Platforms, Introduction to Cybersecurity, Risk Analysis & Compliance, Managing \\nCyberSecurity Operations  \\n \\nBS in Mathematics      Graduation: December 2019  \\nRelevant courses: Calculus I/II/III, Computer Science (Python), Linear Algebra, Differential Equations, Partial Differential Equations, Introduction to \\nMath Analysis I/II, Physics I/II, Biomathematics, Dynamical Systems, Mathematical Physiology \\n \\nTECHNICAL PROFILE \\nOperating systems: Unix(Linux), Windows  \\nTechnologies: SQL, Python, R, MATLAB, Maple, LaTeX, MySQL, Spark, Azure, Hadoop, MapReduce, Databricks, Pandas, Numpy, Scikit-learn, Seaborn, \\nMatplotlib, Folium, PyTorch, Tensoflow \\nData Science Skills: Statistical analysis, Predictive modeling, simulation, Deep Learning models, Time-Series analysis, Clustering, Decision Tree Learning, \\nArtificial Neural Networks, Regression, Exploratory Data Analysis, Data Engineering, Data Prep, Traditional Predictive Modeling, Statistical Analysis, \\nNatural Language Processing  \\n\\n \\n\\nEXPERIENCE \\nONCOSPACE         \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n  2020 – PRESENT  \\n\\n• \\n\\nDesigned models using both supervised and unsupervised machine learning methods that predict radiation doses for oncologists  \\n\\n- \\n\\n- \\n\\nCreated a report of validation accuracy metrics (R^2, mean absolute error, area between the curve, shortest distance error) for \\neach grouping of plans and identification of plans for which the auto planning results are more or less accurate than is typical  \\nPrepared a report of any significant error patterns identified in the auto planning model results and characteristics of plans which \\nexhibited each error pattern \\n- \\nDVH clustering for refraction of plan model assignment  \\n-  Made use of KMean clustering for machine learning analysis  \\n- \\n- \\n\\nOperated different graphing techniques in order to check what variables in our model were causing the clusters. \\nImplemented Principal Component Analysis (PCA) in order to check correlation between clusters using OVH graph \\nAnalyzed a variety of large and complex data sets using a variety of standard analytic techniques including regression, and machine \\nlearning approaches \\nUsed data from SQL databases in python to study information of patients \\n\\n• \\n•       Presented and summarized analyses in various formats including raw output, tables, and graphics, oral and written reports \\n• \\n• \\n• \\n•  Wrote manual test cases using Azure DevOps  \\n\\nDesigned tables and graphs for visualization using Pandas and Matplotlib \\nUtilized statistical packages in order to analyze results from Machine Learning outputs \\nFamiliarized with Azure Cloud as we utilize it as the primary platform for most data studies \\n\\n• \\n\\nUNIVERSITY OF MARYLAND BALTIMORE COUNTY      \\n\\n 2017  – 2019 \\n\\nStudied the time course of nuclear-cytoplasmic distribution of Foxo1 \\n\\n• \\n•  Made mathematical models to show the modulation of nuclear influx and efflux of Foxo1 by IGF-I/PI3K/Akt pathway in skeletal muscle \\n\\n•  Used mathematical models of nuclear-cytoplasmic movements of Foxo1 to provide values of unidirectional influx and efflux under various \\n\\n•  Made mathematical models to help determine the properties of Foxo1 phosphorylation/dephosphorylation status in nuclei and cytoplasm \\n\\nfibers. \\n\\nexperimental conditions \\n\\nof skeletal muscle fibers. \\n\\n•  Made two-state mathematical models to study the Foxo1 nuclear-cytoplasmic movements \\n•  Used MATLAB to test the models by fitting date from blocking nuclear efflux with Leptomycin \\n• \\n•  Used LaTex for project reporting \\n\\nPlotted datas in MATLAB to find parameters and fit parameters \\n\\nUNIVERSITY OF MARYLAND BALTIMORE  \\n\\n \\n\\n \\n\\n 2020 \\n\\nStudying levels of serotonin release in brain when pain is inflicted \\n\\n• \\n•  Making Python and Matlab codes that will record and analyze data when pain is recorded after a given threshold \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\nREFERENCE AVAILABLE UPON REQUEST \\n\\nBS degree in statistics , ma chine lear ning, or data s cien ce  \\nBS degree, or e quivalent experien ce, in syste m engi neering or  decisi on the ory \\nTwo or more years’ experien ce desig ning and leading 50 -per son  or larger techni cal progra ms  \\nEvidence  of sel f-directe d organizational investigation, diag nosis , and re development  \\nTwo or more years’ experien ce usi ng simulation or ot her analytical tools to  model and  control software developme nt  \\nPublicati on hist ory that demonstrates e ffe ctive technical communication skills  \\nPreferred Qualifications  \\n \\nMS degree in statistics,  machine learni ng, or data scie nce  \\nMS or PhD degree i n system e ngineering  or deci sion theory  \\nTwo or more years’ experien ce desig ning, leadi ng, and  optimizi ng 100 -pers on  or larger techni cal progra ms.  \\nFive or more years’ experien ce de signing and buil ding si mulations or other a nalytical tools to  model a nd control s oftware development  \\nPublicati on hist ory that demonstrates e ffe ctive technical communication skills. Speci fically, publications that docume nts sel f-dire cted orga nizational inve stigation, diagnosis, and redevelopment recomme ndations.  \\nSpace industry experien ce in the private, military, or civilian se ctor.  \\n \\n\\n\\x0c'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "extract_ms_school_from_lines(sample_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_bach_school_from_lines(sample_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_phd_school_from_lines(sample_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_master_degree_from_lines(txt):\n",
    "    year =\"(\\d{4})\"\n",
    "    month = \"((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))\"\n",
    "    lines_txt = [l.strip() for l in txt.split('\\n') if len(l.strip()) > 0 and len(l.strip().split()) > 0]\n",
    "    master_degrees_lst = []\n",
    "    ms_lst = []\n",
    "    for index, line in enumerate(lines_txt):\n",
    "        if re.match('(?i:Master)' ,line):\n",
    "            master_degrees_lst.append(line.strip())\n",
    "        if re.match('MS|M\\.S\\.|MA|M\\.A\\.|MBA|M\\.S\\.E|M\\.tech' ,line):\n",
    "            ms_lst.append(line.strip())\n",
    "    if len(master_degrees_lst) > 0:\n",
    "        # cleaned_master_degress_lst = []\n",
    "        # for d in master_degrees_lst:\n",
    "        #     cleaned_master_degress_lst.append(' '.join([m for m in d.split() if m != month or not m.isdigit()]))\n",
    "        mast_deg = [' '.join(m.split()) for m in master_degrees_lst][0]\n",
    "        mast_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_deg)\n",
    "        mast_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",mast_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_deg2:\n",
    "                mast_deg2 = mast_deg2.replace(punct,\" \")\n",
    "        return mast_deg2\n",
    "    elif len(ms_lst) > 0:\n",
    "        mast_deg = [' '.join(m.split()) for m in ms_lst][0]\n",
    "        mast_deg1 = re.sub(\"\\d+|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Sept|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\", \" \",mast_deg)\n",
    "        mast_deg2 = re.sub(\"Current|Cumulative|GPA\",\" \",mast_deg1)\n",
    "        for punct in replace_words:\n",
    "            if punct in mast_deg2:\n",
    "                mast_deg2 = mast_deg2.replace(punct,\" \")\n",
    "        print(mast_deg2)\n",
    "        return mast_deg2\n",
    "    else:\n",
    "        mast_deg2 = None\n",
    "        return mast_deg2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
