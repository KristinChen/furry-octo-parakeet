{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import textract, PyPDF2, glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: Wand>=0.6.7 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from pdfplumber) (0.6.7)\n",
      "Requirement already satisfied: Pillow>=8.4 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from pdfplumber) (9.0.1)\n",
      "Requirement already satisfied: pdfminer.six==20211012 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from pdfplumber) (20211012)\n",
      "Requirement already satisfied: chardet in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from pdfminer.six==20211012->pdfplumber) (3.0.4)\n",
      "Requirement already satisfied: cryptography in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from pdfminer.six==20211012->pdfplumber) (2.8)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.14.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.12.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rtd91\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six==20211012->pdfplumber) (2.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rtd91\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(files1[4]) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    txt = first_page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(files1[0]) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    txt = first_page.extract_text()\n",
    "words = nltk.word_tokenize(txt) \n",
    "##removing stopwords and punctuations\n",
    "simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "res2 = [str(res) for res in res1]\n",
    "res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', ',', 'Baltimore', ',', 'MD', '21231', 'Contact', 'No', ':', '(', '410', ')', '-292-1151', ';', 'Email', 'ID', ':', 'karthikr2194', '@', 'gmail.com', 'LinkedIn', ':', 'https', ':', '//www.linkedin.com/in/karthik-ramanarayana-77b4091b8', 'EDUCATION', 'University', 'of', 'Maryland', ',', 'Baltimore', 'County', ',', 'Baltimore', ',', 'Maryland', 'Dec', '2021', '(', 'Expected', ')', 'Master', 'of', 'Professional', 'Studies', 'in', 'Data', 'Science', ',', 'GPA', ':', '3.74', 'Bangalore', 'Institute', 'of', 'Technology', ',', 'Bangalore', ',', 'India', 'May', '2016', 'Bachelor', 'of', 'Engineering', 'in', 'Electronics', 'and', 'Communication', 'Engineering', 'SKILLS', 'Programming', ':', 'C', ',', 'Python', ',', 'SQL', ',', 'PySpark', 'Database', ':', 'MySQL', ',', 'PostgreSQL', ',', 'MongoDB', ',', 'HBase', 'Data', 'Skills', ':', 'Data', 'Visualization', ',', 'Data', 'Modelling', ',', 'Data', 'normalization', ',', 'Machine', 'Learning', ',', 'Data', 'Mining', ',', 'Statistics', 'Big', 'Data', ':', 'Hadoop', ',', 'Spark', ',', 'Hive', ',', 'Kafka', ',', 'Airflow', 'BI', 'Tools', ':', 'Tableau', ',', 'Power', 'BI', 'AWS', ':', 'S3', ',', 'Redshift', ',', 'Glue', ',', 'Athena', ',', 'Redshift', 'Spectrum', ',', 'QuickSight', 'Libraries', ':', 'Numpy', ',', 'Pandas', ',', 'Spacy', ',', 'NLTK', ',', 'Scikit-Learn', ',', 'Keras', 'Tools', 'and', 'Software', ':', 'Git', ',', 'MS', 'Excel', ',', 'MS', 'PowerPoint', ',', 'MS', 'Word', ',', 'SharePoint', ',', 'Jupyter', ',', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', ',', 'Index', 'Analytics', 'LLC', ',', 'Baltimore', ',', 'MD', 'January', '2021', '–', 'May', '2021', '•', 'Designed', 'and', 'created', 'a', 'database', 'based', 'on', 'Star', 'Schema', 'dimensional', 'modelling', 'for', 'Employee', 'Skills', 'Matrix', 'data', '.', '•', 'Transformed', 'and', 'cleaned', 'raw', 'data', 'using', 'Python', 'to', 'create', 'fact', 'and', 'dimension', 'tables', '.', '•', 'Built', 'an', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'to', 'visualize', 'BI', 'Reports', 'of', 'Employee', 'Skills', 'data', ',', 'reducing', 'the', 'workload', 'by', '30', '%', '•', 'Led', 'and', 'managed', 'a', 'team', 'of', '3', 'by', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', '.', 'Junior', 'Data', 'Engineer', ',', 'Accenture', ',', 'Bangalore', ',', 'India', 'July', '2018—July', '2019', '•', 'Implemented', 'ELT', 'Data', 'Pipeline', 'to', 'examine', 'user', 'behavior', 'for', 'a', 'music', 'streaming', 'client', 'company', '.', '•', 'Utilized', 'PySpark', 'to', 'improve', 'data', 'ingestion', 'and', 'processing', 'speed', 'by', '20', '%', '.', '•', 'Ingested', 'data', 'from', 'versatile', 'data', 'sources', 'like', 'RDBMS', ',', 'NOSQL', ',', 'APIs', 'and', 'loaded', 'it', 'to', 'a', 'Data', 'Lake', ',', 'as', 'a', 'central', 'repository', '.', '•', 'Developed', 'data', 'mappings', 'and', 'data', 'modelling', 'to', 'create', 'OLAP', 'databases', 'for', 'analytics', ',', 'ensuring', 'robust', 'data', 'quality', '.', 'Associate', 'Software', 'Engineer', ',', 'Accenture', ',', 'Bangalore', ',', 'India', 'November', '2016—June', '2018', '•', 'Developed', 'and', 'implemented', 'ABAP', 'programs', 'to', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '.', '•', 'Devised', 'and', 'redesigned', 'custom', 'Reports', 'and', 'Smart', 'forms', 'which', 'enhanced', 'the', 'accuracy', 'of', 'the', 'ABAP', 'jobs', 'by', '30', '%', '.', '•', 'Identified', 'and', 'fixed', 'software', 'bugs', 'by', 'debugging', 'ABAP', 'programs', ',', 'decreasing', 'software', 'downtime', 'by', '15', '%', '.', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'of', 'Billboard', 'Top', '100', 'Songs', 'August', '-', 'October', '2020', '•', 'Collaborated', 'with', 'a', 'team', 'of', '5', 'to', 'analyze', 'the', 'mindset', 'of', 'each', 'year', 'using', 'Natural', 'language', 'Processing', 'of', 'song', 'lyrics', '.', '•', 'Language', ':', 'Python', ';', 'Libraries', ':', 'Pandas', ',', 'Gensim', ',', 'NLTK', ',', 'Spacy', '.', 'Movie', 'Recommendation', 'system', 'October', '-', 'November', '2020', '•', 'Recommended', 'new', 'movies', 'to', 'users', 'using', 'ALS', 'algorithm', 'with', 'a', 'root', 'mean', 'square', 'error', 'of', 'less', 'than', '0.9', '.', '•', 'Language', ':', 'Python', ';', 'Tools', ':', 'Apache', 'Spark', ';', 'Libraries', ':', 'Pandas', ',', 'Pyspark', '.', 'Human', 'Activity', 'Recognition', 'February', '–', 'March', '2019', '•', 'Designed', 'a', 'Machine', 'Learning', 'model', 'that', 'predicts', 'the', 'human', 'activities', 'such', 'as', 'Walking', ',', 'Walking', ',', 'etc.', ',', 'using', 'LSTM', 'algorithm', 'achieving', 'an', 'accuracy', 'of', '90.97', '%', '.', '•', 'Language', ':', 'Python', ';', 'Libraries', ':', 'Scikit-learn', ',', 'Keras', '.', 'LEADERSHIP/AWARDS', 'Mentor', ',', 'Accenture', 'June', '2018', 'Mentored', 'and', 'oversaw', 'a', 'team', 'of', '5', 'new', 'joiners', 'to', 'ensure', 'smooth', 'transition', 'to', 'the', 'team', ',', 'with', 'average', 'rating', 'of', '4.8', 'out', 'of', '5', '.', 'Alchemist', ',', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'for', 'best', 'work', 'efficiency', ',', 'out', 'of', '500', 'employees', '.'] ['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', 'Baltimore', 'MD', '21231', 'Contact', 'No', '410', '-292-1151', 'Email', 'ID', 'karthikr2194', 'gmail.com', 'LinkedIn', 'https', '//www.linkedin.com/in/karthik-ramanarayana-77b4091b8', 'EDUCATION', 'University', 'Maryland', 'Baltimore', 'County', 'Baltimore', 'Maryland', 'Dec', '2021', 'Expected', 'Master', 'Professional', 'Studies', 'Data', 'Science', 'GPA', '3.74', 'Bangalore', 'Institute', 'Technology', 'Bangalore', 'India', 'May', '2016', 'Bachelor', 'Engineering', 'Electronics', 'Communication', 'Engineering', 'SKILLS', 'Programming', 'C', 'Python', 'SQL', 'PySpark', 'Database', 'MySQL', 'PostgreSQL', 'MongoDB', 'HBase', 'Data', 'Skills', 'Data', 'Visualization', 'Data', 'Modelling', 'Data', 'normalization', 'Machine', 'Learning', 'Data', 'Mining', 'Statistics', 'Big', 'Data', 'Hadoop', 'Spark', 'Hive', 'Kafka', 'Airflow', 'BI', 'Tools', 'Tableau', 'Power', 'BI', 'AWS', 'S3', 'Redshift', 'Glue', 'Athena', 'Redshift', 'Spectrum', 'QuickSight', 'Libraries', 'Numpy', 'Pandas', 'Spacy', 'NLTK', 'Scikit-Learn', 'Keras', 'Tools', 'Software', 'Git', 'MS', 'Excel', 'MS', 'PowerPoint', 'MS', 'Word', 'SharePoint', 'Jupyter', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', 'Index', 'Analytics', 'LLC', 'Baltimore', 'MD', 'January', '2021', '–', 'May', '2021', '•', 'Designed', 'created', 'database', 'based', 'Star', 'Schema', 'dimensional', 'modelling', 'Employee', 'Skills', 'Matrix', 'data', '•', 'Transformed', 'cleaned', 'raw', 'data', 'using', 'Python', 'create', 'fact', 'dimension', 'tables', '•', 'Built', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'visualize', 'BI', 'Reports', 'Employee', 'Skills', 'data', 'reducing', 'workload', '30', '•', 'Led', 'managed', 'team', '3', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', 'Junior', 'Data', 'Engineer', 'Accenture', 'Bangalore', 'India', 'July', '2018—July', '2019', '•', 'Implemented', 'ELT', 'Data', 'Pipeline', 'examine', 'user', 'behavior', 'music', 'streaming', 'client', 'company', '•', 'Utilized', 'PySpark', 'improve', 'data', 'ingestion', 'processing', 'speed', '20', '•', 'Ingested', 'data', 'versatile', 'data', 'sources', 'like', 'RDBMS', 'NOSQL', 'APIs', 'loaded', 'Data', 'Lake', 'central', 'repository', '•', 'Developed', 'data', 'mappings', 'data', 'modelling', 'create', 'OLAP', 'databases', 'analytics', 'ensuring', 'robust', 'data', 'quality', 'Associate', 'Software', 'Engineer', 'Accenture', 'Bangalore', 'India', 'November', '2016—June', '2018', '•', 'Developed', 'implemented', 'ABAP', 'programs', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '•', 'Devised', 'redesigned', 'custom', 'Reports', 'Smart', 'forms', 'enhanced', 'accuracy', 'ABAP', 'jobs', '30', '•', 'Identified', 'fixed', 'software', 'bugs', 'debugging', 'ABAP', 'programs', 'decreasing', 'software', 'downtime', '15', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'Billboard', 'Top', '100', 'Songs', 'August', 'October', '2020', '•', 'Collaborated', 'team', '5', 'analyze', 'mindset', 'year', 'using', 'Natural', 'language', 'Processing', 'song', 'lyrics', '•', 'Language', 'Python', 'Libraries', 'Pandas', 'Gensim', 'NLTK', 'Spacy', 'Movie', 'Recommendation', 'system', 'October', 'November', '2020', '•', 'Recommended', 'new', 'movies', 'users', 'using', 'ALS', 'algorithm', 'root', 'mean', 'square', 'error', 'less', '0.9', '•', 'Language', 'Python', 'Tools', 'Apache', 'Spark', 'Libraries', 'Pandas', 'Pyspark', 'Human', 'Activity', 'Recognition', 'February', '–', 'March', '2019', '•', 'Designed', 'Machine', 'Learning', 'model', 'predicts', 'human', 'activities', 'Walking', 'Walking', 'etc.', 'using', 'LSTM', 'algorithm', 'achieving', 'accuracy', '90.97', '•', 'Language', 'Python', 'Libraries', 'Scikit-learn', 'Keras', 'LEADERSHIP/AWARDS', 'Mentor', 'Accenture', 'June', '2018', 'Mentored', 'oversaw', 'team', '5', 'new', 'joiners', 'ensure', 'smooth', 'transition', 'team', 'average', 'rating', '4.8', '5', 'Alchemist', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'best', 'work', 'efficiency', '500', 'employees'] ['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', 'Baltimore', 'MD', '21231', 'Contact', 'No', '410', '2921151', 'Email', 'ID', 'karthikr2194', 'gmailcom', 'LinkedIn', 'https', 'wwwlinkedincominkarthikramanarayana77b4091b8', 'EDUCATION', 'University', 'Maryland', 'Baltimore', 'County', 'Baltimore', 'Maryland', 'Dec', '2021', 'Expected', 'Master', 'Professional', 'Studies', 'Data', 'Science', 'GPA', '374', 'Bangalore', 'Institute', 'Technology', 'Bangalore', 'India', 'May', '2016', 'Bachelor', 'Engineering', 'Electronics', 'Communication', 'Engineering', 'SKILLS', 'Programming', 'C', 'Python', 'SQL', 'PySpark', 'Database', 'MySQL', 'PostgreSQL', 'MongoDB', 'HBase', 'Data', 'Skills', 'Data', 'Visualization', 'Data', 'Modelling', 'Data', 'normalization', 'Machine', 'Learning', 'Data', 'Mining', 'Statistics', 'Big', 'Data', 'Hadoop', 'Spark', 'Hive', 'Kafka', 'Airflow', 'BI', 'Tools', 'Tableau', 'Power', 'BI', 'AWS', 'S3', 'Redshift', 'Glue', 'Athena', 'Redshift', 'Spectrum', 'QuickSight', 'Libraries', 'Numpy', 'Pandas', 'Spacy', 'NLTK', 'ScikitLearn', 'Keras', 'Tools', 'Software', 'Git', 'MS', 'Excel', 'MS', 'PowerPoint', 'MS', 'Word', 'SharePoint', 'Jupyter', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', 'Index', 'Analytics', 'LLC', 'Baltimore', 'MD', 'January', '2021', '', 'May', '2021', '', 'Designed', 'created', 'database', 'based', 'Star', 'Schema', 'dimensional', 'modelling', 'Employee', 'Skills', 'Matrix', 'data', '', 'Transformed', 'cleaned', 'raw', 'data', 'using', 'Python', 'create', 'fact', 'dimension', 'tables', '', 'Built', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'visualize', 'BI', 'Reports', 'Employee', 'Skills', 'data', 'reducing', 'workload', '30', '', 'Led', 'managed', 'team', '3', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', 'Junior', 'Data', 'Engineer', 'Accenture', 'Bangalore', 'India', 'July', '2018July', '2019', '', 'Implemented', 'ELT', 'Data', 'Pipeline', 'examine', 'user', 'behavior', 'music', 'streaming', 'client', 'company', '', 'Utilized', 'PySpark', 'improve', 'data', 'ingestion', 'processing', 'speed', '20', '', 'Ingested', 'data', 'versatile', 'data', 'sources', 'like', 'RDBMS', 'NOSQL', 'APIs', 'loaded', 'Data', 'Lake', 'central', 'repository', '', 'Developed', 'data', 'mappings', 'data', 'modelling', 'create', 'OLAP', 'databases', 'analytics', 'ensuring', 'robust', 'data', 'quality', 'Associate', 'Software', 'Engineer', 'Accenture', 'Bangalore', 'India', 'November', '2016June', '2018', '', 'Developed', 'implemented', 'ABAP', 'programs', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '', 'Devised', 'redesigned', 'custom', 'Reports', 'Smart', 'forms', 'enhanced', 'accuracy', 'ABAP', 'jobs', '30', '', 'Identified', 'fixed', 'software', 'bugs', 'debugging', 'ABAP', 'programs', 'decreasing', 'software', 'downtime', '15', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'Billboard', 'Top', '100', 'Songs', 'August', 'October', '2020', '', 'Collaborated', 'team', '5', 'analyze', 'mindset', 'year', 'using', 'Natural', 'language', 'Processing', 'song', 'lyrics', '', 'Language', 'Python', 'Libraries', 'Pandas', 'Gensim', 'NLTK', 'Spacy', 'Movie', 'Recommendation', 'system', 'October', 'November', '2020', '', 'Recommended', 'new', 'movies', 'users', 'using', 'ALS', 'algorithm', 'root', 'mean', 'square', 'error', 'less', '09', '', 'Language', 'Python', 'Tools', 'Apache', 'Spark', 'Libraries', 'Pandas', 'Pyspark', 'Human', 'Activity', 'Recognition', 'February', '', 'March', '2019', '', 'Designed', 'Machine', 'Learning', 'model', 'predicts', 'human', 'activities', 'Walking', 'Walking', 'etc', 'using', 'LSTM', 'algorithm', 'achieving', 'accuracy', '9097', '', 'Language', 'Python', 'Libraries', 'Scikitlearn', 'Keras', 'LEADERSHIPAWARDS', 'Mentor', 'Accenture', 'June', '2018', 'Mentored', 'oversaw', 'team', '5', 'new', 'joiners', 'ensure', 'smooth', 'transition', 'team', 'average', 'rating', '48', '5', 'Alchemist', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'best', 'work', 'efficiency', '500', 'employees'] ['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', 'Baltimore', 'MD', '21231', 'Contact', 'No', '410', '2921151', 'Email', 'ID', 'karthikr2194', 'gmailcom', 'LinkedIn', 'https', 'wwwlinkedincominkarthikramanarayana77b4091b8', 'EDUCATION', 'University', 'Maryland', 'Baltimore', 'County', 'Baltimore', 'Maryland', 'Dec', '2021', 'Expected', 'Master', 'Professional', 'Studies', 'Data', 'Science', 'GPA', '374', 'Bangalore', 'Institute', 'Technology', 'Bangalore', 'India', 'May', '2016', 'Bachelor', 'Engineering', 'Electronics', 'Communication', 'Engineering', 'SKILLS', 'Programming', 'C', 'Python', 'SQL', 'PySpark', 'Database', 'MySQL', 'PostgreSQL', 'MongoDB', 'HBase', 'Data', 'Skills', 'Data', 'Visualization', 'Data', 'Modelling', 'Data', 'normalization', 'Machine', 'Learning', 'Data', 'Mining', 'Statistics', 'Big', 'Data', 'Hadoop', 'Spark', 'Hive', 'Kafka', 'Airflow', 'BI', 'Tools', 'Tableau', 'Power', 'BI', 'AWS', 'S3', 'Redshift', 'Glue', 'Athena', 'Redshift', 'Spectrum', 'QuickSight', 'Libraries', 'Numpy', 'Pandas', 'Spacy', 'NLTK', 'ScikitLearn', 'Keras', 'Tools', 'Software', 'Git', 'MS', 'Excel', 'MS', 'PowerPoint', 'MS', 'Word', 'SharePoint', 'Jupyter', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', 'Index', 'Analytics', 'LLC', 'Baltimore', 'MD', 'January', '2021', '', 'May', '2021', '', 'Designed', 'created', 'database', 'based', 'Star', 'Schema', 'dimensional', 'modelling', 'Employee', 'Skills', 'Matrix', 'data', '', 'Transformed', 'cleaned', 'raw', 'data', 'using', 'Python', 'create', 'fact', 'dimension', 'tables', '', 'Built', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'visualize', 'BI', 'Reports', 'Employee', 'Skills', 'data', 'reducing', 'workload', '30', '', 'Led', 'managed', 'team', '3', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', 'Junior', 'Data', 'Engineer', 'Accenture', 'Bangalore', 'India', 'July', '2018July', '2019', '', 'Implemented', 'ELT', 'Data', 'Pipeline', 'examine', 'user', 'behavior', 'music', 'streaming', 'client', 'company', '', 'Utilized', 'PySpark', 'improve', 'data', 'ingestion', 'processing', 'speed', '20', '', 'Ingested', 'data', 'versatile', 'data', 'sources', 'like', 'RDBMS', 'NOSQL', 'APIs', 'loaded', 'Data', 'Lake', 'central', 'repository', '', 'Developed', 'data', 'mappings', 'data', 'modelling', 'create', 'OLAP', 'databases', 'analytics', 'ensuring', 'robust', 'data', 'quality', 'Associate', 'Software', 'Engineer', 'Accenture', 'Bangalore', 'India', 'November', '2016June', '2018', '', 'Developed', 'implemented', 'ABAP', 'programs', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '', 'Devised', 'redesigned', 'custom', 'Reports', 'Smart', 'forms', 'enhanced', 'accuracy', 'ABAP', 'jobs', '30', '', 'Identified', 'fixed', 'software', 'bugs', 'debugging', 'ABAP', 'programs', 'decreasing', 'software', 'downtime', '15', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'Billboard', 'Top', '100', 'Songs', 'August', 'October', '2020', '', 'Collaborated', 'team', '5', 'analyze', 'mindset', 'year', 'using', 'Natural', 'language', 'Processing', 'song', 'lyrics', '', 'Language', 'Python', 'Libraries', 'Pandas', 'Gensim', 'NLTK', 'Spacy', 'Movie', 'Recommendation', 'system', 'October', 'November', '2020', '', 'Recommended', 'new', 'movies', 'users', 'using', 'ALS', 'algorithm', 'root', 'mean', 'square', 'error', 'less', '09', '', 'Language', 'Python', 'Tools', 'Apache', 'Spark', 'Libraries', 'Pandas', 'Pyspark', 'Human', 'Activity', 'Recognition', 'February', '', 'March', '2019', '', 'Designed', 'Machine', 'Learning', 'model', 'predicts', 'human', 'activities', 'Walking', 'Walking', 'etc', 'using', 'LSTM', 'algorithm', 'achieving', 'accuracy', '9097', '', 'Language', 'Python', 'Libraries', 'Scikitlearn', 'Keras', 'LEADERSHIPAWARDS', 'Mentor', 'Accenture', 'June', '2018', 'Mentored', 'oversaw', 'team', '5', 'new', 'joiners', 'ensure', 'smooth', 'transition', 'team', 'average', 'rating', '48', '5', 'Alchemist', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'best', 'work', 'efficiency', '500', 'employees'] ['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', 'Baltimore', 'MD', '21231', 'Contact', 'No', '410', '2921151', 'Email', 'ID', 'karthikr2194', 'gmailcom', 'LinkedIn', 'https', 'wwwlinkedincominkarthikramanarayana77b4091b8', 'EDUCATION', 'University', 'Maryland', 'Baltimore', 'County', 'Baltimore', 'Maryland', 'Dec', '2021', 'Expected', 'Master', 'Professional', 'Studies', 'Data', 'Science', 'GPA', '374', 'Bangalore', 'Institute', 'Technology', 'Bangalore', 'India', 'May', '2016', 'Bachelor', 'Engineering', 'Electronics', 'Communication', 'Engineering', 'SKILLS', 'Programming', 'C', 'Python', 'SQL', 'PySpark', 'Database', 'MySQL', 'PostgreSQL', 'MongoDB', 'HBase', 'Data', 'Skills', 'Data', 'Visualization', 'Data', 'Modelling', 'Data', 'normalization', 'Machine', 'Learning', 'Data', 'Mining', 'Statistics', 'Big', 'Data', 'Hadoop', 'Spark', 'Hive', 'Kafka', 'Airflow', 'BI', 'Tools', 'Tableau', 'Power', 'BI', 'AWS', 'S3', 'Redshift', 'Glue', 'Athena', 'Redshift', 'Spectrum', 'QuickSight', 'Libraries', 'Numpy', 'Pandas', 'Spacy', 'NLTK', 'ScikitLearn', 'Keras', 'Tools', 'Software', 'Git', 'MS', 'Excel', 'MS', 'PowerPoint', 'MS', 'Word', 'SharePoint', 'Jupyter', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', 'Index', 'Analytics', 'LLC', 'Baltimore', 'MD', 'January', '2021', '', 'May', '2021', '', 'Designed', 'created', 'database', 'based', 'Star', 'Schema', 'dimensional', 'modelling', 'Employee', 'Skills', 'Matrix', 'data', '', 'Transformed', 'cleaned', 'raw', 'data', 'using', 'Python', 'create', 'fact', 'dimension', 'tables', '', 'Built', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'visualize', 'BI', 'Reports', 'Employee', 'Skills', 'data', 'reducing', 'workload', '30', '', 'Led', 'managed', 'team', '3', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', 'Junior', 'Data', 'Engineer', 'Accenture', 'Bangalore', 'India', 'July', '2018July', '2019', '', 'Implemented', 'ELT', 'Data', 'Pipeline', 'examine', 'user', 'behavior', 'music', 'streaming', 'client', 'company', '', 'Utilized', 'PySpark', 'improve', 'data', 'ingestion', 'processing', 'speed', '20', '', 'Ingested', 'data', 'versatile', 'data', 'sources', 'like', 'RDBMS', 'NOSQL', 'APIs', 'loaded', 'Data', 'Lake', 'central', 'repository', '', 'Developed', 'data', 'mappings', 'data', 'modelling', 'create', 'OLAP', 'databases', 'analytics', 'ensuring', 'robust', 'data', 'quality', 'Associate', 'Software', 'Engineer', 'Accenture', 'Bangalore', 'India', 'November', '2016June', '2018', '', 'Developed', 'implemented', 'ABAP', 'programs', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '', 'Devised', 'redesigned', 'custom', 'Reports', 'Smart', 'forms', 'enhanced', 'accuracy', 'ABAP', 'jobs', '30', '', 'Identified', 'fixed', 'software', 'bugs', 'debugging', 'ABAP', 'programs', 'decreasing', 'software', 'downtime', '15', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'Billboard', 'Top', '100', 'Songs', 'August', 'October', '2020', '', 'Collaborated', 'team', '5', 'analyze', 'mindset', 'year', 'using', 'Natural', 'language', 'Processing', 'song', 'lyrics', '', 'Language', 'Python', 'Libraries', 'Pandas', 'Gensim', 'NLTK', 'Spacy', 'Movie', 'Recommendation', 'system', 'October', 'November', '2020', '', 'Recommended', 'new', 'movies', 'users', 'using', 'ALS', 'algorithm', 'root', 'mean', 'square', 'error', 'less', '09', '', 'Language', 'Python', 'Tools', 'Apache', 'Spark', 'Libraries', 'Pandas', 'Pyspark', 'Human', 'Activity', 'Recognition', 'February', '', 'March', '2019', '', 'Designed', 'Machine', 'Learning', 'model', 'predicts', 'human', 'activities', 'Walking', 'Walking', 'etc', 'using', 'LSTM', 'algorithm', 'achieving', 'accuracy', '9097', '', 'Language', 'Python', 'Libraries', 'Scikitlearn', 'Keras', 'LEADERSHIPAWARDS', 'Mentor', 'Accenture', 'June', '2018', 'Mentored', 'oversaw', 'team', '5', 'new', 'joiners', 'ensure', 'smooth', 'transition', 'team', 'average', 'rating', '48', '5', 'Alchemist', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'best', 'work', 'efficiency', '500', 'employees'] ['Karthik', 'Ramanarayana', '2001', 'Eastern', 'Avenue', 'Baltimore', 'MD', '21231', 'Contact', 'No', '410', '2921151', 'Email', 'ID', 'karthikr2194', 'gmailcom', 'LinkedIn', 'https', 'wwwlinkedincominkarthikramanarayana77b4091b8', 'EDUCATION', 'University', 'Maryland', 'Baltimore', 'County', 'Baltimore', 'Maryland', 'Dec', '2021', 'Expected', 'Master', 'Professional', 'Studies', 'Data', 'Science', 'GPA', '374', 'Bangalore', 'Institute', 'Technology', 'Bangalore', 'India', 'May', '2016', 'Bachelor', 'Engineering', 'Electronics', 'Communication', 'Engineering', 'SKILLS', 'Programming', 'C', 'Python', 'SQL', 'PySpark', 'Database', 'MySQL', 'PostgreSQL', 'MongoDB', 'HBase', 'Data', 'Skills', 'Data', 'Visualization', 'Data', 'Modelling', 'Data', 'normalization', 'Machine', 'Learning', 'Data', 'Mining', 'Statistics', 'Big', 'Data', 'Hadoop', 'Spark', 'Hive', 'Kafka', 'Airflow', 'BI', 'Tools', 'Tableau', 'Power', 'BI', 'AWS', 'S3', 'Redshift', 'Glue', 'Athena', 'Redshift', 'Spectrum', 'QuickSight', 'Libraries', 'Numpy', 'Pandas', 'Spacy', 'NLTK', 'ScikitLearn', 'Keras', 'Tools', 'Software', 'Git', 'MS', 'Excel', 'MS', 'PowerPoint', 'MS', 'Word', 'SharePoint', 'Jupyter', 'Jira', 'WORK', 'EXPERIENCE', 'Data', 'Intern', 'Index', 'Analytics', 'LLC', 'Baltimore', 'MD', 'January', '2021', '', 'May', '2021', '', 'Designed', 'created', 'database', 'based', 'Star', 'Schema', 'dimensional', 'modelling', 'Employee', 'Skills', 'Matrix', 'data', '', 'Transformed', 'cleaned', 'raw', 'data', 'using', 'Python', 'create', 'fact', 'dimension', 'tables', '', 'Built', 'executive', 'level', 'Power', 'BI', 'Dashboard', 'visualize', 'BI', 'Reports', 'Employee', 'Skills', 'data', 'reducing', 'workload', '30', '', 'Led', 'managed', 'team', '3', 'following', 'Agile', 'methodology', 'using', 'Jira', 'tool', 'Junior', 'Data', 'Engineer', 'Accenture', 'Bangalore', 'India', 'July', '2018July', '2019', '', 'Implemented', 'ELT', 'Data', 'Pipeline', 'examine', 'user', 'behavior', 'music', 'streaming', 'client', 'company', '', 'Utilized', 'PySpark', 'improve', 'data', 'ingestion', 'processing', 'speed', '20', '', 'Ingested', 'data', 'versatile', 'data', 'sources', 'like', 'RDBMS', 'NOSQL', 'APIs', 'loaded', 'Data', 'Lake', 'central', 'repository', '', 'Developed', 'data', 'mappings', 'data', 'modelling', 'create', 'OLAP', 'databases', 'analytics', 'ensuring', 'robust', 'data', 'quality', 'Associate', 'Software', 'Engineer', 'Accenture', 'Bangalore', 'India', 'November', '2016June', '2018', '', 'Developed', 'implemented', 'ABAP', 'programs', 'customize', 'SAP', 'transactions', 'using', 'OOPS', 'concepts', '', 'Devised', 'redesigned', 'custom', 'Reports', 'Smart', 'forms', 'enhanced', 'accuracy', 'ABAP', 'jobs', '30', '', 'Identified', 'fixed', 'software', 'bugs', 'debugging', 'ABAP', 'programs', 'decreasing', 'software', 'downtime', '15', 'ACADEMIC', 'PROJECTS', 'Sentiment', 'Analysis', 'Billboard', 'Top', '100', 'Songs', 'August', 'October', '2020', '', 'Collaborated', 'team', '5', 'analyze', 'mindset', 'year', 'using', 'Natural', 'language', 'Processing', 'song', 'lyrics', '', 'Language', 'Python', 'Libraries', 'Pandas', 'Gensim', 'NLTK', 'Spacy', 'Movie', 'Recommendation', 'system', 'October', 'November', '2020', '', 'Recommended', 'new', 'movies', 'users', 'using', 'ALS', 'algorithm', 'root', 'mean', 'square', 'error', 'less', '09', '', 'Language', 'Python', 'Tools', 'Apache', 'Spark', 'Libraries', 'Pandas', 'Pyspark', 'Human', 'Activity', 'Recognition', 'February', '', 'March', '2019', '', 'Designed', 'Machine', 'Learning', 'model', 'predicts', 'human', 'activities', 'Walking', 'Walking', 'etc', 'using', 'LSTM', 'algorithm', 'achieving', 'accuracy', '9097', '', 'Language', 'Python', 'Libraries', 'Scikitlearn', 'Keras', 'LEADERSHIPAWARDS', 'Mentor', 'Accenture', 'June', '2018', 'Mentored', 'oversaw', 'team', '5', 'new', 'joiners', 'ensure', 'smooth', 'transition', 'team', 'average', 'rating', '48', '5', 'Alchemist', 'Accenture', 'March', '2019', 'Awarded', '1st', 'prize', 'best', 'work', 'efficiency', '500', 'employees']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print(words,simple_strings,res,res1,res2,res3)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Indian Last names\n",
    ">Source: https://www.momjunction.com/articles/popular-indian-last-names-for-your-baby_00334734/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "\"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "\"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "\"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "\"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "\"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of chinese last names\n",
    ">Source: \"https://mandarinhouse.com/100-common-chinese-family-names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "\"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "\"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "\"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "\"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "\"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "\"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "    'University'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(i):\n",
    "    ###Extract text from files\n",
    "    try:\n",
    "        with pdfplumber.open(files1[i]) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            txt1 = first_page.extract_text()\n",
    "    except:\n",
    "        txt1=\"\"\n",
    "        i+=1\n",
    "        tokenize(i)\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    #print(txt)\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i,txt,txt1\n",
    "\n",
    "\n",
    "def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "    ##Here i is for iterating over the files and doing the process for each file\n",
    "    flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "    words,i,txt,txt1 = tokenize(i)\n",
    "    #print(txt)\n",
    "    ##removing stopwords and punctuations\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "    ##removing some ascii character\n",
    "    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "    ##removing numbers\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "    ##trying to remove acii characters\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "    ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "    tagged = tagger.tag(res3)\n",
    "    \n",
    "    ## appending the names extracted from name_person\n",
    "    ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "    try:\n",
    "        person.append(name_person(tagged,words,res3))\n",
    "    except:\n",
    "        flag_success = 0\n",
    "        person.append(files1[i])\n",
    "        pass\n",
    "    #tagged_list.append(tagged) \n",
    "    flag_success = 1\n",
    "    if flag_success == 1:\n",
    "        education.append(extract_education(txt))\n",
    "        graduation_year.append(extract_graduation_date(txt1))\n",
    "        phone_nbr.append(extract_phone_number(txt))\n",
    "        emails.append(extract_emails(txt))\n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i, person)\n",
    "    \n",
    "    #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "    return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "\n",
    "def name_person(tagged, words,res3):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    #print(tagged_list)\n",
    "    person = []\n",
    "    temp_person = []\n",
    "    \n",
    "    for k in range(10):\n",
    "        if res3[k].lower() in indian_last_names or res3[k].lower() in chinese_last_names:\n",
    "            j = k-1\n",
    "            return res3[j]#+\"(Extracted using 1st approach)\"\n",
    "    for tuple_ele in tagged:\n",
    "        if \"PERSON\" in tuple_ele[1]:\n",
    "            temp_person.append(tuple_ele[0])\n",
    "    ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "    for word in words:\n",
    "        if word in temp_person:\n",
    "            return word\n",
    "\n",
    "def extract_education(txt):\n",
    "    edu=set()\n",
    "    p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "    for m in re.finditer(p,txt):\n",
    "        try:\n",
    "            if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "                edu.append(m.group(1))\n",
    "        except:\n",
    "            pass\n",
    "        for word in RESERVED_WORDS:\n",
    "            if word in m.group(2):\n",
    "                edu.add(m.group(2))\n",
    "    return edu\n",
    "\n",
    "def extract_graduation_date(txt1):\n",
    "    dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "    # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "    x=\"(?is)education.*?(\\d{4})\"\n",
    "    # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "    if len(re.findall(x,txt1))==0:\n",
    "        return None\n",
    "    return max(re.findall(x,txt1))\n",
    "    ## for dates\n",
    "    #for dt in dates:\n",
    "    #    if dt in txt:\n",
    "    #        return dt\n",
    "\n",
    "def extract_phone_number(txt):\n",
    "    PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if txt.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "def extract_emails(txt):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing().to_csv(\"Extraction_Resume_Output1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Changed the approach of extracting names in this way:***\n",
    "- If your last name is in the resume in the top 10 names then your first name (the previous word will be returned)\n",
    "- Otherwise StanfordNER algorithm will be applied to extract names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Idea: If the extracted name is a skill like Python/Django/Swift then simply return 1st word of the resume\n",
    "(50% of success of getting the name)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karthik Ramanarayana \\n2001 Eastern Avenue, Baltimore, MD 21231 \\nContact No: (410)-292-1151; Email ID: karthikr2194@gmail.com \\nLinkedIn: https://www.linkedin.com/in/karthik-ramanarayana-77b4091b8 \\n \\nEDUCATION  \\nUniversity of Maryland, Baltimore County, Baltimore, Maryland       Dec 2021 (Expected)   \\nMaster of Professional Studies in Data Science, GPA: 3.74          \\nBangalore Institute of Technology, Bangalore, India          May 2016 \\nBachelor of Engineering in Electronics and Communication Engineering       \\n \\nSKILLS \\n \\nProgramming:    C, Python, SQL, PySpark \\nDatabase:      MySQL, PostgreSQL, MongoDB, HBase \\nData Skills:   Data Visualization, Data Modelling, Data normalization, Machine Learning, Data Mining, Statistics \\nBig Data:     Hadoop, Spark, Hive, Kafka, Airflow \\nBI Tools:      Tableau, Power BI \\nAWS:       S3, Redshift, Glue, Athena, Redshift Spectrum, QuickSight \\nLibraries:    Numpy, Pandas, Spacy, NLTK, Scikit-Learn, Keras \\nTools and Software:   Git, MS Excel, MS PowerPoint, MS Word, SharePoint, Jupyter, Jira \\n \\nWORK EXPERIENCE \\nData Intern, Index Analytics LLC, Baltimore, MD                              January 2021 – May 2021 \\n•  Designed and created a database based on Star Schema dimensional modelling for Employee Skills Matrix data. \\n•  Transformed and cleaned raw data using Python to create fact and dimension tables. \\n•  Built an executive level Power BI Dashboard to visualize BI Reports of Employee Skills data, reducing the workload by 30% \\n•  Led and managed a team of 3 by following Agile methodology using Jira tool. \\nJunior Data Engineer, Accenture, Bangalore, India           July 2018—July 2019 \\n•  Implemented ELT Data Pipeline to examine user behavior for a music streaming client company.  \\n•  Utilized PySpark to improve data ingestion and processing speed by 20%. \\n•  Ingested data from versatile data sources like RDBMS, NOSQL, APIs and loaded it to a Data Lake, as a central repository. \\n•  Developed data mappings and data modelling to create OLAP databases for analytics, ensuring robust data quality. \\nAssociate Software Engineer, Accenture, Bangalore, India        November 2016—June 2018 \\n•  Developed and implemented ABAP programs to customize SAP transactions using OOPS concepts. \\n•  Devised and redesigned custom Reports and Smart forms which enhanced the accuracy of the ABAP jobs by 30%. \\n•  Identified and fixed software bugs by debugging ABAP programs, decreasing software downtime by 15%. \\n \\nACADEMIC PROJECTS \\nSentiment Analysis of Billboard Top 100 Songs           August - October 2020                                                                         \\n•  Collaborated with a team of 5 to analyze the mindset of each year using Natural language Processing of song lyrics.  \\n•  Language: Python; Libraries: Pandas, Gensim, NLTK, Spacy. \\nMovie Recommendation system               October - November 2020                                                                                    \\n•  Recommended new movies to users using ALS algorithm with a root mean square error of less than 0.9.  \\n•  Language: Python; Tools: Apache Spark; Libraries: Pandas, Pyspark. \\nHuman Activity Recognition                 February – March 2019 \\n•  Designed a Machine Learning model that predicts the human activities such as Walking, Walking, etc., using LSTM \\nalgorithm achieving an accuracy of 90.97%.  \\n•  Language: Python; Libraries: Scikit-learn, Keras. \\n \\nLEADERSHIP/AWARDS \\nMentor, Accenture                   June 2018 \\nMentored and oversaw a team of 5 new joiners to ensure smooth transition to the team, with average rating of 4.8 out of 5. \\nAlchemist, Accenture                   March 2019  \\nAwarded 1st prize for best work efficiency, out of 500 employees. '"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pdfplumber.open(files1[0]) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    txt = first_page.extract_text()\n",
    "    \n",
    "line1 = 'EDUCATION\\nThe University of Texas at Dallas, Dallas, TX.'\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"p = re.compile('(.*?),\\s+(.*?),(.*?)') \n",
    "with pdfplumber.open(files1[8]) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    txt = first_page.extract_text()\n",
    "line1 = \"Robert H. Smith School of Business, University of Maryland, College Park, MD\"\n",
    "\n",
    "#p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "for m in re.finditer(p,txt):\n",
    "    flag=0\n",
    "    for word in RESERVED_WORDS:\n",
    "        try:\n",
    "            if word in m.group(1):\n",
    "                print(m.group(1))\n",
    "                flag+=1\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if word in m.group(2):\n",
    "                print(m.group(2))\n",
    "                flag+=1\n",
    "        except:\n",
    "            pass\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 of the failed examples:\n",
    "- Here all the words including names are not recognized as \"PERSON\" and instead recognized as something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RACHAN', 'O'), ('VAMSI', 'O'), ('BHOOSHI', 'O'), ('Prospect', 'O'), ('Street', 'O'), ('APT', 'O'), ('N', 'O'), ('Stamford', 'LOCATION'), ('CT', 'O'), ('rachan_vamsibhooshi', 'O'), ('uconnedu', 'O'), ('LinkedIn', 'O'), ('Zoom', 'O'), ('GitHub', 'O'), ('SUMMARY', 'O'), ('Worked', 'O'), ('Graduate', 'O'), ('Research', 'O'), ('Assistant', 'O'), ('University', 'O'), ('Connecticut', 'O'), ('Eversource', 'O'), ('I', 'O'), ('supported', 'O'), ('research', 'O'), ('scheduling', 'O'), ('vehicle', 'O'), ('routing', 'O'), ('optimization', 'O'), ('prescriptive', 'O'), ('modeling', 'O'), ('Strategic', 'O'), ('innovative', 'O'), ('resultsdriven', 'O'), ('professional', 'O'), ('expertise', 'O'), ('implementing', 'O'), ('big', 'O'), ('data', 'O'), ('analytics', 'O'), ('strong', 'O'), ('leadership', 'O'), ('communication', 'O'), ('skills', 'O'), ('EDUCATION', 'O'), ('Master', 'O'), ('Science', 'O'), ('Business', 'O'), ('Analytics', 'O'), ('Project', 'O'), ('Management', 'O'), ('December', 'O'), ('UNIVERSITY', 'O'), ('OF', 'O'), ('CONNECTICUT', 'O'), ('GPA', 'O'), ('years', 'O'), ('work', 'O'), ('authorization', 'O'), ('STEMextension', 'O'), ('OPT', 'O'), ('Bachelor', 'O'), ('Technology', 'O'), ('Mechanical', 'O'), ('Engineering', 'O'), ('May', 'O'), ('NATIONAL', 'O'), ('INSTITUTE', 'O'), ('OF', 'O'), ('TECHNOLOGY', 'O'), ('ROURKELA', 'O'), ('SKILLS', 'O'), ('Programming', 'O'), ('Language', 'O'), ('Tools', 'O'), ('Python', 'O'), ('R', 'O'), ('Google', 'ORGANIZATION'), ('Analytics', 'ORGANIZATION'), ('GitHub', 'O'), ('Docker', 'O'), ('Kubernetes', 'O'), ('Hadoop', 'O'), ('SQL', 'O'), ('Apache', 'O'), ('Airflow', 'O'), ('SAS', 'O'), ('programming', 'O'), ('JMP', 'ORGANIZATION'), ('Alteryx', 'ORGANIZATION'), ('Tableau', 'ORGANIZATION'), ('Power', 'ORGANIZATION'), ('BI', 'ORGANIZATION'), ('Apache', 'ORGANIZATION'), ('Spark', 'ORGANIZATION'), ('Snowflake', 'ORGANIZATION'), ('MS', 'ORGANIZATION'), ('Excel', 'ORGANIZATION'), ('VLOOKUP', 'ORGANIZATION'), ('Pivot', 'ORGANIZATION'), ('Tables', 'ORGANIZATION'), ('MS', 'ORGANIZATION'), ('Office', 'ORGANIZATION'), ('MS', 'ORGANIZATION'), ('Access', 'ORGANIZATION'), ('MS', 'ORGANIZATION'), ('Visio', 'ORGANIZATION'), ('HTML', 'O'), ('Google', 'ORGANIZATION'), ('Cloud', 'O'), ('Platform', 'O'), ('Tools', 'O'), ('Kubernetes', 'O'), ('Engine', 'O'), ('Compute', 'O'), ('Engine', 'O'), ('Big', 'O'), ('Query', 'O'), ('Cloud', 'O'), ('Pub', 'O'), ('Sub', 'ORGANIZATION'), ('Cloud', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Flow', 'ORGANIZATION'), ('Cloud', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Fusion', 'ORGANIZATION'), ('Cloud', 'O'), ('Composer', 'O'), ('Cloud', 'O'), ('Functions', 'O'), ('Stack', 'O'), ('driver', 'O'), ('Google', 'ORGANIZATION'), ('APIs', 'O'), ('Statistical', 'O'), ('ML', 'O'), ('Techniques', 'O'), ('Managerial', 'O'), ('Decision', 'O'), ('Modeling', 'O'), ('Prescriptive', 'O'), ('Analysis', 'O'), ('Descriptive', 'O'), ('Analysis', 'O'), ('Supervised', 'O'), ('Unsupervised', 'O'), ('Machine', 'O'), ('Learning', 'O'), ('Regression', 'O'), ('Classification', 'O'), ('Decision', 'O'), ('Trees', 'O'), ('Random', 'O'), ('Forest', 'O'), ('Principal', 'O'), ('Component', 'O'), ('Analysis', 'O'), ('Clustering', 'O'), ('Text', 'O'), ('Analysis', 'O'), ('Pivot', 'O'), ('tables', 'O'), ('Time', 'ORGANIZATION'), ('Series', 'ORGANIZATION'), ('Forecasting', 'ORGANIZATION'), ('Business', 'ORGANIZATION'), ('Intelligence', 'ORGANIZATION'), ('Convolution', 'ORGANIZATION'), ('Neural', 'ORGANIZATION'), ('Networks', 'ORGANIZATION'), ('Recurrent', 'ORGANIZATION'), ('Neural', 'ORGANIZATION'), ('Network', 'ORGANIZATION'), ('LSTM', 'ORGANIZATION'), ('Modeling', 'ORGANIZATION'), ('Autoencoders', 'ORGANIZATION'), ('AB', 'ORGANIZATION'), ('Testing', 'ORGANIZATION'), ('Association', 'ORGANIZATION'), ('Rule', 'ORGANIZATION'), ('Mining', 'ORGANIZATION'), ('Web', 'ORGANIZATION'), ('Scraping', 'ORGANIZATION'), ('Version', 'ORGANIZATION'), ('Control', 'ORGANIZATION'), ('ETL', 'ORGANIZATION'), ('ELT', 'ORGANIZATION'), ('processing', 'O'), ('parallel', 'O'), ('processing', 'O'), ('data', 'O'), ('pipelines', 'O'), ('deployment', 'O'), ('Route', 'O'), ('optimization', 'O'), ('work', 'O'), ('scheduling', 'O'), ('EXPERIENCE', 'O'), ('UNIVERSITY', 'O'), ('OF', 'O'), ('CONNECTICUT', 'O'), ('Stamford', 'LOCATION'), ('CT', 'O'), ('June', 'O'), ('Present', 'O'), ('Graduate', 'O'), ('Research', 'O'), ('Assistant', 'O'), ('Develop', 'O'), ('implement', 'O'), ('code', 'O'), ('TSP', 'O'), ('Work', 'O'), ('Scheduling', 'O'), ('Problems', 'O'), ('Python', 'PERSON'), ('using', 'O'), ('optimization', 'O'), ('Design', 'O'), ('test', 'O'), ('algorithms', 'O'), ('strategies', 'O'), ('decrease', 'O'), ('costs', 'O'), ('million', 'O'), ('dollars', 'O'), ('Eversource', 'O'), ('USA', 'LOCATION'), ('decrease', 'O'), ('total', 'O'), ('restoration', 'O'), ('time', 'O'), ('days', 'O'), ('increase', 'O'), ('work', 'O'), ('efficiency', 'O'), ('using', 'O'), ('KPIs', 'O'), ('OTHER', 'O'), ('EXPERIENCES', 'O'), ('CONVERGENCE', 'O'), ('INC', 'O'), ('Norwalk', 'LOCATION'), ('CT', 'O'), ('May', 'O'), ('July', 'O'), ('Analyst', 'O'), ('Intern', 'O'), ('Designed', 'O'), ('Created', 'O'), ('Manager', 'O'), ('Affiliation', 'O'), ('model', 'O'), ('ingestion', 'O'), ('process', 'O'), ('Advisors', 'O'), ('Big', 'O'), ('Query', 'O'), ('Snowflake', 'O'), ('Schema', 'O'), ('Developed', 'O'), ('performed', 'O'), ('unit', 'O'), ('testing', 'O'), ('ELT', 'O'), ('data', 'O'), ('pipelines', 'O'), ('improve', 'O'), ('risk', 'O'), ('identification', 'O'), ('UNIVERSITY', 'O'), ('OF', 'O'), ('CONNECTICUT', 'O'), ('Stamford', 'LOCATION'), ('CT', 'O'), ('January', 'O'), ('Present', 'O'), ('Teaching', 'O'), ('Assistant', 'O'), ('Business', 'O'), ('Decision', 'O'), ('Modeling', 'O'), ('Guide', 'O'), ('students', 'O'), ('understanding', 'O'), ('concepts', 'O'), ('Managerial', 'O'), ('Decision', 'O'), ('Modeling', 'O'), ('Develop', 'O'), ('code', 'O'), ('real', 'O'), ('world', 'O'), ('problems', 'O'), ('Operational', 'O'), ('Research', 'O'), ('leveraging', 'O'), ('environmental', 'O'), ('factors', 'O'), ('ANALYTICS', 'O'), ('ACADEMIC', 'O'), ('PROJECTS', 'O'), ('Realtime', 'O'), ('Stock', 'O'), ('Market', 'O'), ('Prediction', 'O'), ('voice', 'O'), ('input', 'O'), ('Python', 'O'), ('NLTK', 'O'), ('Google', 'ORGANIZATION'), ('Speech', 'O'), ('Text', 'O'), ('API', 'O'), ('pandas', 'O'), ('ScikitLearn', 'O'), ('TensorFlow', 'O'), ('Google', 'ORGANIZATION'), ('Natural', 'O'), ('Language', 'O'), ('Processing', 'O'), ('API', 'O'), ('Emotion', 'O'), ('Recognition', 'O'), ('Project', 'O'), ('accuracy', 'O'), ('improve', 'O'), ('productivity', 'O'), ('health', 'O'), ('work', 'O'), ('Python', 'O'), ('Sentiment', 'O'), ('Analysis', 'O'), ('Deep', 'O'), ('Learning', 'O'), ('Model', 'O'), ('TensorFlow', 'O'), ('Serving', 'O'), ('TensorFlow', 'O'), ('Python', 'O'), ('Docker', 'O'), ('Tweet', 'O'), ('Classification', 'O'), ('Model', 'O'), ('predicting', 'O'), ('tweets', 'O'), ('Messi', 'PERSON'), ('Ronaldo', 'PERSON'), ('fans', 'O'), ('TensorFlow', 'O'), ('Python', 'O'), ('Travel', 'O'), ('Optimization', 'O'), ('using', 'O'), ('ORTools', 'O'), ('different', 'O'), ('route', 'O'), ('strategies', 'O'), ('travel', 'O'), ('parameters', 'O'), ('Python', 'O'), ('Market', 'O'), ('Basket', 'O'), ('Analysis', 'O'), ('Gas', 'O'), ('prices', 'O'), ('CO', 'O'), ('levels', 'O'), ('COVID', 'O'), ('cases', 'O'), ('decrease', 'O'), ('travel', 'O'), ('costs', 'O'), ('R', 'O')]\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "#txt = extract_text(files1[8], codec='utf-8')\n",
    "words = nltk.word_tokenize(txt)\n",
    "simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "res2 = [str(res) for res in res1]\n",
    "res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "\n",
    "##removing some ascii character\n",
    "res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "\n",
    "##removing numbers\n",
    "res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "\n",
    "##trying to remove acii characters\n",
    "res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "\n",
    "##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "tagged = tagger.tag(res3)\n",
    "print(tagged)\n",
    "## appending the names extracted from name_person\n",
    "print(name_person(tagged,words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "1. Ground Truth (All 180 files)\n",
    "2. Recall, Precision of all approaches (Next week)\n",
    "3. Expand the reference database\n",
    "4. Graduation Date "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
