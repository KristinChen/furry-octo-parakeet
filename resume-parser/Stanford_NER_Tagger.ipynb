{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import textract, PyPDF2, glob\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(i):\n",
    "    ###Extract text from files\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i\n",
    "\n",
    "\n",
    "def preprocessing(i=0, person=[]):\n",
    "    ##Here i is for iterating over the files and doing the process for each file\n",
    "    words,i = tokenize(i)\n",
    "    ##removing stopwords and punctuations\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "    ##removing some ascii character\n",
    "    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "    ##removing numbers\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "    ##trying to remove acii characters\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "    ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "    tagged = tagger.tag(res3)\n",
    "    \n",
    "    ## appending the names extracted from name_person\n",
    "    person.append(name_person(tagged,words))\n",
    "    #tagged_list.append(tagged)\n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i, person)\n",
    "    return person\n",
    "\n",
    "def name_person(tagged, words):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    #print(tagged_list)\n",
    "    person = []\n",
    "    temp_person = []\n",
    "    for tuple_ele in tagged:\n",
    "        if \"PERSON\" in tuple_ele[1]:\n",
    "            temp_person.append(tuple_ele[0])\n",
    "    ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "    for word in words:\n",
    "        if word in temp_person:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Karthik',\n",
       " 'Robert',\n",
       " 'Holbrook',\n",
       " 'Akanksha',\n",
       " 'Mythri',\n",
       " 'Qizhe',\n",
       " 'VAMSI',\n",
       " 'Python',\n",
       " None,\n",
       " 'Akash',\n",
       " 'Andrew',\n",
       " 'Alexandra',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Manaswini',\n",
       " 'Andrew',\n",
       " 'Python',\n",
       " 'JOHNSON',\n",
       " 'Indupriya',\n",
       " 'James',\n",
       " 'ANYA',\n",
       " 'Ryan',\n",
       " None,\n",
       " None,\n",
       " 'Noah',\n",
       " 'Python',\n",
       " 'JACCARD',\n",
       " 'Python',\n",
       " 'Jira',\n",
       " 'Akshay',\n",
       " 'Python',\n",
       " 'Amar',\n",
       " None,\n",
       " 'Andrew',\n",
       " None,\n",
       " 'Middough',\n",
       " 'Antton',\n",
       " 'ANYA',\n",
       " None,\n",
       " 'Django',\n",
       " 'Python',\n",
       " 'Priyanka',\n",
       " 'Dunbar',\n",
       " 'Python',\n",
       " 'Swift',\n",
       " 'Andrew',\n",
       " None,\n",
       " 'Python',\n",
       " 'Carlos',\n",
       " 'Cassin',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Mario',\n",
       " None,\n",
       " 'JOHNSON',\n",
       " 'Divya',\n",
       " 'Elijah',\n",
       " 'Evan',\n",
       " 'EVAN',\n",
       " None,\n",
       " 'Caldwell',\n",
       " 'Hamza',\n",
       " 'Indupriya',\n",
       " 'Jairo',\n",
       " 'James',\n",
       " 'Jialu',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'JULIA',\n",
       " 'KRISTIN',\n",
       " 'Python',\n",
       " 'Swift',\n",
       " 'Manaswini',\n",
       " None,\n",
       " 'MASEN',\n",
       " 'Matthew',\n",
       " 'Bayes',\n",
       " 'Mikhail',\n",
       " 'Pepys',\n",
       " 'Mohammad',\n",
       " 'Mona',\n",
       " 'Nahian',\n",
       " None,\n",
       " 'Metaphlan',\n",
       " 'Bennett',\n",
       " 'Databricks',\n",
       " None,\n",
       " None,\n",
       " 'Ali',\n",
       " 'Django',\n",
       " 'Anqi',\n",
       " None,\n",
       " 'Arunava',\n",
       " 'Python',\n",
       " 'Paul',\n",
       " 'Bala',\n",
       " None,\n",
       " 'Chaeeun',\n",
       " 'Chantelle',\n",
       " 'Jira',\n",
       " 'P',\n",
       " 'ETHAN',\n",
       " 'Python',\n",
       " 'Harshitha',\n",
       " 'Robert',\n",
       " 'Hemanth',\n",
       " None,\n",
       " 'Vidyavardhini',\n",
       " 'Jessica',\n",
       " 'Ahmed',\n",
       " 'Kaleb',\n",
       " 'Maria',\n",
       " 'Katherine',\n",
       " None,\n",
       " 'Kunjan',\n",
       " 'Python',\n",
       " 'Bayes',\n",
       " 'Naina',\n",
       " 'Nasir',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Jayshree',\n",
       " 'Rishitha',\n",
       " 'Rohin',\n",
       " None,\n",
       " 'Monte',\n",
       " 'Hadoop',\n",
       " 'Python',\n",
       " 'Tesla',\n",
       " 'Tesla',\n",
       " None,\n",
       " 'Siyu',\n",
       " None,\n",
       " None,\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Tableau',\n",
       " 'Tezeswi',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Python',\n",
       " 'Yann',\n",
       " 'Glen',\n",
       " 'Yingkun',\n",
       " 'Powerpivot',\n",
       " None,\n",
       " 'Richie',\n",
       " 'Ricky',\n",
       " 'ROHITH',\n",
       " None,\n",
       " 'Rutvik',\n",
       " 'Ryan',\n",
       " 'Sahana',\n",
       " 'KUMAR',\n",
       " 'Sam',\n",
       " 'Senthil',\n",
       " 'Sharmista',\n",
       " None,\n",
       " 'Zara',\n",
       " None,\n",
       " 'Steven',\n",
       " 'Sushant',\n",
       " 'Python',\n",
       " 'Duke',\n",
       " 'UDAY',\n",
       " None,\n",
       " 'VICTOR',\n",
       " None,\n",
       " 'Python',\n",
       " 'Yichi',\n",
       " 'Python',\n",
       " 'Yifan',\n",
       " 'Yiru',\n",
       " 'Yunhan',\n",
       " 'Zhengyang',\n",
       " 'Python',\n",
       " 'Zixiao',\n",
       " None]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 of the failed examples:\n",
    "- Here all the words including names are not recognized as \"PERSON\" and instead recognized as something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SAIDA', 'LOCATION'), ('MUKTAR', 'ORGANIZATION'), ('Phone', 'ORGANIZATION'), ('Email', 'ORGANIZATION'), ('saidam', 'O'), ('umbcedu', 'O'), ('EDUCATION', 'O'), ('MS', 'O'), ('Data', 'O'), ('Science', 'O'), ('Graduation', 'O'), ('August', 'O'), ('Graduate', 'O'), ('Certificate', 'O'), ('Cybersecurity', 'O'), ('Operations', 'O'), ('Relevant', 'O'), ('courses', 'O'), ('Introduction', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Science', 'ORGANIZATION'), ('Introduction', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Analysis', 'ORGANIZATION'), ('Machine', 'ORGANIZATION'), ('Learning', 'ORGANIZATION'), ('Systems', 'ORGANIZATION'), ('Information', 'ORGANIZATION'), ('Integration', 'ORGANIZATION'), ('Ethical', 'ORGANIZATION'), ('Legal', 'ORGANIZATION'), ('Issues', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Science', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Management', 'ORGANIZATION'), ('Big', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Platforms', 'ORGANIZATION'), ('Introduction', 'ORGANIZATION'), ('Cybersecurity', 'ORGANIZATION'), ('Risk', 'ORGANIZATION'), ('Analysis', 'ORGANIZATION'), ('Compliance', 'ORGANIZATION'), ('Managing', 'ORGANIZATION'), ('CyberSecurity', 'ORGANIZATION'), ('Operations', 'ORGANIZATION'), ('BS', 'O'), ('Mathematics', 'O'), ('Graduation', 'O'), ('December', 'O'), ('Relevant', 'O'), ('courses', 'O'), ('Calculus', 'O'), ('IIIIII', 'O'), ('Computer', 'O'), ('Science', 'O'), ('Python', 'O'), ('Linear', 'O'), ('Algebra', 'O'), ('Differential', 'O'), ('Equations', 'O'), ('Partial', 'O'), ('Differential', 'O'), ('Equations', 'O'), ('Introduction', 'O'), ('Math', 'O'), ('Analysis', 'O'), ('III', 'O'), ('Physics', 'O'), ('III', 'O'), ('Biomathematics', 'O'), ('Dynamical', 'O'), ('Systems', 'O'), ('Mathematical', 'O'), ('Physiology', 'O'), ('TECHNICAL', 'O'), ('PROFILE', 'O'), ('Operating', 'O'), ('systems', 'O'), ('Unix', 'O'), ('Linux', 'O'), ('Windows', 'O'), ('Technologies', 'O'), ('SQL', 'O'), ('Python', 'O'), ('R', 'O'), ('MATLAB', 'O'), ('Maple', 'O'), ('LaTeX', 'O'), ('MySQL', 'O'), ('Spark', 'O'), ('Azure', 'O'), ('Hadoop', 'O'), ('MapReduce', 'O'), ('Databricks', 'O'), ('Pandas', 'O'), ('Numpy', 'O'), ('Scikitlearn', 'O'), ('Seaborn', 'O'), ('Matplotlib', 'O'), ('Folium', 'O'), ('PyTorch', 'O'), ('Tensoflow', 'O'), ('Data', 'O'), ('Science', 'O'), ('Skills', 'O'), ('Statistical', 'O'), ('analysis', 'O'), ('Predictive', 'O'), ('modeling', 'O'), ('simulation', 'O'), ('Deep', 'O'), ('Learning', 'O'), ('models', 'O'), ('TimeSeries', 'O'), ('analysis', 'O'), ('Clustering', 'O'), ('Decision', 'O'), ('Tree', 'O'), ('Learning', 'O'), ('Artificial', 'O'), ('Neural', 'O'), ('Networks', 'O'), ('Regression', 'O'), ('Exploratory', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Analysis', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Engineering', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Prep', 'O'), ('Traditional', 'O'), ('Predictive', 'O'), ('Modeling', 'O'), ('Statistical', 'O'), ('Analysis', 'O'), ('Natural', 'O'), ('Language', 'O'), ('Processing', 'O'), ('EXPERIENCE', 'O'), ('ONCOSPACE', 'O'), ('PRESENT', 'O'), ('Designed', 'O'), ('models', 'O'), ('using', 'O'), ('supervised', 'O'), ('unsupervised', 'O'), ('machine', 'O'), ('learning', 'O'), ('methods', 'O'), ('predict', 'O'), ('radiation', 'O'), ('doses', 'O'), ('oncologists', 'O'), ('Created', 'O'), ('report', 'O'), ('validation', 'O'), ('accuracy', 'O'), ('metrics', 'O'), ('R', 'O'), ('mean', 'O'), ('absolute', 'O'), ('error', 'O'), ('area', 'O'), ('curve', 'O'), ('shortest', 'O'), ('distance', 'O'), ('error', 'O'), ('grouping', 'O'), ('plans', 'O'), ('identification', 'O'), ('plans', 'O'), ('auto', 'O'), ('planning', 'O'), ('results', 'O'), ('less', 'O'), ('accurate', 'O'), ('typical', 'O'), ('Prepared', 'O'), ('report', 'O'), ('significant', 'O'), ('error', 'O'), ('patterns', 'O'), ('identified', 'O'), ('auto', 'O'), ('planning', 'O'), ('model', 'O'), ('results', 'O'), ('characteristics', 'O'), ('plans', 'O'), ('exhibited', 'O'), ('error', 'O'), ('pattern', 'O'), ('DVH', 'O'), ('clustering', 'O'), ('refraction', 'O'), ('plan', 'O'), ('model', 'O'), ('assignment', 'O'), ('Made', 'O'), ('use', 'O'), ('KMean', 'O'), ('clustering', 'O'), ('machine', 'O'), ('learning', 'O'), ('analysis', 'O'), ('Operated', 'O'), ('different', 'O'), ('graphing', 'O'), ('techniques', 'O'), ('order', 'O'), ('check', 'O'), ('variables', 'O'), ('model', 'O'), ('causing', 'O'), ('clusters', 'O'), ('Implemented', 'O'), ('Principal', 'O'), ('Component', 'O'), ('Analysis', 'O'), ('PCA', 'ORGANIZATION'), ('order', 'O'), ('check', 'O'), ('correlation', 'O'), ('clusters', 'O'), ('using', 'O'), ('OVH', 'O'), ('graph', 'O'), ('Analyzed', 'O'), ('variety', 'O'), ('large', 'O'), ('complex', 'O'), ('data', 'O'), ('sets', 'O'), ('using', 'O'), ('variety', 'O'), ('standard', 'O'), ('analytic', 'O'), ('techniques', 'O'), ('including', 'O'), ('regression', 'O'), ('machine', 'O'), ('learning', 'O'), ('approaches', 'O'), ('Used', 'O'), ('data', 'O'), ('SQL', 'O'), ('databases', 'O'), ('python', 'O'), ('study', 'O'), ('information', 'O'), ('patients', 'O'), ('Presented', 'O'), ('summarized', 'O'), ('analyses', 'O'), ('various', 'O'), ('formats', 'O'), ('including', 'O'), ('raw', 'O'), ('output', 'O'), ('tables', 'O'), ('graphics', 'O'), ('oral', 'O'), ('written', 'O'), ('reports', 'O'), ('Wrote', 'O'), ('manual', 'O'), ('test', 'O'), ('cases', 'O'), ('using', 'O'), ('Azure', 'O'), ('DevOps', 'O'), ('Designed', 'O'), ('tables', 'O'), ('graphs', 'O'), ('visualization', 'O'), ('using', 'O'), ('Pandas', 'O'), ('Matplotlib', 'O'), ('Utilized', 'O'), ('statistical', 'O'), ('packages', 'O'), ('order', 'O'), ('analyze', 'O'), ('results', 'O'), ('Machine', 'O'), ('Learning', 'O'), ('outputs', 'O'), ('Familiarized', 'O'), ('Azure', 'O'), ('Cloud', 'O'), ('utilize', 'O'), ('primary', 'O'), ('platform', 'O'), ('data', 'O'), ('studies', 'O'), ('UNIVERSITY', 'O'), ('OF', 'O'), ('MARYLAND', 'LOCATION'), ('BALTIMORE', 'LOCATION'), ('COUNTY', 'O'), ('Studied', 'O'), ('time', 'O'), ('course', 'O'), ('nuclearcytoplasmic', 'O'), ('distribution', 'O'), ('Foxo', 'O'), ('Made', 'O'), ('mathematical', 'O'), ('models', 'O'), ('show', 'O'), ('modulation', 'O'), ('nuclear', 'O'), ('influx', 'O'), ('efflux', 'O'), ('Foxo', 'O'), ('IGFIPIKAkt', 'O'), ('pathway', 'O'), ('skeletal', 'O'), ('muscle', 'O'), ('Used', 'O'), ('mathematical', 'O'), ('models', 'O'), ('nuclearcytoplasmic', 'O'), ('movements', 'O'), ('Foxo', 'PERSON'), ('provide', 'O'), ('values', 'O'), ('unidirectional', 'O'), ('influx', 'O'), ('efflux', 'O'), ('various', 'O'), ('Made', 'O'), ('mathematical', 'O'), ('models', 'O'), ('help', 'O'), ('determine', 'O'), ('properties', 'O'), ('Foxo', 'O'), ('phosphorylationdephosphorylation', 'O'), ('status', 'O'), ('nuclei', 'O'), ('cytoplasm', 'O'), ('fibers', 'O'), ('experimental', 'O'), ('conditions', 'O'), ('skeletal', 'O'), ('muscle', 'O'), ('fibers', 'O'), ('Made', 'O'), ('twostate', 'O'), ('mathematical', 'O'), ('models', 'O'), ('study', 'O'), ('Foxo', 'O'), ('nuclearcytoplasmic', 'O'), ('movements', 'O'), ('Used', 'O'), ('MATLAB', 'O'), ('test', 'O'), ('models', 'O'), ('fitting', 'O'), ('date', 'O'), ('blocking', 'O'), ('nuclear', 'O'), ('efflux', 'O'), ('Leptomycin', 'O'), ('Used', 'O'), ('LaTex', 'O'), ('project', 'O'), ('reporting', 'O'), ('Plotted', 'O'), ('datas', 'O'), ('MATLAB', 'O'), ('find', 'O'), ('parameters', 'O'), ('fit', 'O'), ('parameters', 'O'), ('UNIVERSITY', 'O'), ('OF', 'O'), ('MARYLAND', 'LOCATION'), ('BALTIMORE', 'O'), ('Studying', 'O'), ('levels', 'O'), ('serotonin', 'O'), ('release', 'O'), ('brain', 'O'), ('pain', 'O'), ('inflicted', 'O'), ('Making', 'O'), ('Python', 'O'), ('Matlab', 'O'), ('codes', 'O'), ('record', 'O'), ('analyze', 'O'), ('data', 'O'), ('pain', 'O'), ('recorded', 'O'), ('given', 'O'), ('threshold', 'O'), ('REFERENCE', 'O'), ('AVAILABLE', 'O'), ('UPON', 'O'), ('REQUEST', 'O'), ('BS', 'O'), ('degree', 'O'), ('statistics', 'O'), ('chine', 'O'), ('lear', 'O'), ('ning', 'O'), ('data', 'O'), ('cien', 'O'), ('ce', 'O'), ('BS', 'O'), ('degree', 'O'), ('e', 'O'), ('quivalent', 'O'), ('experien', 'O'), ('ce', 'O'), ('syste', 'O'), ('engi', 'O'), ('neering', 'O'), ('decisi', 'O'), ('ory', 'O'), ('Two', 'O'), ('years', 'O'), ('experien', 'O'), ('ce', 'O'), ('desig', 'O'), ('ning', 'O'), ('leading', 'O'), ('per', 'O'), ('son', 'O'), ('larger', 'O'), ('techni', 'O'), ('cal', 'O'), ('progra', 'O'), ('ms', 'O'), ('Evidence', 'O'), ('sel', 'O'), ('fdirecte', 'O'), ('organizational', 'O'), ('investigation', 'O'), ('diag', 'O'), ('nosis', 'O'), ('development', 'O'), ('Two', 'O'), ('years', 'O'), ('experien', 'O'), ('ce', 'O'), ('usi', 'O'), ('ng', 'O'), ('simulation', 'O'), ('ot', 'O'), ('analytical', 'O'), ('tools', 'O'), ('model', 'O'), ('control', 'O'), ('software', 'O'), ('developme', 'O'), ('nt', 'O'), ('Publicati', 'O'), ('hist', 'O'), ('ory', 'O'), ('demonstrates', 'O'), ('e', 'O'), ('ffe', 'O'), ('ctive', 'O'), ('technical', 'O'), ('communication', 'O'), ('skills', 'O'), ('Preferred', 'O'), ('Qualifications', 'O'), ('MS', 'O'), ('degree', 'O'), ('statistics', 'O'), ('machine', 'O'), ('learni', 'O'), ('ng', 'O'), ('data', 'O'), ('scie', 'O'), ('nce', 'O'), ('MS', 'O'), ('PhD', 'O'), ('degree', 'O'), ('n', 'O'), ('system', 'O'), ('e', 'O'), ('ngineering', 'O'), ('deci', 'O'), ('sion', 'O'), ('theory', 'O'), ('Two', 'O'), ('years', 'O'), ('experien', 'O'), ('ce', 'O'), ('desig', 'O'), ('ning', 'O'), ('leadi', 'O'), ('ng', 'O'), ('optimizi', 'O'), ('ng', 'O'), ('pers', 'O'), ('larger', 'O'), ('techni', 'O'), ('cal', 'O'), ('progra', 'O'), ('ms', 'O'), ('Five', 'O'), ('years', 'O'), ('experien', 'O'), ('ce', 'O'), ('de', 'O'), ('signing', 'O'), ('buil', 'O'), ('ding', 'O'), ('si', 'O'), ('mulations', 'O'), ('nalytical', 'O'), ('tools', 'O'), ('model', 'O'), ('nd', 'O'), ('control', 'O'), ('oftware', 'O'), ('development', 'O'), ('Publicati', 'O'), ('hist', 'O'), ('ory', 'O'), ('demonstrates', 'O'), ('e', 'O'), ('ffe', 'O'), ('ctive', 'O'), ('technical', 'O'), ('communication', 'O'), ('skills', 'O'), ('Speci', 'O'), ('fically', 'O'), ('publications', 'O'), ('docume', 'O'), ('nts', 'O'), ('sel', 'O'), ('fdire', 'O'), ('cted', 'O'), ('orga', 'O'), ('nizational', 'O'), ('inve', 'O'), ('stigation', 'O'), ('diagnosis', 'O'), ('redevelopment', 'O'), ('recomme', 'O'), ('ndations', 'O'), ('Space', 'O'), ('industry', 'O'), ('experien', 'O'), ('ce', 'O'), ('private', 'O'), ('military', 'O'), ('civilian', 'O'), ('se', 'O'), ('ctor', 'O')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "txt = extract_text(files1[8], codec='utf-8')\n",
    "words = nltk.word_tokenize(txt)\n",
    "simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "res2 = [str(res) for res in res1]\n",
    "res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "\n",
    "##removing some ascii character\n",
    "res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "\n",
    "##removing numbers\n",
    "res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "\n",
    "##trying to remove acii characters\n",
    "res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "\n",
    "##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "tagged = tagger.tag(res3)\n",
    "print(tagged)\n",
    "## appending the names extracted from name_person\n",
    "print(name_person(tagged,words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def tokenize(i):\\n    txt = extract_text(files1[i], codec=\\'utf-8\\')\\n    words = nltk.word_tokenize(txt) \\n    return words,i\\n\\n\\ndef preprocessing(i, tagged_list, words_list):\\n    words,i = tokenize(i)\\n    words_list.append(word)\\n    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\\n    res = [re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', word) for word in simple_strings]\\n    res1 = [re.sub(r\\'^abc(.*?)=[A-Z0-9]+(.*)\\', r\\'\\x01\\x02\\', word) for word in res]\\n    res2 = [str(res) for res in res1]\\n    res3 = [re.sub(r\\'/^[A-Za-z]+$/\\', \\'\\', res) for res in res2]\\n    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\\n    res3 = [re.sub(\\'\\\\d\\', \\'%d\\', res) for res in res3]\\n    res3 = [res.encode(\\'ascii\\',\"ignore\").decode() for res in res3]\\n    tagged = tagger.tag(res3)\\n    name_person(tagged,words)\\n    #tagged_list.append(tagged)\\n    if i+1 < len(files1):\\n        i+=1\\n        preprocessing(i,tagged_list)\\n    return tagged_list\\n\\ndef name_person(tagged, words):\\n    #tagged_list = preprocessing(0,[],[])\\n    print(tagged_list)\\n    person = []\\n    for tag in tagged_list:\\n        temp_person = []\\n        for tuple_ele in tag:\\n            if \"PERSON\" in tuple_ele[1]:\\n                temp_person.append(tuple_ele[0])\\n        ## loop through the original words so we can extract from the first words and get the first PERSON\\n        for words in word_list\\n        for word in words:\\n            if word in temp_person:\\n                print(word)\\n                person.append(word)\\n                break\\n    return person'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def tokenize(i):\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i\n",
    "\n",
    "\n",
    "def preprocessing(i, tagged_list, words_list):\n",
    "    words,i = tokenize(i)\n",
    "    words_list.append(word)\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    res3 = [re.sub('\\d', '%d', res) for res in res3]\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    tagged = tagger.tag(res3)\n",
    "    name_person(tagged,words)\n",
    "    #tagged_list.append(tagged)\n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i,tagged_list)\n",
    "    return tagged_list\n",
    "\n",
    "def name_person(tagged, words):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    print(tagged_list)\n",
    "    person = []\n",
    "    for tag in tagged_list:\n",
    "        temp_person = []\n",
    "        for tuple_ele in tag:\n",
    "            if \"PERSON\" in tuple_ele[1]:\n",
    "                temp_person.append(tuple_ele[0])\n",
    "        ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "        for words in word_list\n",
    "        for word in words:\n",
    "            if word in temp_person:\n",
    "                print(word)\n",
    "                person.append(word)\n",
    "                break\n",
    "    return person\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
