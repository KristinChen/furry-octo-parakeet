{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "import spacy\n",
    "# spacy.load(\"en_core_web_sm\")\n",
    "from pyresparser import ResumeParser\n",
    "from resume_parser import resumeparse\n",
    "import os\n",
    "import docx2txt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text\n",
    "# WOERKING COCDE DONT EDIt\n",
    "import sys\n",
    "import textract, PyPDF2, glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "import os\n",
    "#java_path = r\"C:\\Program Files\\Java\\jdk-17.0.2\\bin\\java.exe\"\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import pdfplumber\n",
    "from nltk.tag import pos_tag\n",
    "#files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing model object and some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")\n",
    "\n",
    "tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "us_names = nd.get_top_names(n=1000, gender='Male', country_alpha2='US')['US']['M']\n",
    "indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "\"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "\"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "\"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "\"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "\"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "\"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "\"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "\"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "\"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "\"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "\"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'Ã¼nivers',\n",
    "    'okul',\n",
    "    'University'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined function to calculate all the details about the  applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>graduation_year</th>\n",
       "      <th>Contact</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>{University of Maryland}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(410)-292-1151</td>\n",
       "      <td>[karthikr2194@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[hpundir@umd.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRTH PATEL</td>\n",
       "      <td>{The University of Texas at Dallas, Nirma Univ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[tirth2410@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>{University of Houston}</td>\n",
       "      <td>2015</td>\n",
       "      <td>(281) 725-7080</td>\n",
       "      <td>[mythripartha8@gmail.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                          education  \\\n",
       "0  Karthik Ramanarayana                           {University of Maryland}   \n",
       "1          HARSH PUNDIR                                                 {}   \n",
       "2           TIRTH PATEL  {The University of Texas at Dallas, Nirma Univ...   \n",
       "3        Akanksha Bapna                                                 {}   \n",
       "4         Mythri Partha                            {University of Houston}   \n",
       "\n",
       "  graduation_year         Contact                      email  \n",
       "0            2021  (410)-292-1151   [karthikr2194@gmail.com]  \n",
       "1            2020            None          [hpundir@umd.edu]  \n",
       "2            2021    469-370-9437      [tirth2410@gmail.com]  \n",
       "3            2021            None                         []  \n",
       "4            2015  (281) 725-7080  [mythripartha8@gmail.com]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize the text from the .pdf file\n",
    "def tokenize(i, person=[], education=[], graduation_year=[], phone_nbr=[], emails=[]):\n",
    "    ###Extract text from files\n",
    "    try:\n",
    "        with pdfplumber.open(files1[i]) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            txt1 = first_page.extract_text()\n",
    "    except:\n",
    "        txt1=\"\"\n",
    "        person.append(files1[i])\n",
    "        education.append(\"Err\")\n",
    "        graduation_year.append(extract_graduation_date(\"Err\"))\n",
    "        phone_nbr.append(extract_phone_number(\"Err\"))\n",
    "        emails.append(extract_emails(\"Err\"))\n",
    "        i+=1\n",
    "        tokenize(i)\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    #print(txt)\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails\n",
    "\n",
    "## Doing some preprocessing steps and appending the details of the applicants\n",
    "def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "    ##Here i is for iterating over the files and doing the process for each file\n",
    "    flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "    words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails = tokenize(i)\n",
    "    #print(txt)\n",
    "    ##removing stopwords and punctuations\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "    ##removing some ascii character\n",
    "    res3 = [res.replace(\"Ã¯\",\"i\") if \"Ã¯\" in res else res for res in res3]\n",
    "    \n",
    "    ##removing numbers\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "    ##trying to remove acii characters\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "    ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "    tagged = tagger.tag(res3)\n",
    "    \n",
    "    ## appending the names extracted from name_person\n",
    "    ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "    try:\n",
    "        person.append(name_person(tagged,words,res3))\n",
    "    except:\n",
    "        flag_success = 0\n",
    "        #person.append(files1[i])\n",
    "        pass\n",
    "    #tagged_list.append(tagged) \n",
    "    #flag_success = 1\n",
    "    if flag_success == 1:\n",
    "        education.append(extract_education(txt))\n",
    "        graduation_year.append(extract_graduation_date(txt1))\n",
    "        phone_nbr.append(extract_phone_number(txt))\n",
    "        emails.append(extract_emails(txt))\n",
    "    else:\n",
    "        flag_success == 1\n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i, person)\n",
    "    \n",
    "    #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "    return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "\n",
    "\n",
    "\n",
    "def name_person(tagged, words,res3):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    #print(tagged_list)\n",
    "    person = []\n",
    "    temp_person = []\n",
    "    nltk_tagged = pos_tag(res3[:10])\n",
    "    \n",
    "    for k in range(10):\n",
    "        if k<=9:\n",
    "            if nltk_tagged[k][1] == 'NNP' and nltk_tagged[k+1][1] == 'NNP':\n",
    "                nltk_name = nltk_tagged[k][0] +' '+ nltk_tagged[k+1][0] \n",
    "            return nltk_name\n",
    "        print()pyr\n",
    "        if (res3[k].lower() in indian_last_names) or (res3[k].lower() in chinese_last_names):\n",
    "            j = k-1\n",
    "            return res3[j]+\" \" +res3[k]#+\"(Extracted using 1st approach)\"\n",
    "    for tuple_ele in tagged:\n",
    "        if \"PERSON\" in tuple_ele[1]:\n",
    "            temp_person.append(tuple_ele[0])\n",
    "    ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "    for word in words:\n",
    "        if word in temp_person:\n",
    "            return word\n",
    "\n",
    "def extract_education(txt):\n",
    "    edu=set()\n",
    "    p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "    for m in re.finditer(p,txt):\n",
    "        try:\n",
    "            if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "                edu.append(m.group(1))\n",
    "        except:\n",
    "            pass\n",
    "        for word in RESERVED_WORDS:\n",
    "            if word in m.group(2):\n",
    "                edu.add(m.group(2))\n",
    "    return edu\n",
    "\n",
    "def extract_graduation_date(txt1):\n",
    "    dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "    # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "    x=\"(?is)education.*?(\\d{4})\"\n",
    "    # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "    if len(re.findall(x,txt1))==0:\n",
    "        return None\n",
    "    return max(re.findall(x,txt1))\n",
    "    ## for dates\n",
    "    #for dt in dates:\n",
    "    #    if dt in txt:\n",
    "    #        return dt\n",
    "\n",
    "def extract_phone_number(txt):\n",
    "    PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if txt.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "def extract_emails(txt):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "StanfordNER = preprocessing()\n",
    "StanfordNER.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating results using fuzzy logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karthik Ramanarayana Karthik Ramanarayana\n",
      "HARSH PUNDIR HARSH PUNDIR\n",
      "TIRTH PATEL TIRTH PATEL\n",
      "Akanksha Bapna Akanksha Bapna\n",
      "Mythri Partha Mythri Partha\n",
      "Qizhe Wang Qizhe Wang\n",
      "RACHAN BHOOSHI RACHAN VAMSI\n",
      "RAGHUVEERA MADIREDDY RAGHUVEERA KARTHIK\n",
      "SAIDA MUKTAR SAIDA MUKTAR\n",
      "Akash Patel Allentown PA\n",
      "AKSHARA ARCOT AKSHARA ARCOT\n",
      "Alexandra Manetas Alexandra Manetas\n",
      "NO MATCH Pranav Gulghane, MOHIT MANJARIA. Score = 28\n",
      "NO MATCH Wanting Lu, Yuchen Xie. Score = 10\n",
      "NO MATCH Manaswini Nagaraj, Indupriya Kompi. Score = 25\n",
      "MOHIT MANJARIA James Renier\n",
      "NO MATCH Yuchen Xie, ANYA PATEL. Score = 30\n",
      "NO MATCH Dimple Mehra, Ryan Martin. Score = 17\n",
      "NO MATCH Indupriya Kompi, Mayank Dubey. Score = 22\n",
      "NO MATCH James Domingo, ABHINAY NAGULAPALLI. Score = 25\n",
      "ANYA PATEL Noah Mattheis\n",
      "NO MATCH Ryan Goodwin, Abhilash Rajaram. Score = 14\n",
      "Mayank Dubey Adam Dinser\n",
      "ABHINAY NAGULAPALLI AJAY PRASATH\n",
      "NO MATCH Noah Mattheis, Akshay Kajale. Score = 15\n",
      "Abhilash Rajaram ALEXANDER FRIEND\n",
      "NO MATCH Adam Dinser, Amar Nath. Score = 30\n",
      "ADEOLA ADESOBA Amol Agrawal\n",
      "NO MATCH AJAY MANOHARAN, Andrew Yuchen. Score = 7\n",
      "Akshay Kajale ANIRUDH REDDY\n",
      "NO MATCH Alexander Friend, ANNAPOORNA M. Score = 21\n",
      "NO MATCH Amar Jha, Antton Wilbanks. Score = 26\n",
      "Amol Agrawal ANYA PATEL\n",
      "NO MATCH Andrew Lin, Archit Prem. Score = 19\n",
      "NO MATCH ANIRUDH SURAM, Washington DC. Score = 8\n",
      "NO MATCH ANNAPOORNA GOPALAKRISHNA, BAIYU CHEN. Score = 6\n",
      "NO MATCH Antton Wilbanks, BHAVIKA CHAVDA. Score = 21\n",
      "NO MATCH ANYA PATEL, BOKYEUNG KIM. Score = 9\n",
      "NO MATCH Archit Prem, B O. Score = 14\n",
      "NO MATCH Ashvi Soni, Buka Cakrawala. Score = 8\n",
      "NO MATCH BAIYU CHEN, Andrew Decker. Score = 9\n",
      "BHAVIKA CHAVDA Vienna VA\n",
      "NO MATCH BOKYEUNG KIM, Carlos Perez. Score = 8\n",
      "BON TRINH Cassin Thangam\n",
      "NO MATCH Buka Cakrawala, CHENGWEI CHEN. Score = 7\n",
      "NO MATCH Andrew Decker, CHETNA KHANNA. Score = 15\n",
      "NO MATCH C:\\Users\\rtd91\\Data\\resume_samples\\C01-21120201_Jiayue_Fei.pdf, Dhruv Parikh. Score = 16\n",
      "Cahyarini (Crystal) Hariga Dhwani Gandhi\n",
      "Carlos Perez Divya Reddy\n",
      "NO MATCH Cassin Edwin, Elijah James. Score = 17\n",
      "CHENGWEI CHEN EVAN W\n",
      "CHETNA KHANNA EVAN M\n",
      "NO MATCH Dhruv Parikh, Gali Sabyr. Score = 18\n",
      "Dhwani Gandhi EDUCATION GRISHMA\n",
      "NO MATCH Dimple Mehra, Hamza Abdelghani. Score = 14\n",
      "Divya Manku Indupriya Kompi\n",
      "Elijah Hall Jairo Carreon\n",
      "NO MATCH Evan JONES, James Renier. Score = 18\n",
      "NO MATCH EVAN JOYCE, Sep . Score = 14\n",
      "NO MATCH Gali Sabyr, Jiehong Liu. Score = 19\n",
      "NO MATCH GRISHMA PARAJULI, KRISTIN JIATING. Score = 6\n",
      "NO MATCH Hamza Abdelghani, KUNJ MITHAPARA. Score = 20\n",
      "Indupriya Kompi Manisha Patel\n",
      "NO MATCH Jairo Carreon, MASEN BACHLEDA. Score = 22\n",
      "James Domingo Matthew Holcombe\n",
      "NO MATCH Stellar(Jialu) Xia, MAYURI LALWANI. Score = 25\n",
      "NO MATCH Jianbo Gu, Mikhail Tokarev. Score = 25\n",
      "NO MATCH Jiehong Liu, San Diego. Score = 30\n",
      "Jim Liu Mona Eslamijam\n",
      "NO MATCH JULIA XIA, NIDHI CHOVATIYA. Score = 8\n",
      "KRISTIN CHEN  NIKSON PANIGRAHI\n",
      "NO MATCH KUNJ MITHAPARA, C:\\Users\\rtd91\\Data\\resume_samples\\output data. Score = 3\n",
      "NO MATCH Lejian He, PRATIK SATPUTE. Score = 26\n",
      "NO MATCH Manaswini Nagaraj, PRAVEEN PANDEY. Score = 13\n",
      "NO MATCH Manisha Patel, Adedamola . Score = 26\n",
      "NO MATCH MASEN BACHLEDA, Aditya Deshpande. Score = 7\n",
      "Matthew Holcombe ANKIT HEMANT\n",
      "NO MATCH MAYURI LALWANI, Anqi Cheng. Score = 8\n",
      "NO MATCH Mikhail Tokarev, Greensboro NC. Score = 14\n",
      "NO MATCH Mistere Abate, Arunava Ray. Score = 8\n",
      "NO MATCH Mohammad Zarei, ASHUTOSH SHINDE. Score = 21\n",
      "Mona Eslamijam BAH YANNICK\n",
      "Nahian Siddique Bala Harimani\n",
      "NO MATCH NIDHI CHOVATIYA, BONFACE NJUGUNA. Score = 7\n",
      "NO MATCH NIKSON PANIGRAHI, Chaeeun . Score = 8\n",
      "NO MATCH PRATIK SATPUTE, Chantelle Lim. Score = 7\n",
      "PRAVEEN PANDEY CHITRANJAN JOSHI\n",
      "NO MATCH Adedamola Olawoye, ETHAN LAURENCEAU. Score = 30\n",
      "Aditya Deshpande GIRIJA BANDARU\n",
      "NO MATCH Ali Majidian, Harshitha Prasad. Score = 21\n",
      "NO MATCH ANKIT LADE, HARSH PUNDIR. Score = 9\n",
      "NO MATCH Anqi Cheng, Hemanth Reddy. Score = 26\n",
      "NO MATCH Arbaaz Mohideen, Isioma . Score = 9\n",
      "Arunava Ray Jishan Ahmed\n",
      "ASHUTOSH SHINDE KALEB SHIKUR\n",
      "NO MATCH BAH KONAN, Kanth Juvadi. Score = 10\n",
      "NO MATCH Bala Harimani, KRUTI GUPTA. Score = 25\n",
      "NO MATCH BONFACE NJUGUNA, Kunjan Devendra. Score = 7\n",
      "NO MATCH Chaeeun Lim, L E. Score = 14\n",
      "NO MATCH Chantelle Lim, MAAZ ZAHID. Score = 9\n",
      "NO MATCH CHITRANJAN JOSHI, Naina Grover. Score = 7\n",
      "NO MATCH Diego Burgos, Nasir U. Score = 11\n",
      "ETHAN LAURENCEAU Nate Tsegaw\n",
      "GIRIJA BANDARU Navina Kaur\n",
      "Harshitha Prasad Rakshit Sinha\n",
      "NO MATCH HARSH PUNDIR, Rishitha Muddana. Score = 7\n",
      "NO MATCH Hemanth Kodakandla, Rohin Bhagavatula. Score = 23\n",
      "Isioma Ochia SAIDA MUKTAR\n",
      "NO MATCH JAY PANDYA, Sai Sri. Score = 12\n",
      "Jessica Rega Saumil Sudhir\n",
      "Jishan Ahmed Saumil Sudhir\n",
      "NO MATCH KALEB SHIKUR, SOBANAA JAYAKUMAR. Score = 7\n",
      "NO MATCH Kanth Juvadi, C:\\Users\\rtd91\\Data\\resume_samples\\Resume_Srinath_Narayanan.pdf. Score = 16\n",
      "NO MATCH Katherine Pearson, Srushti Shah. Score = 7\n",
      "NO MATCH KRUTI ALLENKI, TANISHK PARIHAR. Score = 7\n",
      "NO MATCH Kunjan Khatri, Tej Gottapu. Score = 8\n",
      "NO MATCH LEONEL FLORES, Tezeswi Madarasu. Score = 7\n",
      "NO MATCH MAAZ SHAIKH , TIA MOODY. Score = 10\n",
      "NO MATCH Naina Grover , VIJAYALAKSHMI GIRIJALA. Score = 11\n",
      "NO MATCH Nasir Sarkar, WEICHEN LU. Score = 18\n",
      "Nate Tsegaw Yann Tamraz\n",
      "NO MATCH Navina Sethi , YENI PEREZ. Score = 26\n",
      "NO MATCH Rakshit Sinha , Yingkun Wang. Score = 23\n",
      "REETIKA CHATURVEDI  Greenbelt MD\n",
      "Rishitha Muddana  Richie Lahoti\n",
      "NO MATCH Rohin Bhagavatula , Ricky Donnell. Score = 19\n",
      "NO MATCH SAIDA MUKTAR , ROHITH MALLULA. Score = 7\n",
      "NO MATCH Saivalini Durvasula , Ruoshi Zhao. Score = 19\n",
      "Sai Malempati Rutvik Patel\n",
      "Sanat Lal Ryan Martin\n",
      "Saumil Jariwala  San Jose\n",
      "Shalin Shanghavi  SAI KUMAR\n",
      "Siyu Tao Sam Song\n",
      "NO MATCH SOBANAA JAYAKUMAR , Senthil Nathan. Score = 6\n",
      "Srishti Piplani  Sharmista Vemulapalli\n",
      "NO MATCH Srushti Shah, Aug . Score = 12\n",
      "NO MATCH Taegeun Ohe, Location US. Score = 27\n",
      "NO MATCH TANISHK PARIHAR , Steven H. Score = 8\n",
      "NO MATCH Tej Gottapu, Tanvi Tembhurne. Score = 23\n",
      "NO MATCH Tezeswi Madarasu, TZUYAO LIN. Score = 15\n",
      "TIA MOODY  UDAY M\n",
      "NO MATCH VIJAYALAKSHMI GIRIJALA , VANISA ACHAKULVISUT. Score = 5\n",
      "WEI-CHEN, LU    Wenmo Sun\n",
      "NO MATCH   Yann Tamraz, New York. Score = 29\n",
      "YENI PEREZ Yichi Oliver\n",
      "NO MATCH YING LU , RELEVANT EXPERIENCEPROJECTS. Score = 6\n",
      "Yingkun Wang Yunhan Zoe\n",
      "NO MATCH Yutong Tang , Jersey City. Score = 9\n",
      "NO MATCH Richie Lahoti  , Zhuohan Zhang. Score = 21\n",
      "NO MATCH Ricky Lindsey, Zixiao Huang. Score = 24\n",
      "NO MATCH ROHITH MALLULA , Ziyong . Score = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-bf4708fb3f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mStanfordNER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"degree\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m### Scoring for pyres parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m# email = metrics(StanfordNER['email'],ground_truth_tail['email'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# mobile = metrics(StanfordNER['Contact'],ground_truth_tail['Contact'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-bf4708fb3f0b>\u001b[0m in \u001b[0;36mmetrics\u001b[1;34m(parse_col, ground_truth_col)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#         print(fuzz.ratio(str.lower(list2[i]), list1[i]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlist2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ground_truth_tail = ground_truth\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "def metrics(parse_col,ground_truth_col):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    list1 = ground_truth_col.to_list()\n",
    "    list2 = parse_col.to_list()\n",
    "\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "#         print(fuzz.ratio(str.lower(list2[i]), list1[i]))\n",
    "        if list2[i]==None:\n",
    "            fp+=1\n",
    "            continue\n",
    "        try:\n",
    "           \n",
    "            if fuzz.ratio(str.lower(list2[i]), str.lower(list1[i]))>30:\n",
    "                print(list1[i],list2[i])\n",
    "                tp+=1\n",
    "            else:\n",
    "                print(f'NO MATCH {list1[i]}, {list2[i]}. Score = {fuzz.ratio(str.lower(list2[i]), list1[i])}')\n",
    "                fp+=1\n",
    "        except:\n",
    "            \n",
    "            fp+=1\n",
    "    print(i)\n",
    "    i+=1\n",
    "    return tp,fp\n",
    "\n",
    "metric_list = []\n",
    "StanfordNER[\"mobile\"] = 0\n",
    "StanfordNER[\"degree\"] = \"\"\n",
    "### Scoring for pyres parser\n",
    "name = metrics(StanfordNER['name'],ground_truth_tail['name'])\n",
    "# email = metrics(StanfordNER['email'],ground_truth_tail['email'])\n",
    "# mobile = metrics(StanfordNER['Contact'],ground_truth_tail['Contact'])\n",
    "# university = metrics(StanfordNER['education'],ground_truth_tail['education'])\n",
    "# degree = metrics(StanfordNER['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "# metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "name\n",
    "#print(name,email,mobile,university,degree)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75e802e3c978afb93b9b8a4fcdbb12c74d1245b9e37bc1cfa064b95f94c1eab1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
