{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "import spacy\n",
    "# spacy.load(\"en_core_web_sm\")\n",
    "from pyresparser import ResumeParser\n",
    "from resume_parser import resumeparse\n",
    "import os\n",
    "import docx2txt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karthik Ramanarayana'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResumeParser(r'C:\\Users\\rtd91\\Data\\resume_samples\\721091408_phonescreening.pdf').get_extracted_data()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721091408_phonescreening.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 02:36:05,327 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\rtd91\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2022-03-27 02:36:12,699 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\rtd91\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2022-03-27 02:36:13,223 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821101104_phonescreening.pdf\n",
      "821101136_phonescreening.pdf\n",
      "821110120.pdf\n",
      "821110440_Mythri.pdf\n",
      "821110445_Qizhe.pdf\n",
      "821110447_rachan.pdf\n",
      "821110448_RAGHUVEERA.pdf\n",
      "821110464_Saida.pdf\n",
      "821110509_Akash.pdf\n",
      "821110510_AKSHARA.pdf\n",
      "821110511_Alexandra.pdf\n",
      "821110527_.pdf\n",
      "821110528_Wanting.pdf\n",
      "821111539_manaswini.pdf\n",
      "821112919_mohit.pdf\n",
      "821112928_yechen.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e6022c2e34c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\resume_parser\\resumeparse.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(file, docx_parser)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_segments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contact_info'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mtotal_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_segments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[0muniversity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_university\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'world-universities.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mdesignition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_designition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\resume_parser\\resumeparse.py\u001b[0m in \u001b[0;36mextract_university\u001b[1;34m(text, file)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_university\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[0muniversities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mcollege_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "directory = r'C:\\Users\\rtd91\\Data\\resume_samples'\n",
    "pdf = []\n",
    "docs = []\n",
    "pyresparser = []\n",
    "resumeparser = []\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    data = {}\n",
    "    if filename.endswith(\".docx\"):\n",
    "        path = os.path.join(directory, filename)\n",
    "        \n",
    "        data = resumeparse.read_file(path)\n",
    "        data['text'] = extract_text_from_docx(path)\n",
    "        \n",
    "              \n",
    "        resumeparser.append(data)\n",
    "        \n",
    "        data = ResumeParser(path).get_extracted_data()\n",
    "        data['text'] = extract_text_from_docx(path)\n",
    "        \n",
    "        pyresparser.append(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif filename.endswith(\".pdf\"):\n",
    "\n",
    "               \n",
    "        path = os.path.join(directory, filename)\n",
    "        \n",
    "        data = resumeparse.read_file(path)\n",
    "        data['text'] = extract_text_from_pdf(path)\n",
    "        \n",
    "              \n",
    "        resumeparser.append(data)\n",
    "        \n",
    "        data = ResumeParser(path).get_extracted_data()\n",
    "        data['text'] = extract_text_from_pdf(path)\n",
    "        \n",
    "        pyresparser.append(data)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>skills</th>\n",
       "      <th>college_name</th>\n",
       "      <th>degree</th>\n",
       "      <th>designation</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_names</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>total_experience</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>karthikr2194@gmail.com</td>\n",
       "      <td>292-1151</td>\n",
       "      <td>[Communication, Hbase, Statistics, Matrix, Tab...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Data Intern, Index Analytics LLC, Baltimore, MD]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>hpundir@umd.edu</td>\n",
       "      <td>240.423.5453</td>\n",
       "      <td>[Algorithms, Six sigma, Tableau, Big data, Pay...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Business Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[SMSA (  VP, Operations), ■  Help with the pla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX Master</td>\n",
       "      <td>tirth2410@gmail.com</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[Statistics, Algorithms, Tableau, Statistical ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Web Analyst, The University of Texas at Dalla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Retail, Health, Strategy, Ibm, Budget, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelors in Technology, Chemical Engineering</td>\n",
       "      <td>None</td>\n",
       "      <td>[The Editorial Board, Head of Logistics, Oct 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Akanksha Bapna\\nabapna.com ● Email ● Personal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>mythripartha8@gmail.com</td>\n",
       "      <td>725-7080</td>\n",
       "      <td>[Health, Numpy, Email, Access, Pandas, Enginee...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[InventXYZ, Data Engineering Intern, Aug. 2015...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Mythri Partha \\nHouston, TX | Phone: (281) 725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>yd2578@columbia.edu·(319)</td>\n",
       "      <td>512-0421</td>\n",
       "      <td>[Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...</td>\n",
       "      <td>None</td>\n",
       "      <td>M.S. in Computer and Information Science</td>\n",
       "      <td>None</td>\n",
       "      <td>[Streaming Data Retrieval System              ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yd2578@columbia.edu·(319) 512-0421·www.zoedong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Jersey City</td>\n",
       "      <td>zzhan67@stevens.edu</td>\n",
       "      <td>531-8942</td>\n",
       "      <td>[Communication, Algorithms, Statistics, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Utegration LLC, Houston, TX, Data Analytics I...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jersey City, NJ                               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>zz2751@columbia.edu</td>\n",
       "      <td>763-4795</td>\n",
       "      <td>[Health, Statistics, Data analytics, Tableau, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelor of Arts, Mathematics (Statistics), Ec...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Project on Covid Social Media Data           ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Zixiao Huang</td>\n",
       "      <td>zixiao.huang.74@gmail.com</td>\n",
       "      <td>615-977-2160</td>\n",
       "      <td>[Algorithms, Statistics, Tableau, Startup, Big...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[Stealth Mode Startup, Deep Learning Innovatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Stream Bank</td>\n",
       "      <td>zwang160@terpmail.umd.edu</td>\n",
       "      <td>216-6448</td>\n",
       "      <td>[Communication, Strategy, Statistics, Tableau,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>[Investment Manager Assistant]</td>\n",
       "      <td>[FRM designation. CFA level one exam passed., ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                      email mobile_number  \\\n",
       "0    Karthik Ramanarayana     karthikr2194@gmail.com      292-1151   \n",
       "1            HARSH PUNDIR            hpundir@umd.edu  240.423.5453   \n",
       "2               TX Master        tirth2410@gmail.com  469-370-9437   \n",
       "3          Akanksha Bapna                       None          None   \n",
       "4           Mythri Partha    mythripartha8@gmail.com      725-7080   \n",
       "..                    ...                        ...           ...   \n",
       "177   Columbia University  yd2578@columbia.edu·(319)      512-0421   \n",
       "178           Jersey City        zzhan67@stevens.edu      531-8942   \n",
       "179   Columbia University        zz2751@columbia.edu      763-4795   \n",
       "180          Zixiao Huang  zixiao.huang.74@gmail.com  615-977-2160   \n",
       "181           Stream Bank  zwang160@terpmail.umd.edu      216-6448   \n",
       "\n",
       "                                                skills college_name  \\\n",
       "0    [Communication, Hbase, Statistics, Matrix, Tab...         None   \n",
       "1    [Algorithms, Six sigma, Tableau, Big data, Pay...         None   \n",
       "2    [Statistics, Algorithms, Tableau, Statistical ...         None   \n",
       "3    [Retail, Health, Strategy, Ibm, Budget, Tablea...         None   \n",
       "4    [Health, Numpy, Email, Access, Pandas, Enginee...         None   \n",
       "..                                                 ...          ...   \n",
       "177  [Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...         None   \n",
       "178  [Communication, Algorithms, Statistics, Tablea...         None   \n",
       "179  [Health, Statistics, Data analytics, Tableau, ...         None   \n",
       "180  [Algorithms, Statistics, Tableau, Startup, Big...         None   \n",
       "181  [Communication, Strategy, Statistics, Tableau,...         None   \n",
       "\n",
       "                                                degree  \\\n",
       "0                                                        \n",
       "1              Master of Science in Business Analytics   \n",
       "2                                                        \n",
       "3        Bachelors in Technology, Chemical Engineering   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "177           M.S. in Computer and Information Science   \n",
       "178                                                      \n",
       "179  Bachelor of Arts, Mathematics (Statistics), Ec...   \n",
       "180                     Master of Science in Analytics   \n",
       "181                                                      \n",
       "\n",
       "                        designation  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "..                              ...   \n",
       "177                            None   \n",
       "178                            None   \n",
       "179                            None   \n",
       "180                            None   \n",
       "181  [Investment Manager Assistant]   \n",
       "\n",
       "                                            experience company_names  \\\n",
       "0    [Data Intern, Index Analytics LLC, Baltimore, MD]          None   \n",
       "1    [SMSA (  VP, Operations), ■  Help with the pla...          None   \n",
       "2    [Web Analyst, The University of Texas at Dalla...          None   \n",
       "3    [The Editorial Board, Head of Logistics, Oct 2...          None   \n",
       "4    [InventXYZ, Data Engineering Intern, Aug. 2015...          None   \n",
       "..                                                 ...           ...   \n",
       "177  [Streaming Data Retrieval System              ...          None   \n",
       "178  [Utegration LLC, Houston, TX, Data Analytics I...          None   \n",
       "179  [Project on Covid Social Media Data           ...          None   \n",
       "180  [Stealth Mode Startup, Deep Learning Innovatio...          None   \n",
       "181  [FRM designation. CFA level one exam passed., ...          None   \n",
       "\n",
       "     no_of_pages  total_experience  \\\n",
       "0              1              0.00   \n",
       "1              1              0.00   \n",
       "2              1              2.83   \n",
       "3              2              3.92   \n",
       "4              1              0.75   \n",
       "..           ...               ...   \n",
       "177            1              0.00   \n",
       "178            1              0.00   \n",
       "179            1              0.75   \n",
       "180            1              1.25   \n",
       "181            1              0.00   \n",
       "\n",
       "                                                  text  \n",
       "0    Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...  \n",
       "1    HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...  \n",
       "2    TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...  \n",
       "3    Akanksha Bapna\\nabapna.com ● Email ● Personal ...  \n",
       "4    Mythri Partha \\nHouston, TX | Phone: (281) 725...  \n",
       "..                                                 ...  \n",
       "177  yd2578@columbia.edu·(319) 512-0421·www.zoedong...  \n",
       "178  Jersey City, NJ                               ...  \n",
       "179  Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...  \n",
       "180  Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...  \n",
       "181  Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...  \n",
       "\n",
       "[182 rows x 12 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyres_df = pd.DataFrame(pyresparser)\n",
    "def parse_degree(x):\n",
    "    if x is None:\n",
    "        return ''\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "pyres_df['degree'] = pyres_df['degree'].apply(lambda x: parse_degree(x))\n",
    "pyres_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'university'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'university'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1007fe0c4b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_univ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_degree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'university'"
     ]
    }
   ],
   "source": [
    "resume_df = pd.DataFrame(resumeparser)\n",
    "def parse_univ(x):\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "resume_df['university'] = resume_df['university'].apply(lambda x: parse_univ(x))\n",
    "resume_df['degree'] = resume_df['degree'].apply(lambda x: parse_degree(x))\n",
    "resume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': None, 'last_name': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "nd.search(\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legit name present\n"
     ]
    }
   ],
   "source": [
    "if nd.search('abhiram')['first_name'] != None:\n",
    "    print(\"Legit name present\")\n",
    "else:\n",
    "    print(\"not legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': {'country': {'Canada': 0.033,\n",
       "   'Chile': 0.032,\n",
       "   'Colombia': 0.047,\n",
       "   'France': 0.084,\n",
       "   'United Kingdom': 0.261,\n",
       "   'Nigeria': 0.038,\n",
       "   'Netherlands': 0.074,\n",
       "   'Peru': 0.067,\n",
       "   'United States': 0.306,\n",
       "   'South Africa': 0.058},\n",
       "  'gender': {'Female': 0.008, 'Male': 0.992},\n",
       "  'rank': {'Canada': 26,\n",
       "   'Chile': 158,\n",
       "   'Colombia': 271,\n",
       "   'France': 200,\n",
       "   'United Kingdom': 13,\n",
       "   'Nigeria': 127,\n",
       "   'Netherlands': 18,\n",
       "   'Peru': 52,\n",
       "   'United States': 40,\n",
       "   'South Africa': 76}},\n",
       " 'last_name': {'country': {'Canada': 0.033,\n",
       "   'Cameroon': 0.019,\n",
       "   'France': 0.491,\n",
       "   'United Kingdom': 0.017,\n",
       "   'Ghana': 0.054,\n",
       "   'Italy': 0.016,\n",
       "   'Malaysia': 0.035,\n",
       "   'Nigeria': 0.151,\n",
       "   'United States': 0.122,\n",
       "   'South Africa': 0.063},\n",
       "  'gender': {},\n",
       "  'rank': {'Canada': 171,\n",
       "   'Cameroon': 179,\n",
       "   'France': 12,\n",
       "   'United Kingdom': 2267,\n",
       "   'Ghana': 34,\n",
       "   'Italy': 9110,\n",
       "   'Malaysia': 1022,\n",
       "   'Nigeria': 158,\n",
       "   'United States': 751,\n",
       "   'South Africa': 652}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.search('richard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install names-dataset\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "country_list = ['US','ES','CH','IN']\n",
    "all_names = []\n",
    "for country_code in country_list:\n",
    "    temp = nd.get_top_names(n=10000000, gender=None, country_alpha2=country_code)\n",
    "    all_names.append(temp[country_code]['M'] + temp[country_code]['F'])\n",
    "\n",
    "len(all_names)\n",
    "# k['US']['M'] + k['US']['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WOERKING COCDE DONT EDIt\n",
    "# import sys\n",
    "# import textract, PyPDF2, glob\n",
    "# import nltk\n",
    "# import pandas as pd\n",
    "# from names_dataset import NameDataset\n",
    "# nd = NameDataset()\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('averaged_perceptron_tagger')\n",
    "# # nltk.download('maxent_ne_chunker')\n",
    "# # nltk.download('words')\n",
    "# # nltk.download('stopwords')\n",
    "# import os\n",
    "# #java_path = r\"C:\\Program Files\\Java\\jdk-17.0.2\\bin\\java.exe\"\n",
    "# # os.environ['JAVAHOME'] = java_path\n",
    "# import re\n",
    "# import os\n",
    "# import subprocess\n",
    "# from pdfminer.high_level import extract_text\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "# import nltk\n",
    "# from nltk.tag.stanford import StanfordNERTagger\n",
    "# import pdfplumber\n",
    "# #files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "# PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "# PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "# files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")\n",
    "\n",
    "# tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "# us_names = nd.get_top_names(n=1000, gender='Male', country_alpha2='US')['US']['M']\n",
    "# indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "# \"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "# \"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "# \"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "# \"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "# \"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "# chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "# \"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "# \"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "# \"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "# \"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "# \"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "# \"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "# chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "# indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "# RESERVED_WORDS = [\n",
    "#     'school',\n",
    "#     'college',\n",
    "#     'univers',\n",
    "#     'academy',\n",
    "#     'faculty',\n",
    "#     'institute',\n",
    "#     'faculdades',\n",
    "#     'Schola',\n",
    "#     'schule',\n",
    "#     'lise',\n",
    "#     'lyceum',\n",
    "#     'lycee',\n",
    "#     'polytechnic',\n",
    "#     'kolej',\n",
    "#     'ünivers',\n",
    "#     'okul',\n",
    "#     'University'\n",
    "# ]\n",
    "\n",
    "# def tokenize(i, person=[], education=[], graduation_year=[], phone_nbr=[], emails=[]):\n",
    "#     ###Extract text from files\n",
    "#     try:\n",
    "#         with pdfplumber.open(files1[i]) as pdf:\n",
    "#             first_page = pdf.pages[0]\n",
    "#             txt1 = first_page.extract_text()\n",
    "#     except:\n",
    "#         txt1=\"\"\n",
    "#         person.append(files1[i])\n",
    "#         education.append(\"Err\")\n",
    "#         graduation_year.append(extract_graduation_date(\"Err\"))\n",
    "#         phone_nbr.append(extract_phone_number(\"Err\"))\n",
    "#         emails.append(extract_emails(\"Err\"))\n",
    "#         i+=1\n",
    "#         tokenize(i)\n",
    "#     txt = extract_text(files1[i], codec='utf-8')\n",
    "#     #print(txt)\n",
    "#     words = nltk.word_tokenize(txt) \n",
    "#     return words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails\n",
    "\n",
    "\n",
    "# def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "#     ##Here i is for iterating over the files and doing the process for each file\n",
    "#     flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "#     words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails = tokenize(i)\n",
    "#     #print(txt)\n",
    "#     ##removing stopwords and punctuations\n",
    "#     simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "#     res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "#     res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "#     res2 = [str(res) for res in res1]\n",
    "#     res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "#     ##removing some ascii character\n",
    "#     res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "#     ##removing numbers\n",
    "#     res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "#     ##trying to remove acii characters\n",
    "#     res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "#     ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "#     tagged = tagger.tag(res3)\n",
    "    \n",
    "#     ## appending the names extracted from name_person\n",
    "#     ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "#     try:\n",
    "#         person.append(name_person(tagged,words,res3))\n",
    "#     except:\n",
    "#         flag_success = 0\n",
    "#         #person.append(files1[i])\n",
    "#         pass\n",
    "#     #tagged_list.append(tagged) \n",
    "#     #flag_success = 1\n",
    "#     if flag_success == 1:\n",
    "#         education.append(extract_education(txt))\n",
    "#         graduation_year.append(extract_graduation_date(txt1))\n",
    "#         phone_nbr.append(extract_phone_number(txt))\n",
    "#         emails.append(extract_emails(txt))\n",
    "#     else:\n",
    "#         flag_success == 1\n",
    "#     if i+1 < 5:\n",
    "#         i+=1\n",
    "#         preprocessing(i, person)\n",
    "    \n",
    "#     #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "#     return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "\n",
    "# def name_person(tagged, words,res3):\n",
    "#     #tagged_list = preprocessing(0,[],[])\n",
    "#     #print(tagged_list)\n",
    "#     person = []\n",
    "#     temp_person = []\n",
    "    \n",
    "#     for k in range(10):\n",
    "#         if (res3[k].lower() in indian_last_names) or (res3[k].lower() in chinese_last_names):\n",
    "#             j = k-1\n",
    "#             return res3[j]+\" \" +res3[k]#+\"(Extracted using 1st approach)\"\n",
    "#     for tuple_ele in tagged:\n",
    "#         if \"PERSON\" in tuple_ele[1]:\n",
    "#             temp_person.append(tuple_ele[0])\n",
    "#     ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "#     for word in words:\n",
    "#         if word in temp_person:\n",
    "#             return word\n",
    "\n",
    "# def extract_education(txt):\n",
    "#     edu=set()\n",
    "#     p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "#     for m in re.finditer(p,txt):\n",
    "#         try:\n",
    "#             if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "#                 edu.append(m.group(1))\n",
    "#         except:\n",
    "#             pass\n",
    "#         for word in RESERVED_WORDS:\n",
    "#             if word in m.group(2):\n",
    "#                 edu.add(m.group(2))\n",
    "#     return edu\n",
    "\n",
    "# def extract_graduation_date(txt1):\n",
    "#     dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "#     #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "#     # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "#     x=\"(?is)education.*?(\\d{4})\"\n",
    "#     # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "#     if len(re.findall(x,txt1))==0:\n",
    "#         return None\n",
    "#     return max(re.findall(x,txt1))\n",
    "#     ## for dates\n",
    "#     #for dt in dates:\n",
    "#     #    if dt in txt:\n",
    "#     #        return dt\n",
    "\n",
    "# def extract_phone_number(txt):\n",
    "#     PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "#     phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "#     if phone:\n",
    "#         number = ''.join(phone[0])\n",
    "\n",
    "#         if txt.find(number) >= 0 and len(number) < 16:\n",
    "#             return number\n",
    "#     return None\n",
    "\n",
    "# def extract_emails(txt):\n",
    "#     EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "#     return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "# StanfordNER = preprocessing()\n",
    "# StanfordNER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Iteration 100\n",
      "Iteration 101\n",
      "Iteration 102\n",
      "Iteration 103\n",
      "Iteration 104\n",
      "Iteration 105\n",
      "Iteration 106\n",
      "Iteration 107\n",
      "Iteration 108\n",
      "Iteration 109\n",
      "Iteration 110\n",
      "Iteration 111\n",
      "Iteration 112\n",
      "Iteration 113\n",
      "Iteration 114\n",
      "Iteration 115\n",
      "Iteration 116\n",
      "Iteration 117\n",
      "Iteration 118\n",
      "Iteration 119\n",
      "Iteration 120\n",
      "Iteration 121\n",
      "Iteration 122\n",
      "Iteration 123\n",
      "Iteration 124\n",
      "Iteration 125\n",
      "Iteration 126\n",
      "Iteration 127\n",
      "Iteration 128\n",
      "Iteration 129\n",
      "Iteration 130\n",
      "Iteration 131\n",
      "Iteration 132\n",
      "Iteration 133\n",
      "Iteration 134\n",
      "Iteration 135\n",
      "Iteration 137\n",
      "Iteration 138\n",
      "Iteration 139\n",
      "Iteration 140\n",
      "Iteration 141\n",
      "Iteration 142\n",
      "Iteration 143\n",
      "Iteration 144\n",
      "Iteration 145\n",
      "Iteration 146\n",
      "Iteration 147\n",
      "Iteration 148\n",
      "Iteration 149\n",
      "Iteration 150\n",
      "Iteration 151\n",
      "Iteration 152\n",
      "Iteration 153\n",
      "Iteration 154\n",
      "Iteration 155\n",
      "Iteration 156\n",
      "Iteration 157\n",
      "Iteration 158\n",
      "Iteration 159\n",
      "Iteration 160\n",
      "Iteration 161\n",
      "Iteration 162\n",
      "Iteration 163\n",
      "Iteration 164\n",
      "Iteration 165\n",
      "Iteration 166\n",
      "Iteration 167\n",
      "Iteration 168\n",
      "Iteration 169\n",
      "Iteration 170\n",
      "Iteration 171\n",
      "Iteration 172\n",
      "Iteration 173\n",
      "Iteration 174\n",
      "Iteration 175\n",
      "Iteration 176\n",
      "Iteration 177\n",
      "Iteration 178\n",
      "Iteration 179\n",
      "Iteration 180\n",
      "Iteration 181\n",
      "Iteration 182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>graduation_year</th>\n",
       "      <th>Contact</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>{University of Maryland}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(410)-292-1151</td>\n",
       "      <td>[karthikr2194@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[hpundir@umd.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRTH PATEL</td>\n",
       "      <td>{The University of Texas at Dallas, Nirma Univ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[tirth2410@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>{University of Houston}</td>\n",
       "      <td>2015</td>\n",
       "      <td>(281) 725-7080</td>\n",
       "      <td>[mythripartha8@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qizhe Wang</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[ziyaotingyu@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RACHAN VAMSI</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(475) 685 0166</td>\n",
       "      <td>[rachan_vamsi.bhooshi@uconn.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAGHUVEERA KARTHIK</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>240-713-8296</td>\n",
       "      <td>[rmadireddy1@student.gsu.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAIDA MUKTAR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>443-833-6344</td>\n",
       "      <td>[saidam1@umbc.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Allentown PA</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>+1 4845387112</td>\n",
       "      <td>[adp178@scarletmail.rutgers.edu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                          education  \\\n",
       "0  Karthik Ramanarayana                           {University of Maryland}   \n",
       "1          HARSH PUNDIR                                                 {}   \n",
       "2           TIRTH PATEL  {The University of Texas at Dallas, Nirma Univ...   \n",
       "3        Akanksha Bapna                                                 {}   \n",
       "4         Mythri Partha                            {University of Houston}   \n",
       "5            Qizhe Wang                                                 {}   \n",
       "6          RACHAN VAMSI                                                 {}   \n",
       "7    RAGHUVEERA KARTHIK                                                 {}   \n",
       "8          SAIDA MUKTAR                                                 {}   \n",
       "9          Allentown PA                                                 {}   \n",
       "\n",
       "  graduation_year         Contact                             email  \n",
       "0            2021  (410)-292-1151          [karthikr2194@gmail.com]  \n",
       "1            2020            None                 [hpundir@umd.edu]  \n",
       "2            2021    469-370-9437             [tirth2410@gmail.com]  \n",
       "3            2021            None                                []  \n",
       "4            2015  (281) 725-7080         [mythripartha8@gmail.com]  \n",
       "5            2020            None           [ziyaotingyu@gmail.com]  \n",
       "6            2021  (475) 685 0166  [rachan_vamsi.bhooshi@uconn.edu]  \n",
       "7            2021    240-713-8296     [rmadireddy1@student.gsu.edu]  \n",
       "8            2021    443-833-6344                [saidam1@umbc.edu]  \n",
       "9            2020   +1 4845387112  [adp178@scarletmail.rutgers.edu]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import textract, PyPDF2, glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "import os\n",
    "#java_path = r\"C:\\Program Files\\Java\\jdk-17.0.2\\bin\\java.exe\"\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import pdfplumber\n",
    "#files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")\n",
    "\n",
    "tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "us_names = nd.get_top_names(n=1000, gender='Male', country_alpha2='US')['US']['M']\n",
    "indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "\"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "\"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "\"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "\"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "\"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "\"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "\"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "\"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "\"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "\"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "\"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "    'University'\n",
    "]\n",
    "\n",
    "def tokenize(i, person=[], education=[], graduation_year=[], phone_nbr=[], emails=[]):\n",
    "    ###Extract text from files\n",
    "    try:\n",
    "        with pdfplumber.open(files1[i]) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            txt1 = first_page.extract_text()\n",
    "    except:\n",
    "        txt1=\"\"\n",
    "        person.append(files1[i])\n",
    "        education.append(\"Err\")\n",
    "        graduation_year.append(extract_graduation_date(\"Err\"))\n",
    "        phone_nbr.append(extract_phone_number(\"Err\"))\n",
    "        emails.append(extract_emails(\"Err\"))\n",
    "        i+=1\n",
    "        tokenize(i)\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    #print(txt)\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails\n",
    "\n",
    "\n",
    "def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "    ##Here i is for iterating over the files and doing the process for each file\n",
    "    flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "    words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails = tokenize(i)\n",
    "    #print(txt)\n",
    "    ##removing stopwords and punctuations\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "    ##removing some ascii character\n",
    "    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "    ##removing numbers\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "    ##trying to remove acii characters\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "    ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "    tagged = tagger.tag(res3)\n",
    "    print(\"Iteration {}\".format(i))\n",
    "    ## appending the names extracted from name_person\n",
    "    ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "    try:\n",
    "        person.append(name_person(tagged,words,res3))\n",
    "    except:\n",
    "        flag_success = 0\n",
    "        person.append(\"No name detected\")\n",
    "        pass\n",
    "    #tagged_list.append(tagged) \n",
    "    #flag_success = 1\n",
    "    if flag_success == 1:\n",
    "        education.append(extract_education(txt))\n",
    "        graduation_year.append(extract_graduation_date(txt1))\n",
    "        phone_nbr.append(extract_phone_number(txt))\n",
    "        emails.append(extract_emails(txt))\n",
    "    else:\n",
    "        education.append(\"No education detected\")\n",
    "        graduation_year.append(\"No graduation_year detected\")\n",
    "        phone_nbr.append(\"No phone_nbr detected\")\n",
    "        emails.append(\"No emails detected\")\n",
    "        flag_success == 1\n",
    "        \n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i, person)\n",
    "    \n",
    "    #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "    return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "from nltk.tag import pos_tag\n",
    "def name_person(tagged, words,res3):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    #print(tagged_list)\n",
    "    person = []\n",
    "    temp_person = []\n",
    "    nltk_tagged = pos_tag(res3[:10])\n",
    "    \n",
    "    for k in range(10):\n",
    "        if k<=9:\n",
    "            if nltk_tagged[k][1] == 'NNP' and nltk_tagged[k+1][1] == 'NNP':\n",
    "                nltk_name = nltk_tagged[k][0] +' '+ nltk_tagged[k+1][0] \n",
    "            return nltk_name\n",
    "        print()\n",
    "        if (res3[k].lower() in indian_last_names) or (res3[k].lower() in chinese_last_names):\n",
    "            j = k-1\n",
    "            return res3[j]+\" \" +res3[k]#+\"(Extracted using 1st approach)\"\n",
    "    for tuple_ele in tagged:\n",
    "        if \"PERSON\" in tuple_ele[1]:\n",
    "            temp_person.append(tuple_ele[0])\n",
    "    ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "    for word in words:\n",
    "        if word in temp_person:\n",
    "            return word\n",
    "\n",
    "def extract_education(txt):\n",
    "    edu=set()\n",
    "    p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "    for m in re.finditer(p,txt):\n",
    "        try:\n",
    "            if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "                edu.append(m.group(1))\n",
    "        except:\n",
    "            pass\n",
    "        for word in RESERVED_WORDS:\n",
    "            if word in m.group(2):\n",
    "                edu.add(m.group(2))\n",
    "    return edu\n",
    "\n",
    "def extract_graduation_date(txt1):\n",
    "    dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "    # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "    x=\"(?is)education.*?(\\d{4})\"\n",
    "    # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "    if len(re.findall(x,txt1))==0:\n",
    "        return None\n",
    "    return max(re.findall(x,txt1))\n",
    "    ## for dates\n",
    "    #for dt in dates:\n",
    "    #    if dt in txt:\n",
    "    #        return dt\n",
    "\n",
    "def extract_phone_number(txt):\n",
    "    PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if txt.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_emails(txt):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "StanfordNER = preprocessing()\n",
    "StanfordNER.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>graduation_year</th>\n",
       "      <th>Contact</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KarthikRamanarayana</td>\n",
       "      <td>{University of Maryland}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(410)-292-1151</td>\n",
       "      <td>[karthikr2194@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSHPUNDIR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[hpundir@umd.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRTHPATEL</td>\n",
       "      <td>{The University of Texas at Dallas, Nirma Univ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[tirth2410@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AkankshaBapna</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MythriPartha</td>\n",
       "      <td>{University of Houston}</td>\n",
       "      <td>2015</td>\n",
       "      <td>(281) 725-7080</td>\n",
       "      <td>[mythripartha8@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QizheWang</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[ziyaotingyu@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RACHANVAMSI</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(475) 685 0166</td>\n",
       "      <td>[rachan_vamsi.bhooshi@uconn.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAGHUVEERAKARTHIK</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>240-713-8296</td>\n",
       "      <td>[rmadireddy1@student.gsu.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAIDAMUKTAR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>443-833-6344</td>\n",
       "      <td>[saidam1@umbc.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AllentownPA</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>+1 4845387112</td>\n",
       "      <td>[adp178@scarletmail.rutgers.edu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                          education  \\\n",
       "0  KarthikRamanarayana                           {University of Maryland}   \n",
       "1          HARSHPUNDIR                                                 {}   \n",
       "2           TIRTHPATEL  {The University of Texas at Dallas, Nirma Univ...   \n",
       "3        AkankshaBapna                                                 {}   \n",
       "4         MythriPartha                            {University of Houston}   \n",
       "5            QizheWang                                                 {}   \n",
       "6          RACHANVAMSI                                                 {}   \n",
       "7    RAGHUVEERAKARTHIK                                                 {}   \n",
       "8          SAIDAMUKTAR                                                 {}   \n",
       "9          AllentownPA                                                 {}   \n",
       "\n",
       "  graduation_year         Contact                             email  \n",
       "0            2021  (410)-292-1151          [karthikr2194@gmail.com]  \n",
       "1            2020            None                 [hpundir@umd.edu]  \n",
       "2            2021    469-370-9437             [tirth2410@gmail.com]  \n",
       "3            2021            None                                []  \n",
       "4            2015  (281) 725-7080         [mythripartha8@gmail.com]  \n",
       "5            2020            None           [ziyaotingyu@gmail.com]  \n",
       "6            2021  (475) 685 0166  [rachan_vamsi.bhooshi@uconn.edu]  \n",
       "7            2021    240-713-8296     [rmadireddy1@student.gsu.edu]  \n",
       "8            2021    443-833-6344                [saidam1@umbc.edu]  \n",
       "9            2020   +1 4845387112  [adp178@scarletmail.rutgers.edu]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StanfordNER.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-dd1f0bd70907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(StanfordNER),len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data\\\\ground_truth.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-526432ee95cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data\\ground_truth.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcsv_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mline_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data\\\\ground_truth.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Data\\ground_truth.csv',encoding = 'latin1') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def parse_email(email_str):\\n    email = re.findall(r'[\\\\w.+-]+@[\\\\w-]+\\\\.[\\\\w.-]+', email_str)\\n    if len(email)>0:\\n        return email[0]\\n    else:\\n        return ''\\n\\n\\nground_truth['email'] = ground_truth['email'].apply(lambda x: parse_email(x))\\nground_truth.head()\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ground_truth = pd.read_csv(r\"C:\\Users\\rtd91\\Data\\resume_samples\\output data\\ground_truth.csv\", encoding = 'latin1')\n",
    "# ground_truth = ground_truth.drop(\"Unnamed: 0\",axis=1)\n",
    "# ground_truth[\"graduation_year\"] = ground_truth[\"graduation_year\"].fillna(-1).astype(int)\n",
    "\"\"\"def parse_email(email_str):\n",
    "    email = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', email_str)\n",
    "    if len(email)>0:\n",
    "        return email[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "ground_truth['email'] = ground_truth['email'].apply(lambda x: parse_email(x))\n",
    "ground_truth.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>skills</th>\n",
       "      <th>college_name</th>\n",
       "      <th>degree</th>\n",
       "      <th>designation</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_names</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>total_experience</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>karthikr2194@gmail.com</td>\n",
       "      <td>292-1151</td>\n",
       "      <td>[Communication, Hbase, Statistics, Matrix, Tab...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Data Intern, Index Analytics LLC, Baltimore, MD]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>hpundir@umd.edu</td>\n",
       "      <td>240.423.5453</td>\n",
       "      <td>[Algorithms, Six sigma, Tableau, Big data, Pay...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Business Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[SMSA (  VP, Operations), ■  Help with the pla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX Master</td>\n",
       "      <td>tirth2410@gmail.com</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[Statistics, Algorithms, Tableau, Statistical ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Web Analyst, The University of Texas at Dalla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Retail, Health, Strategy, Ibm, Budget, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelors in Technology, Chemical Engineering</td>\n",
       "      <td>None</td>\n",
       "      <td>[The Editorial Board, Head of Logistics, Oct 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Akanksha Bapna\\nabapna.com ● Email ● Personal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>mythripartha8@gmail.com</td>\n",
       "      <td>725-7080</td>\n",
       "      <td>[Health, Numpy, Email, Access, Pandas, Enginee...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[InventXYZ, Data Engineering Intern, Aug. 2015...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Mythri Partha \\nHouston, TX | Phone: (281) 725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>yd2578@columbia.edu·(319)</td>\n",
       "      <td>512-0421</td>\n",
       "      <td>[Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...</td>\n",
       "      <td>None</td>\n",
       "      <td>M.S. in Computer and Information Science</td>\n",
       "      <td>None</td>\n",
       "      <td>[Streaming Data Retrieval System              ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yd2578@columbia.edu·(319) 512-0421·www.zoedong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Jersey City</td>\n",
       "      <td>zzhan67@stevens.edu</td>\n",
       "      <td>531-8942</td>\n",
       "      <td>[Communication, Algorithms, Statistics, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Utegration LLC, Houston, TX, Data Analytics I...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jersey City, NJ                               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>zz2751@columbia.edu</td>\n",
       "      <td>763-4795</td>\n",
       "      <td>[Health, Statistics, Data analytics, Tableau, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelor of Arts, Mathematics (Statistics), Ec...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Project on Covid Social Media Data           ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Zixiao Huang</td>\n",
       "      <td>zixiao.huang.74@gmail.com</td>\n",
       "      <td>615-977-2160</td>\n",
       "      <td>[Algorithms, Statistics, Tableau, Startup, Big...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[Stealth Mode Startup, Deep Learning Innovatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Stream Bank</td>\n",
       "      <td>zwang160@terpmail.umd.edu</td>\n",
       "      <td>216-6448</td>\n",
       "      <td>[Communication, Strategy, Statistics, Tableau,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>[Investment Manager Assistant]</td>\n",
       "      <td>[FRM designation. CFA level one exam passed., ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                      email mobile_number  \\\n",
       "0    Karthik Ramanarayana     karthikr2194@gmail.com      292-1151   \n",
       "1            HARSH PUNDIR            hpundir@umd.edu  240.423.5453   \n",
       "2               TX Master        tirth2410@gmail.com  469-370-9437   \n",
       "3          Akanksha Bapna                       None          None   \n",
       "4           Mythri Partha    mythripartha8@gmail.com      725-7080   \n",
       "..                    ...                        ...           ...   \n",
       "177   Columbia University  yd2578@columbia.edu·(319)      512-0421   \n",
       "178           Jersey City        zzhan67@stevens.edu      531-8942   \n",
       "179   Columbia University        zz2751@columbia.edu      763-4795   \n",
       "180          Zixiao Huang  zixiao.huang.74@gmail.com  615-977-2160   \n",
       "181           Stream Bank  zwang160@terpmail.umd.edu      216-6448   \n",
       "\n",
       "                                                skills college_name  \\\n",
       "0    [Communication, Hbase, Statistics, Matrix, Tab...         None   \n",
       "1    [Algorithms, Six sigma, Tableau, Big data, Pay...         None   \n",
       "2    [Statistics, Algorithms, Tableau, Statistical ...         None   \n",
       "3    [Retail, Health, Strategy, Ibm, Budget, Tablea...         None   \n",
       "4    [Health, Numpy, Email, Access, Pandas, Enginee...         None   \n",
       "..                                                 ...          ...   \n",
       "177  [Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...         None   \n",
       "178  [Communication, Algorithms, Statistics, Tablea...         None   \n",
       "179  [Health, Statistics, Data analytics, Tableau, ...         None   \n",
       "180  [Algorithms, Statistics, Tableau, Startup, Big...         None   \n",
       "181  [Communication, Strategy, Statistics, Tableau,...         None   \n",
       "\n",
       "                                                degree  \\\n",
       "0                                                        \n",
       "1              Master of Science in Business Analytics   \n",
       "2                                                        \n",
       "3        Bachelors in Technology, Chemical Engineering   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "177           M.S. in Computer and Information Science   \n",
       "178                                                      \n",
       "179  Bachelor of Arts, Mathematics (Statistics), Ec...   \n",
       "180                     Master of Science in Analytics   \n",
       "181                                                      \n",
       "\n",
       "                        designation  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "..                              ...   \n",
       "177                            None   \n",
       "178                            None   \n",
       "179                            None   \n",
       "180                            None   \n",
       "181  [Investment Manager Assistant]   \n",
       "\n",
       "                                            experience company_names  \\\n",
       "0    [Data Intern, Index Analytics LLC, Baltimore, MD]          None   \n",
       "1    [SMSA (  VP, Operations), ■  Help with the pla...          None   \n",
       "2    [Web Analyst, The University of Texas at Dalla...          None   \n",
       "3    [The Editorial Board, Head of Logistics, Oct 2...          None   \n",
       "4    [InventXYZ, Data Engineering Intern, Aug. 2015...          None   \n",
       "..                                                 ...           ...   \n",
       "177  [Streaming Data Retrieval System              ...          None   \n",
       "178  [Utegration LLC, Houston, TX, Data Analytics I...          None   \n",
       "179  [Project on Covid Social Media Data           ...          None   \n",
       "180  [Stealth Mode Startup, Deep Learning Innovatio...          None   \n",
       "181  [FRM designation. CFA level one exam passed., ...          None   \n",
       "\n",
       "     no_of_pages  total_experience  \\\n",
       "0              1              0.00   \n",
       "1              1              0.00   \n",
       "2              1              2.83   \n",
       "3              2              3.92   \n",
       "4              1              0.75   \n",
       "..           ...               ...   \n",
       "177            1              0.00   \n",
       "178            1              0.00   \n",
       "179            1              0.75   \n",
       "180            1              1.25   \n",
       "181            1              0.00   \n",
       "\n",
       "                                                  text  \n",
       "0    Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...  \n",
       "1    HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...  \n",
       "2    TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...  \n",
       "3    Akanksha Bapna\\nabapna.com ● Email ● Personal ...  \n",
       "4    Mythri Partha \\nHouston, TX | Phone: (281) 725...  \n",
       "..                                                 ...  \n",
       "177  yd2578@columbia.edu·(319) 512-0421·www.zoedong...  \n",
       "178  Jersey City, NJ                               ...  \n",
       "179  Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...  \n",
       "180  Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...  \n",
       "181  Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...  \n",
       "\n",
       "[182 rows x 12 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Karthik\n",
       "1      Robert\n",
       "2       TIRTH\n",
       "3    Akanksha\n",
       "4      Mythri\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StanfordNER['name'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(StanfordNER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jose',\n",
       " 'David',\n",
       " 'Michael',\n",
       " 'John',\n",
       " 'Juan',\n",
       " 'Carlos',\n",
       " 'Luis',\n",
       " 'Chris',\n",
       " 'Alex',\n",
       " 'Daniel']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karthik Ramanarayana Karthik Ramanarayana\n",
      "HARSH PUNDIR HARSH PUNDIR\n",
      "TIRTH PATEL TIRTH PATEL\n",
      "Akanksha Bapna Akanksha Bapna\n",
      "Mythri Partha Mythri Partha\n",
      "Qizhe Wang Qizhe Wang\n",
      "RACHAN BHOOSHI RACHAN VAMSI\n",
      "RAGHUVEERA MADIREDDY RAGHUVEERA KARTHIK\n",
      "SAIDA MUKTAR SAIDA MUKTAR\n",
      "Akash Patel Allentown PA\n",
      "AKSHARA ARCOT AKSHARA ARCOT\n",
      "Alexandra Manetas Alexandra Manetas\n",
      "NO MATCH Pranav Gulghane, No name detected. Score = 19\n",
      "NO MATCH Wanting Lu, No name detected. Score = 15\n",
      "NO MATCH Manaswini Nagaraj, No name detected. Score = 18\n",
      "MOHIT MANJARIA MOHIT MANJARIA\n",
      "Yuchen Xie Yuchen Xie\n",
      "NO MATCH Dimple Mehra, No name detected. Score = 29\n",
      "Indupriya Kompi Indupriya Kompi\n",
      "James Domingo James Renier\n",
      "ANYA PATEL ANYA PATEL\n",
      "Ryan Goodwin Ryan Martin\n",
      "Mayank Dubey Mayank Dubey\n",
      "ABHINAY NAGULAPALLI ABHINAY NAGULAPALLI\n",
      "Noah Mattheis Noah Mattheis\n",
      "Abhilash Rajaram Abhilash Rajaram\n",
      "Adam Dinser Adam Dinser\n",
      "NO MATCH ADEOLA ADESOBA, No name detected. Score = 7\n",
      "AJAY MANOHARAN AJAY PRASATH\n",
      "Akshay Kajale Akshay Kajale\n",
      "Alexander Friend ALEXANDER FRIEND\n",
      "Amar Jha Amar Nath\n",
      "Amol Agrawal Amol Agrawal\n",
      "Andrew Lin Andrew Yuchen\n",
      "ANIRUDH SURAM ANIRUDH REDDY\n",
      "ANNAPOORNA GOPALAKRISHNA ANNAPOORNA M\n",
      "Antton Wilbanks Antton Wilbanks\n",
      "ANYA PATEL ANYA PATEL\n",
      "Archit Prem Archit Prem\n",
      "Ashvi Soni Washington DC\n",
      "BAIYU CHEN BAIYU CHEN\n",
      "BHAVIKA CHAVDA BHAVIKA CHAVDA\n",
      "BOKYEUNG KIM BOKYEUNG KIM\n",
      "BON TRINH B O\n",
      "Buka Cakrawala Buka Cakrawala\n",
      "Andrew Decker Andrew Decker\n",
      "NO MATCH C:\\Users\\rtd91\\Data\\resume_samples\\C01-21120201_Jiayue_Fei.pdf, No name detected. Score = 10\n",
      "NO MATCH Cahyarini (Crystal) Hariga, Vienna VA. Score = 29\n",
      "Carlos Perez Carlos Perez\n",
      "Cassin Edwin Cassin Thangam\n",
      "CHENGWEI CHEN CHENGWEI CHEN\n",
      "CHETNA KHANNA CHETNA KHANNA\n",
      "Dhruv Parikh Dhruv Parikh\n",
      "Dhwani Gandhi Dhwani Gandhi\n",
      "NO MATCH Dimple Mehra, No name detected. Score = 29\n",
      "Divya Manku Divya Reddy\n",
      "Elijah Hall Elijah James\n",
      "Evan JONES EVAN W\n",
      "EVAN JOYCE EVAN M\n",
      "Gali Sabyr Gali Sabyr\n",
      "GRISHMA PARAJULI EDUCATION GRISHMA\n",
      "Hamza Abdelghani Hamza Abdelghani\n",
      "Indupriya Kompi Indupriya Kompi\n",
      "Jairo Carreon Jairo Carreon\n",
      "James Domingo James Renier\n",
      "NO MATCH Stellar(Jialu) Xia, Sep . Score = 18\n",
      "NO MATCH Jianbo Gu, No name detected. Score = 24\n",
      "Jiehong Liu Jiehong Liu\n",
      "NO MATCH Jim Liu, No name detected. Score = 9\n",
      "NO MATCH JULIA XIA, No name detected. Score = 8\n",
      "KRISTIN CHEN  KRISTIN JIATING\n",
      "KUNJ MITHAPARA KUNJ MITHAPARA\n",
      "NO MATCH Lejian He, No name detected. Score = 24\n",
      "NO MATCH Manaswini Nagaraj, No name detected. Score = 18\n",
      "Manisha Patel Manisha Patel\n",
      "MASEN BACHLEDA MASEN BACHLEDA\n",
      "Matthew Holcombe Matthew Holcombe\n",
      "MAYURI LALWANI MAYURI LALWANI\n",
      "Mikhail Tokarev Mikhail Tokarev\n",
      "Mistere Abate No name detected\n",
      "NO MATCH Mohammad Zarei, San Diego. Score = 26\n",
      "Mona Eslamijam Mona Eslamijam\n",
      "NO MATCH Nahian Siddique, No name detected. Score = 19\n",
      "NIDHI CHOVATIYA NIDHI CHOVATIYA\n",
      "NIKSON PANIGRAHI NIKSON PANIGRAHI\n",
      "NO MATCH PRATIK SATPUTE, C:\\Users\\rtd91\\Data\\resume_samples\\output data. Score = 3\n",
      "PRAVEEN PANDEY PRATIK SATPUTE\n",
      "NO MATCH Adedamola Olawoye, PRAVEEN PANDEY. Score = 19\n",
      "Aditya Deshpande Adedamola \n",
      "Ali Majidian Aditya Deshpande\n",
      "ANKIT LADE No name detected\n",
      "Anqi Cheng ANKIT HEMANT\n",
      "Arbaaz Mohideen Anqi Cheng\n",
      "NO MATCH Arunava Ray, Greensboro NC. Score = 25\n",
      "NO MATCH ASHUTOSH SHINDE, Arunava Ray. Score = 8\n",
      "BAH KONAN ASHUTOSH SHINDE\n",
      "Bala Harimani BAH YANNICK\n",
      "NO MATCH BONFACE NJUGUNA, Bala Harimani. Score = 7\n",
      "Chaeeun Lim BONFACE NJUGUNA\n",
      "Chantelle Lim Chaeeun \n",
      "CHITRANJAN JOSHI Chantelle Lim\n",
      "NO MATCH Diego Burgos, CHITRANJAN JOSHI. Score = 29\n",
      "ETHAN LAURENCEAU No name detected\n",
      "NO MATCH GIRIJA BANDARU, ETHAN LAURENCEAU. Score = 7\n",
      "Harshitha Prasad GIRIJA BANDARU\n",
      "HARSH PUNDIR Harshitha Prasad\n",
      "Hemanth Kodakandla HARSH PUNDIR\n",
      "NO MATCH Isioma Ochia, Hemanth Reddy. Score = 24\n",
      "NO MATCH JAY PANDYA, Isioma . Score = 12\n",
      "NO MATCH Jessica Rega, No name detected. Score = 14\n",
      "Jishan Ahmed No name detected\n",
      "KALEB SHIKUR KALEB SHIKUR\n",
      "Kanth Juvadi Kanth Juvadi\n",
      "NO MATCH Katherine Pearson, No name detected. Score = 24\n",
      "KRUTI ALLENKI KRUTI GUPTA\n",
      "Kunjan Khatri Kunjan Devendra\n",
      "LEONEL FLORES L E\n",
      "MAAZ SHAIKH  MAAZ ZAHID\n",
      "Naina Grover  Naina Grover\n",
      "Nasir Sarkar Nasir U\n",
      "Nate Tsegaw Nate Tsegaw\n",
      "Navina Sethi  Navina Kaur\n",
      "Rakshit Sinha  Rakshit Sinha\n",
      "REETIKA CHATURVEDI  No name detected\n",
      "Rishitha Muddana  Rishitha Muddana\n",
      "Rohin Bhagavatula  Rohin Bhagavatula\n",
      "SAIDA MUKTAR  SAIDA MUKTAR\n",
      "NO MATCH Saivalini Durvasula , No name detected. Score = 22\n",
      "Sai Malempati Sai Sri\n",
      "NO MATCH Sanat Lal, No name detected. Score = 24\n",
      "Saumil Jariwala  Saumil Sudhir\n",
      "Shalin Shanghavi  Saumil Sudhir\n",
      "NO MATCH Siyu Tao, No name detected. Score = 8\n",
      "SOBANAA JAYAKUMAR  Siyu Tao\n",
      "NO MATCH Srishti Piplani , SOBANAA JAYAKUMAR. Score = 24\n",
      "NO MATCH Srushti Shah, C:\\Users\\rtd91\\Data\\resume_samples\\Resume_Srinath_Narayanan.pdf. Score = 13\n",
      "NO MATCH Taegeun Ohe, Srushti Shah. Score = 17\n",
      "NO MATCH TANISHK PARIHAR , No name detected. Score = 12\n",
      "NO MATCH Tej Gottapu, TANISHK PARIHAR. Score = 23\n",
      "Tezeswi Madarasu Tej Gottapu\n",
      "TIA MOODY  Tezeswi Madarasu\n",
      "NO MATCH VIJAYALAKSHMI GIRIJALA , TIA MOODY. Score = 6\n",
      "NO MATCH WEI-CHEN, LU   , VIJAYALAKSHMI GIRIJALA. Score = 5\n",
      "NO MATCH   Yann Tamraz, WEICHEN LU. Score = 17\n",
      "YENI PEREZ Yann Tamraz\n",
      "YING LU  YENI PEREZ\n",
      "Yingkun Wang Yingkun Wang\n",
      "NO MATCH Yutong Tang , No name detected. Score = 29\n",
      "Richie Lahoti   Greenbelt MD\n",
      "Ricky Lindsey Richie Lahoti\n",
      "ROHITH MALLULA  Ricky Donnell\n",
      "Ruoshi Zhao  ROHITH MALLULA\n",
      "Rutvik Patel Ruoshi Zhao\n",
      "NO MATCH Ryan Goodwin, Rutvik Patel. Score = 8\n",
      "Sahana Halady  Ryan Martin\n",
      "SAI KUMAR San Jose\n",
      "Sam Song SAI KUMAR\n",
      "NO MATCH Senthil Nathan, Sam Song. Score = 18\n",
      "NO MATCH Sharmista Vemulapalli  , Senthil Nathan. Score = 11\n",
      "Shian Chen  Sharmista Vemulapalli\n",
      "NO MATCH Shreya Dharmarajan , Aug . Score = 17\n",
      "SHWETANK ROKADE   EDUCATIONAL BACKGROUND\n",
      "NO MATCH Steven Zhang , Location US. Score = 25\n",
      "NO MATCH Sushant Patankar, Steven H. Score = 25\n",
      "Tanvi Tembhurne No name detected\n",
      "NO MATCH TZUYAO Lin, Tanvi Tembhurne. Score = 8\n",
      "NO MATCH UDAY hiremath, TZUYAO LIN. Score = 9\n",
      "NO MATCH Vanisa ACHAKULVISUT, UDAY M. Score = 16\n",
      "NO MATCH VICTOR JOHNSON, JR, VANISA ACHAKULVISUT. Score = 5\n",
      "Wenmo Sun No name detected\n",
      "NO MATCH Xiaolan Li, Wenmo Sun. Score = 21\n",
      "NO MATCH Yichi Qian, New York. Score = 11\n",
      "Yichuan Hong Yichi Oliver\n",
      "NO MATCH Yifan Liu, RELEVANT EXPERIENCEPROJECTS. Score = 22\n",
      "NO MATCH Yiru Wen, No name detected. Score = 8\n",
      "NO MATCH Yanhan Dong, No name detected. Score = 22\n",
      "Zhengyang Zhang Yunhan Zoe\n",
      "NO MATCH Zhuohan Zhang, Jersey City. Score = 8\n",
      "Zixiao Huang Zhuohan Zhang\n",
      "Ziyong Wang Zixiao Huang\n",
      "181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125, 57)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_tail = ground_truth\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "def metrics(parse_col,ground_truth_col):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    list1 = ground_truth_col.to_list()\n",
    "    list2 = parse_col.to_list()\n",
    "\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "#         print(fuzz.ratio(str.lower(list2[i]), list1[i]))\n",
    "        if list2[i]==None:\n",
    "            fp+=1\n",
    "            continue\n",
    "        try:\n",
    "           \n",
    "            if fuzz.ratio(str.lower(list2[i]), str.lower(list1[i]))>30:\n",
    "                print(list1[i],list2[i])\n",
    "                tp+=1\n",
    "            else:\n",
    "                print(f'NO MATCH {list1[i]}, {list2[i]}. Score = {fuzz.ratio(str.lower(list2[i]), list1[i])}')\n",
    "                fp+=1\n",
    "        except:\n",
    "            \n",
    "            fp+=1\n",
    "    print(i)\n",
    "    i+=1\n",
    "    return tp,fp\n",
    "\n",
    "metric_list = []\n",
    "StanfordNER[\"mobile\"] = 0\n",
    "StanfordNER[\"degree\"] = \"\"\n",
    "### Scoring for pyres parser\n",
    "name = metrics(StanfordNER['name'],ground_truth_tail['name'])\n",
    "# email = metrics(StanfordNER['email'],ground_truth_tail['email'])\n",
    "# mobile = metrics(StanfordNER['Contact'],ground_truth_tail['Contact'])\n",
    "# university = metrics(StanfordNER['education'],ground_truth_tail['education'])\n",
    "# degree = metrics(StanfordNER['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "# metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "name\n",
    "#print(name,email,mobile,university,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Karthik Ramanarayana', 'HARSH PUNDIR', 'TIRTH PATEL', 'Akanksha Bapna', 'Mythri Partha', 'Qizhe Wang', 'RACHAN BHOOSHI', 'RAGHUVEERA MADIREDDY', 'SAIDA MUKTAR', 'Akash Patel', 'AKSHARA ARCOT', 'Alexandra Manetas', 'Pranav Gulghane', 'Wanting Lu', 'Manaswini Nagaraj', 'MOHIT MANJARIA', 'Yuchen Xie', 'Dimple Mehra', 'Indupriya Kompi', 'James Domingo', 'ANYA PATEL', 'Ryan Goodwin', 'Mayank Dubey', 'ABHINAY NAGULAPALLI', 'Noah Mattheis', 'Abhilash Rajaram', 'Adam Dinser', 'ADEOLA ADESOBA', 'AJAY MANOHARAN', 'Akshay Kajale', 'Alexander Friend', 'Amar Jha', 'Amol Agrawal', 'Andrew Lin', 'ANIRUDH SURAM', 'ANNAPOORNA GOPALAKRISHNA', 'Antton Wilbanks', 'ANYA PATEL', 'Archit Prem', 'Ashvi Soni', 'BAIYU CHEN', 'BHAVIKA CHAVDA', 'BOKYEUNG KIM', 'BON TRINH', 'Buka Cakrawala', 'Andrew Decker', 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\C01-21120201_Jiayue_Fei.pdf', 'Cahyarini (Crystal) Hariga', 'Carlos Perez', 'Cassin Edwin', 'CHENGWEI CHEN', 'CHETNA KHANNA', 'Dhruv Parikh', 'Dhwani Gandhi', 'Dimple Mehra', 'Divya Manku', 'Elijah Hall', 'Evan JONES', 'EVAN JOYCE', 'Gali Sabyr', 'GRISHMA PARAJULI', 'Hamza Abdelghani', 'Indupriya Kompi', 'Jairo Carreon', 'James Domingo', 'Stellar(Jialu) Xia', 'Jianbo Gu', 'Jiehong Liu', 'Jim Liu', 'JULIA XIA', 'KRISTIN CHEN ', 'KUNJ MITHAPARA', 'Lejian He', 'Manaswini Nagaraj', 'Manisha Patel', 'MASEN BACHLEDA', 'Matthew Holcombe', 'MAYURI LALWANI', 'Mikhail Tokarev', 'Mistere Abate', 'Mohammad Zarei', 'Mona Eslamijam', 'Nahian Siddique', 'NIDHI CHOVATIYA', 'NIKSON PANIGRAHI', 'PRATIK SATPUTE', 'PRAVEEN PANDEY', 'Adedamola Olawoye', 'Aditya Deshpande', 'Ali Majidian', 'ANKIT LADE', 'Anqi Cheng', 'Arbaaz Mohideen', 'Arunava Ray', 'ASHUTOSH SHINDE', 'BAH KONAN', 'Bala Harimani', 'BONFACE NJUGUNA', 'Chaeeun Lim', 'Chantelle Lim', 'CHITRANJAN JOSHI', 'Diego Burgos', 'ETHAN LAURENCEAU', 'GIRIJA BANDARU', 'Harshitha Prasad', 'HARSH PUNDIR', 'Hemanth Kodakandla', 'Isioma Ochia', 'JAY PANDYA', 'Jessica Rega', 'Jishan Ahmed', nan, 'KALEB SHIKUR', 'Kanth Juvadi', 'Katherine Pearson', 'KRUTI ALLENKI', 'Kunjan Khatri', 'LEONEL FLORES', 'MAAZ SHAIKH ', 'Naina Grover ', 'Nasir Sarkar', 'Nate Tsegaw', 'Navina Sethi ', 'Rakshit Sinha ', 'REETIKA CHATURVEDI ', 'Rishitha Muddana ', 'Rohin Bhagavatula ', 'SAIDA MUKTAR ', 'Saivalini Durvasula ', 'Sai Malempati', 'Sanat Lal', 'Saumil Jariwala ', 'Shalin Shanghavi ', 'Siyu Tao', 'SOBANAA JAYAKUMAR ', 'Srishti Piplani ', 'Srushti Shah', nan, 'Taegeun Ohe', 'TANISHK PARIHAR ', 'Tej Gottapu', 'Tezeswi Madarasu', 'TIA MOODY ', 'VIJAYALAKSHMI GIRIJALA ', 'WEI-CHEN, LU   ', '  Yann Tamraz', 'YENI PEREZ', 'YING LU ', 'Yingkun Wang', 'Yutong Tang ', 'Richie Lahoti  ', 'Ricky Lindsey', 'ROHITH MALLULA ', 'Ruoshi Zhao ', 'Rutvik Patel', 'Ryan Goodwin', 'Sahana Halady ', 'SAI KUMAR', 'Sam Song', 'Senthil Nathan', 'Sharmista Vemulapalli  ', 'Shian Chen ', 'Shreya Dharmarajan ', 'SHWETANK ROKADE  ', 'Steven Zhang ', 'Sushant Patankar', 'Tanvi Tembhurne', 'TZUYAO Lin', 'UDAY hiremath', 'Vanisa ACHAKULVISUT', 'VICTOR JOHNSON, JR', 'Wenmo Sun', 'Xiaolan Li', 'Yichi Qian', 'Yichuan Hong', 'Yifan Liu', 'Yiru Wen', 'Yanhan Dong', 'Zhengyang Zhang', 'Zhuohan Zhang', 'Zixiao Huang', 'Ziyong Wang'] ['Karthik', 'Robert', 'TIRTH', 'Akanksha', 'Mythri', 'Qizhe', 'Python', 'Python', None, 'Akash', 'Sharpe', 'Alexandra', 'Python', 'Wanting', 'Boston', 'Boston', 'Yuchen', 'JOHNSON', 'Indupriya', 'James', 'ANYA', 'Ryan', 'Mayank', None, 'Noah', 'Python', 'Adam', 'Python', 'Jira', 'Akshay', 'Python', 'Nath', 'Amol', 'Yuchen', 'ANIRUDH', 'Middough', 'Antton', 'ANYA', None, 'Django', 'BAIYU', 'Priyanka', 'Dunbar', 'Python', 'Swift', 'Andrew', 'Python', 'Carlos', 'Cassin', 'CHENGWEI', 'CHETNA', 'Mario', 'Dhwani', 'JOHNSON', 'Divya', 'Elijah', 'Evan', 'EVAN', 'Gali', 'Caldwell', 'Hamza', 'Indupriya', 'Jairo', 'James', 'Hong', 'Jianbo', 'Jiehong', 'Jim', None, 'JIATING', 'Python', 'Lejian', 'Boston', 'Manisha', 'MASEN', 'Matthew', 'Bayes', 'Mikhail', 'Pepys', 'Kaplan', 'Mona', 'Siddique', None, 'Boston', 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\output data', None, 'PRAVEEN', None, 'Aditya', 'Ali', 'Django', 'Anqi', None, 'Arunava', 'Python', 'Paul', 'Bala', None, 'Chaeeun', 'Chantelle', 'CHITRANJAN', 'P', 'ETHAN', 'Python', 'Harshitha', 'Robert', 'Hemanth', None, 'Vidyavardhini', None, 'Ahmed', 'Kaleb', 'Maria', 'Katherine', 'KRUTI', 'Devendra', 'Python', 'Bayes', 'Naina', 'Nasir', None, 'Navina', 'Rakshit', 'Jayshree', 'Rishitha', 'Rohin', None, 'Monte', 'Hadoop', 'Python', 'Tesla', 'Tesla', None, 'Siyu', None, 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\Resume_Srinath_Narayanan.pdf', 'Python', 'Srushti', 'Python', 'Python', 'Tableau', 'Tezeswi', 'Python', 'Python', 'WEICHEN', 'Yann', 'Glen', 'Yingkun', 'YING', 'Yutong', 'Richie', 'Ricky', 'Boston', 'Ruoshi', 'Rutvik', 'Ryan', 'Sahana', 'SAI', 'Sam', 'Senthil', 'Sharmista', 'Shian', 'Shreya', None, 'H', 'Sushant', 'Boston', 'TZUYAO', 'UDAY', None, 'VICTOR', 'Wenmo', 'Python', 'Oliver', 'Python', None, 'Yiru', 'Zoe', 'Zhengyang', 'Zhuohan', 'Zixiao', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 182)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = metrics(StanfordNER['name'],ground_truth_tail['name'])\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyres_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b7307270e759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m### Scoring for pyres parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0memail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mobile_number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Contact'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyres_df' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth_tail = ground_truth\n",
    "from fuzzywuzzy import fuzz\n",
    "def metrics(parse_col,ground_truth_col):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    list1 = ground_truth_col.to_list()\n",
    "    list2 = parse_col.to_list()\n",
    "    print(list1,list2)\n",
    "    for i in range(len(list1)):\n",
    "        if list2[i]==None:\n",
    "            fp+=1\n",
    "            continue\n",
    "        try:\n",
    "            if fuzz.ratio(list2[i],list1[i])>50:\n",
    "                tp+=1\n",
    "            else:\n",
    "                \n",
    "                fp+=1\n",
    "        except:\n",
    "            fp+=1\n",
    "    return tp,fp\n",
    "\n",
    "metric_list = []\n",
    "\n",
    "### Scoring for pyres parser\n",
    "\n",
    "name = metrics(pyres_df['name'],ground_truth_tail['name'])\n",
    "# email = metrics(pyres_df['email'],ground_truth_tail['email'])\n",
    "# mobile = metrics(pyres_df['mobile_number'],ground_truth_tail['Contact'])\n",
    "# university = metrics(pyres_df['college_name'],ground_truth_tail['education'])\n",
    "# degree = metrics(pyres_df['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "# metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "\n",
    "# print(name,email,mobile,university,degree)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ee20b93c802b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0memail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phone'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Contact'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muniversity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'education'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Major'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resume_df' is not defined"
     ]
    }
   ],
   "source": [
    "name = metrics(resume_df['name'],ground_truth_tail['name'])\n",
    "email = metrics(resume_df['email'],ground_truth_tail['email'])\n",
    "mobile = metrics(resume_df['phone'],ground_truth_tail['Contact'])\n",
    "university = metrics(resume_df['university'],ground_truth_tail['education'])\n",
    "degree = metrics(pyres_df['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "\n",
    "\n",
    "print(name,email,mobile,university,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>university</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pyres_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resumeparser_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name     email    mobile university    degree\n",
       "pyres_model         (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)\n",
       "resumeparser_model  (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_list,index=['pyres_model','resumeparser_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>university</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stanford_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name     email    mobile university    degree\n",
       "stanford_model  (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_list,index=['stanford_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Things to work on \n",
    "## 1. Resume Parser extracts many universities & stores them in a list. But I only took the 1st item in list to compare against groundtruth\n",
    "## 2. Raj's NER model needs to be compiled & scored\n",
    "## 3. Degree scoring evaluation should be a fuzzy match\n",
    "## 4. Univeristy scoring evaluation should be a fuzzy match \n",
    "## 5. Need to work on extracting Degree like.. M.S , PhD..etc \n",
    "## 6. Both resume parser model & pyres_model has functionalities to extract the technical skills \n",
    "## 7. Can work on feature to extract years of experiance aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Questions\n",
    "1. Will this be used in the front-end(candidate facing) to facilitate their ease of application process? or Is it going to be used by us to shortlist?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75e802e3c978afb93b9b8a4fcdbb12c74d1245b9e37bc1cfa064b95f94c1eab1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
