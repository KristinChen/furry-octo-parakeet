{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "import spacy\n",
    "# spacy.load(\"en_core_web_sm\")\n",
    "from pyresparser import ResumeParser\n",
    "from resume_parser import resumeparse\n",
    "import os\n",
    "import docx2txt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karthik Ramanarayana'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResumeParser(r'C:\\Users\\rtd91\\Data\\resume_samples\\721091408_phonescreening.pdf').get_extracted_data()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721091408_phonescreening.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 02:36:05,327 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\rtd91\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2022-03-27 02:36:12,699 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\rtd91\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2022-03-27 02:36:13,223 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821101104_phonescreening.pdf\n",
      "821101136_phonescreening.pdf\n",
      "821110120.pdf\n",
      "821110440_Mythri.pdf\n",
      "821110445_Qizhe.pdf\n",
      "821110447_rachan.pdf\n",
      "821110448_RAGHUVEERA.pdf\n",
      "821110464_Saida.pdf\n",
      "821110509_Akash.pdf\n",
      "821110510_AKSHARA.pdf\n",
      "821110511_Alexandra.pdf\n",
      "821110527_.pdf\n",
      "821110528_Wanting.pdf\n",
      "821111539_manaswini.pdf\n",
      "821112919_mohit.pdf\n",
      "821112928_yechen.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e6022c2e34c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\resume_parser\\resumeparse.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(file, docx_parser)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_segments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contact_info'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mtotal_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_segments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[0muniversity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_university\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'world-universities.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mdesignition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_designition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\resume_parser\\resumeparse.py\u001b[0m in \u001b[0;36mextract_university\u001b[1;34m(text, file)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_university\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[0muniversities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mcollege_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "directory = r'C:\\Users\\rtd91\\Data\\resume_samples'\n",
    "pdf = []\n",
    "docs = []\n",
    "pyresparser = []\n",
    "resumeparser = []\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    data = {}\n",
    "    if filename.endswith(\".docx\"):\n",
    "        path = os.path.join(directory, filename)\n",
    "        \n",
    "        data = resumeparse.read_file(path)\n",
    "        data['text'] = extract_text_from_docx(path)\n",
    "        \n",
    "              \n",
    "        resumeparser.append(data)\n",
    "        \n",
    "        data = ResumeParser(path).get_extracted_data()\n",
    "        data['text'] = extract_text_from_docx(path)\n",
    "        \n",
    "        pyresparser.append(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif filename.endswith(\".pdf\"):\n",
    "\n",
    "               \n",
    "        path = os.path.join(directory, filename)\n",
    "        \n",
    "        data = resumeparse.read_file(path)\n",
    "        data['text'] = extract_text_from_pdf(path)\n",
    "        \n",
    "              \n",
    "        resumeparser.append(data)\n",
    "        \n",
    "        data = ResumeParser(path).get_extracted_data()\n",
    "        data['text'] = extract_text_from_pdf(path)\n",
    "        \n",
    "        pyresparser.append(data)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>skills</th>\n",
       "      <th>college_name</th>\n",
       "      <th>degree</th>\n",
       "      <th>designation</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_names</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>total_experience</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>karthikr2194@gmail.com</td>\n",
       "      <td>292-1151</td>\n",
       "      <td>[Communication, Hbase, Statistics, Matrix, Tab...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Data Intern, Index Analytics LLC, Baltimore, MD]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>hpundir@umd.edu</td>\n",
       "      <td>240.423.5453</td>\n",
       "      <td>[Algorithms, Six sigma, Tableau, Big data, Pay...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Business Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[SMSA (  VP, Operations), ■  Help with the pla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX Master</td>\n",
       "      <td>tirth2410@gmail.com</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[Statistics, Algorithms, Tableau, Statistical ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Web Analyst, The University of Texas at Dalla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Retail, Health, Strategy, Ibm, Budget, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelors in Technology, Chemical Engineering</td>\n",
       "      <td>None</td>\n",
       "      <td>[The Editorial Board, Head of Logistics, Oct 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Akanksha Bapna\\nabapna.com ● Email ● Personal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>mythripartha8@gmail.com</td>\n",
       "      <td>725-7080</td>\n",
       "      <td>[Health, Numpy, Email, Access, Pandas, Enginee...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[InventXYZ, Data Engineering Intern, Aug. 2015...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Mythri Partha \\nHouston, TX | Phone: (281) 725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>yd2578@columbia.edu·(319)</td>\n",
       "      <td>512-0421</td>\n",
       "      <td>[Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...</td>\n",
       "      <td>None</td>\n",
       "      <td>M.S. in Computer and Information Science</td>\n",
       "      <td>None</td>\n",
       "      <td>[Streaming Data Retrieval System              ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yd2578@columbia.edu·(319) 512-0421·www.zoedong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Jersey City</td>\n",
       "      <td>zzhan67@stevens.edu</td>\n",
       "      <td>531-8942</td>\n",
       "      <td>[Communication, Algorithms, Statistics, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Utegration LLC, Houston, TX, Data Analytics I...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jersey City, NJ                               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>zz2751@columbia.edu</td>\n",
       "      <td>763-4795</td>\n",
       "      <td>[Health, Statistics, Data analytics, Tableau, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelor of Arts, Mathematics (Statistics), Ec...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Project on Covid Social Media Data           ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Zixiao Huang</td>\n",
       "      <td>zixiao.huang.74@gmail.com</td>\n",
       "      <td>615-977-2160</td>\n",
       "      <td>[Algorithms, Statistics, Tableau, Startup, Big...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[Stealth Mode Startup, Deep Learning Innovatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Stream Bank</td>\n",
       "      <td>zwang160@terpmail.umd.edu</td>\n",
       "      <td>216-6448</td>\n",
       "      <td>[Communication, Strategy, Statistics, Tableau,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>[Investment Manager Assistant]</td>\n",
       "      <td>[FRM designation. CFA level one exam passed., ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                      email mobile_number  \\\n",
       "0    Karthik Ramanarayana     karthikr2194@gmail.com      292-1151   \n",
       "1            HARSH PUNDIR            hpundir@umd.edu  240.423.5453   \n",
       "2               TX Master        tirth2410@gmail.com  469-370-9437   \n",
       "3          Akanksha Bapna                       None          None   \n",
       "4           Mythri Partha    mythripartha8@gmail.com      725-7080   \n",
       "..                    ...                        ...           ...   \n",
       "177   Columbia University  yd2578@columbia.edu·(319)      512-0421   \n",
       "178           Jersey City        zzhan67@stevens.edu      531-8942   \n",
       "179   Columbia University        zz2751@columbia.edu      763-4795   \n",
       "180          Zixiao Huang  zixiao.huang.74@gmail.com  615-977-2160   \n",
       "181           Stream Bank  zwang160@terpmail.umd.edu      216-6448   \n",
       "\n",
       "                                                skills college_name  \\\n",
       "0    [Communication, Hbase, Statistics, Matrix, Tab...         None   \n",
       "1    [Algorithms, Six sigma, Tableau, Big data, Pay...         None   \n",
       "2    [Statistics, Algorithms, Tableau, Statistical ...         None   \n",
       "3    [Retail, Health, Strategy, Ibm, Budget, Tablea...         None   \n",
       "4    [Health, Numpy, Email, Access, Pandas, Enginee...         None   \n",
       "..                                                 ...          ...   \n",
       "177  [Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...         None   \n",
       "178  [Communication, Algorithms, Statistics, Tablea...         None   \n",
       "179  [Health, Statistics, Data analytics, Tableau, ...         None   \n",
       "180  [Algorithms, Statistics, Tableau, Startup, Big...         None   \n",
       "181  [Communication, Strategy, Statistics, Tableau,...         None   \n",
       "\n",
       "                                                degree  \\\n",
       "0                                                        \n",
       "1              Master of Science in Business Analytics   \n",
       "2                                                        \n",
       "3        Bachelors in Technology, Chemical Engineering   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "177           M.S. in Computer and Information Science   \n",
       "178                                                      \n",
       "179  Bachelor of Arts, Mathematics (Statistics), Ec...   \n",
       "180                     Master of Science in Analytics   \n",
       "181                                                      \n",
       "\n",
       "                        designation  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "..                              ...   \n",
       "177                            None   \n",
       "178                            None   \n",
       "179                            None   \n",
       "180                            None   \n",
       "181  [Investment Manager Assistant]   \n",
       "\n",
       "                                            experience company_names  \\\n",
       "0    [Data Intern, Index Analytics LLC, Baltimore, MD]          None   \n",
       "1    [SMSA (  VP, Operations), ■  Help with the pla...          None   \n",
       "2    [Web Analyst, The University of Texas at Dalla...          None   \n",
       "3    [The Editorial Board, Head of Logistics, Oct 2...          None   \n",
       "4    [InventXYZ, Data Engineering Intern, Aug. 2015...          None   \n",
       "..                                                 ...           ...   \n",
       "177  [Streaming Data Retrieval System              ...          None   \n",
       "178  [Utegration LLC, Houston, TX, Data Analytics I...          None   \n",
       "179  [Project on Covid Social Media Data           ...          None   \n",
       "180  [Stealth Mode Startup, Deep Learning Innovatio...          None   \n",
       "181  [FRM designation. CFA level one exam passed., ...          None   \n",
       "\n",
       "     no_of_pages  total_experience  \\\n",
       "0              1              0.00   \n",
       "1              1              0.00   \n",
       "2              1              2.83   \n",
       "3              2              3.92   \n",
       "4              1              0.75   \n",
       "..           ...               ...   \n",
       "177            1              0.00   \n",
       "178            1              0.00   \n",
       "179            1              0.75   \n",
       "180            1              1.25   \n",
       "181            1              0.00   \n",
       "\n",
       "                                                  text  \n",
       "0    Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...  \n",
       "1    HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...  \n",
       "2    TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...  \n",
       "3    Akanksha Bapna\\nabapna.com ● Email ● Personal ...  \n",
       "4    Mythri Partha \\nHouston, TX | Phone: (281) 725...  \n",
       "..                                                 ...  \n",
       "177  yd2578@columbia.edu·(319) 512-0421·www.zoedong...  \n",
       "178  Jersey City, NJ                               ...  \n",
       "179  Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...  \n",
       "180  Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...  \n",
       "181  Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...  \n",
       "\n",
       "[182 rows x 12 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyres_df = pd.DataFrame(pyresparser)\n",
    "def parse_degree(x):\n",
    "    if x is None:\n",
    "        return ''\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "pyres_df['degree'] = pyres_df['degree'].apply(lambda x: parse_degree(x))\n",
    "pyres_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'university'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'university'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1007fe0c4b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_univ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_degree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'university'"
     ]
    }
   ],
   "source": [
    "resume_df = pd.DataFrame(resumeparser)\n",
    "def parse_univ(x):\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "resume_df['university'] = resume_df['university'].apply(lambda x: parse_univ(x))\n",
    "resume_df['degree'] = resume_df['degree'].apply(lambda x: parse_degree(x))\n",
    "resume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': None, 'last_name': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "nd.search(\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legit name present\n"
     ]
    }
   ],
   "source": [
    "if nd.search('abhiram')['first_name'] != None:\n",
    "    print(\"Legit name present\")\n",
    "else:\n",
    "    print(\"not legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': {'country': {'Canada': 0.033,\n",
       "   'Chile': 0.032,\n",
       "   'Colombia': 0.047,\n",
       "   'France': 0.084,\n",
       "   'United Kingdom': 0.261,\n",
       "   'Nigeria': 0.038,\n",
       "   'Netherlands': 0.074,\n",
       "   'Peru': 0.067,\n",
       "   'United States': 0.306,\n",
       "   'South Africa': 0.058},\n",
       "  'gender': {'Female': 0.008, 'Male': 0.992},\n",
       "  'rank': {'Canada': 26,\n",
       "   'Chile': 158,\n",
       "   'Colombia': 271,\n",
       "   'France': 200,\n",
       "   'United Kingdom': 13,\n",
       "   'Nigeria': 127,\n",
       "   'Netherlands': 18,\n",
       "   'Peru': 52,\n",
       "   'United States': 40,\n",
       "   'South Africa': 76}},\n",
       " 'last_name': {'country': {'Canada': 0.033,\n",
       "   'Cameroon': 0.019,\n",
       "   'France': 0.491,\n",
       "   'United Kingdom': 0.017,\n",
       "   'Ghana': 0.054,\n",
       "   'Italy': 0.016,\n",
       "   'Malaysia': 0.035,\n",
       "   'Nigeria': 0.151,\n",
       "   'United States': 0.122,\n",
       "   'South Africa': 0.063},\n",
       "  'gender': {},\n",
       "  'rank': {'Canada': 171,\n",
       "   'Cameroon': 179,\n",
       "   'France': 12,\n",
       "   'United Kingdom': 2267,\n",
       "   'Ghana': 34,\n",
       "   'Italy': 9110,\n",
       "   'Malaysia': 1022,\n",
       "   'Nigeria': 158,\n",
       "   'United States': 751,\n",
       "   'South Africa': 652}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.search('richard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install names-dataset\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "country_list = ['US','ES','CH','IN']\n",
    "all_names = []\n",
    "for country_code in country_list:\n",
    "    temp = nd.get_top_names(n=10000000, gender=None, country_alpha2=country_code)\n",
    "    all_names.append(temp[country_code]['M'] + temp[country_code]['F'])\n",
    "\n",
    "len(all_names)\n",
    "# k['US']['M'] + k['US']['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WOERKING COCDE DONT EDIt\n",
    "# import sys\n",
    "# import textract, PyPDF2, glob\n",
    "# import nltk\n",
    "# import pandas as pd\n",
    "# from names_dataset import NameDataset\n",
    "# nd = NameDataset()\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('averaged_perceptron_tagger')\n",
    "# # nltk.download('maxent_ne_chunker')\n",
    "# # nltk.download('words')\n",
    "# # nltk.download('stopwords')\n",
    "# import os\n",
    "# #java_path = r\"C:\\Program Files\\Java\\jdk-17.0.2\\bin\\java.exe\"\n",
    "# # os.environ['JAVAHOME'] = java_path\n",
    "# import re\n",
    "# import os\n",
    "# import subprocess\n",
    "# from pdfminer.high_level import extract_text\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "# import nltk\n",
    "# from nltk.tag.stanford import StanfordNERTagger\n",
    "# import pdfplumber\n",
    "# #files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "# PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "# PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "# files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")\n",
    "\n",
    "# tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "# us_names = nd.get_top_names(n=1000, gender='Male', country_alpha2='US')['US']['M']\n",
    "# indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "# \"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "# \"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "# \"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "# \"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "# \"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "# chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "# \"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "# \"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "# \"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "# \"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "# \"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "# \"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "# chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "# indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "# RESERVED_WORDS = [\n",
    "#     'school',\n",
    "#     'college',\n",
    "#     'univers',\n",
    "#     'academy',\n",
    "#     'faculty',\n",
    "#     'institute',\n",
    "#     'faculdades',\n",
    "#     'Schola',\n",
    "#     'schule',\n",
    "#     'lise',\n",
    "#     'lyceum',\n",
    "#     'lycee',\n",
    "#     'polytechnic',\n",
    "#     'kolej',\n",
    "#     'ünivers',\n",
    "#     'okul',\n",
    "#     'University'\n",
    "# ]\n",
    "\n",
    "# def tokenize(i, person=[], education=[], graduation_year=[], phone_nbr=[], emails=[]):\n",
    "#     ###Extract text from files\n",
    "#     try:\n",
    "#         with pdfplumber.open(files1[i]) as pdf:\n",
    "#             first_page = pdf.pages[0]\n",
    "#             txt1 = first_page.extract_text()\n",
    "#     except:\n",
    "#         txt1=\"\"\n",
    "#         person.append(files1[i])\n",
    "#         education.append(\"Err\")\n",
    "#         graduation_year.append(extract_graduation_date(\"Err\"))\n",
    "#         phone_nbr.append(extract_phone_number(\"Err\"))\n",
    "#         emails.append(extract_emails(\"Err\"))\n",
    "#         i+=1\n",
    "#         tokenize(i)\n",
    "#     txt = extract_text(files1[i], codec='utf-8')\n",
    "#     #print(txt)\n",
    "#     words = nltk.word_tokenize(txt) \n",
    "#     return words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails\n",
    "\n",
    "\n",
    "# def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "#     ##Here i is for iterating over the files and doing the process for each file\n",
    "#     flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "#     words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails = tokenize(i)\n",
    "#     #print(txt)\n",
    "#     ##removing stopwords and punctuations\n",
    "#     simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "#     res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "#     res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "#     res2 = [str(res) for res in res1]\n",
    "#     res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "#     ##removing some ascii character\n",
    "#     res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "#     ##removing numbers\n",
    "#     res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "#     ##trying to remove acii characters\n",
    "#     res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "#     ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "#     tagged = tagger.tag(res3)\n",
    "    \n",
    "#     ## appending the names extracted from name_person\n",
    "#     ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "#     try:\n",
    "#         person.append(name_person(tagged,words,res3))\n",
    "#     except:\n",
    "#         flag_success = 0\n",
    "#         #person.append(files1[i])\n",
    "#         pass\n",
    "#     #tagged_list.append(tagged) \n",
    "#     #flag_success = 1\n",
    "#     if flag_success == 1:\n",
    "#         education.append(extract_education(txt))\n",
    "#         graduation_year.append(extract_graduation_date(txt1))\n",
    "#         phone_nbr.append(extract_phone_number(txt))\n",
    "#         emails.append(extract_emails(txt))\n",
    "#     else:\n",
    "#         flag_success == 1\n",
    "#     if i+1 < 5:\n",
    "#         i+=1\n",
    "#         preprocessing(i, person)\n",
    "    \n",
    "#     #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "#     return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "\n",
    "# def name_person(tagged, words,res3):\n",
    "#     #tagged_list = preprocessing(0,[],[])\n",
    "#     #print(tagged_list)\n",
    "#     person = []\n",
    "#     temp_person = []\n",
    "    \n",
    "#     for k in range(10):\n",
    "#         if (res3[k].lower() in indian_last_names) or (res3[k].lower() in chinese_last_names):\n",
    "#             j = k-1\n",
    "#             return res3[j]+\" \" +res3[k]#+\"(Extracted using 1st approach)\"\n",
    "#     for tuple_ele in tagged:\n",
    "#         if \"PERSON\" in tuple_ele[1]:\n",
    "#             temp_person.append(tuple_ele[0])\n",
    "#     ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "#     for word in words:\n",
    "#         if word in temp_person:\n",
    "#             return word\n",
    "\n",
    "# def extract_education(txt):\n",
    "#     edu=set()\n",
    "#     p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "#     for m in re.finditer(p,txt):\n",
    "#         try:\n",
    "#             if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "#                 edu.append(m.group(1))\n",
    "#         except:\n",
    "#             pass\n",
    "#         for word in RESERVED_WORDS:\n",
    "#             if word in m.group(2):\n",
    "#                 edu.add(m.group(2))\n",
    "#     return edu\n",
    "\n",
    "# def extract_graduation_date(txt1):\n",
    "#     dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "#     #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "#     # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "#     x=\"(?is)education.*?(\\d{4})\"\n",
    "#     # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "#     if len(re.findall(x,txt1))==0:\n",
    "#         return None\n",
    "#     return max(re.findall(x,txt1))\n",
    "#     ## for dates\n",
    "#     #for dt in dates:\n",
    "#     #    if dt in txt:\n",
    "#     #        return dt\n",
    "\n",
    "# def extract_phone_number(txt):\n",
    "#     PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "#     phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "#     if phone:\n",
    "#         number = ''.join(phone[0])\n",
    "\n",
    "#         if txt.find(number) >= 0 and len(number) < 16:\n",
    "#             return number\n",
    "#     return None\n",
    "\n",
    "# def extract_emails(txt):\n",
    "#     EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "#     return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "# StanfordNER = preprocessing()\n",
    "# StanfordNER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>graduation_year</th>\n",
       "      <th>Contact</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>{University of Maryland}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(410)-292-1151</td>\n",
       "      <td>[karthikr2194@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[hpundir@umd.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRTH PATEL</td>\n",
       "      <td>{The University of Texas at Dallas, Nirma Univ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[tirth2410@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>{University of Houston}</td>\n",
       "      <td>2015</td>\n",
       "      <td>(281) 725-7080</td>\n",
       "      <td>[mythripartha8@gmail.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                          education  \\\n",
       "0  Karthik Ramanarayana                           {University of Maryland}   \n",
       "1          HARSH PUNDIR                                                 {}   \n",
       "2           TIRTH PATEL  {The University of Texas at Dallas, Nirma Univ...   \n",
       "3        Akanksha Bapna                                                 {}   \n",
       "4         Mythri Partha                            {University of Houston}   \n",
       "\n",
       "  graduation_year         Contact                      email  \n",
       "0            2021  (410)-292-1151   [karthikr2194@gmail.com]  \n",
       "1            2020            None          [hpundir@umd.edu]  \n",
       "2            2021    469-370-9437      [tirth2410@gmail.com]  \n",
       "3            2021            None                         []  \n",
       "4            2015  (281) 725-7080  [mythripartha8@gmail.com]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WOERKING COCDE DONT EDIt\n",
    "import sys\n",
    "import textract, PyPDF2, glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from names_dataset import NameDataset\n",
    "nd = NameDataset()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "import os\n",
    "#java_path = r\"C:\\Program Files\\Java\\jdk-17.0.2\\bin\\java.exe\"\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import pdfplumber\n",
    "#files = glob.glob(r\"C:\\Users\\rtd91\\Data\\Resume.pdf\")\n",
    "PATH_TO_JAR = \"C:\\\\Users\\rtd91\\Data\\stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"C:\\\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "files1 = glob.glob(r\"C:\\Users\\rtd91\\Data\\resume_samples\\*\")\n",
    "\n",
    "tagger = StanfordNERTagger(model_filename=r\"C:\\Users\\rtd91\\Data\\english.all.3class.distsim.crf.ser.gz\",path_to_jar=r\"C:\\Users\\rtd91\\Data\\stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "us_names = nd.get_top_names(n=1000, gender='Male', country_alpha2='US')['US']['M']\n",
    "indian_last_names = [\"Acharya\", \"Agarwal\", \"Khatri\", \"Ahuja\", \"Anand\", \"Laghari\", \"Patel\",\n",
    "\n",
    "\"Reddy\", \"Bakshi\", \"Anthony\", \"Babu\", \"Arya\", \"Balakrishnan\", \"Banerjee\", \"Burman\", \"Bhatt\", \"Basu\", \"Bedi\", \"Varma\", \"Dara\", \"Dalal\", \"Chowdhury\",\n",
    "\"Chabra\", \"Chadha\", \"Chakrabarti\",\"Chawla\",\"Ahluwalia\", \"Amin\", \"Apte\", \"Datta\", \"Deol\", \"Deshpande\", \"Dewan\", \"Lal\", \"Kohli\", \"Mangal\", \"Malhotra\", \"Jha\",\n",
    "\"Joshi\",\"Kapadia\", \"Iyer\", \"Jain\", \"Khanna\", \"Grover\", \"Kaur\", \"Kashyap\", \"Gokhale\", \"Ghosh\", \"Garg\", \"Dhar\", \"Gandhi\", \"Ganguly\", \"Gupta\", \"Das\", \"Chopra\", \"Dhawan\",\n",
    "\"Dixit\", \"Dubey\", \"Haldar\", \"Kapoor\", \"Khurana\", \"Kulkarni\", \"Madan\", \"Bajwa\", \"Bhasin\", \"Chandra\", \"Chauhan\", \"Deshmukh\", \"Dayal\", \"Dhillon\", \"Goswami\", \"Goel\", \"Mallick\",\n",
    "\"Mahajan\", \"Kumar\", \"Mani\",  \"Gill\", \"Mannan\", \"Biswas\", \"Batra\", \"Bawa\", \"Mehta\", \"Mukherjee\", \"Saxena\", \"Zacharia\", \"Shah\", \"Ray\", \"Rao\", \"Purohit\", \"Parekh\", \"Thakur\", \"Singh\", \"Sharma\", \"Seth\", \"Sachdev\", \"Ranganathan\", \"Puri\", \"Pandey\", \"Naidu\", \"Modi\"]\n",
    "\n",
    "chinese_last_names = [\"Li\", \"Wang\", \"Zhang\", \"Liu\", \"Chen\", \"Yang\", \"Zhao\", \"Huang\", \"Zhou\",\n",
    "\n",
    "\"Wu\", \"Xu\", \"Sun\", \"Hu\", \"Zhu\", \"Gao\", \"Lin\", \"He\", \"Guo\", \"Ma\", \"Luo\", \"Liang\",\n",
    "\n",
    "\"Song\", \"Zheng\", \"Xie\", \"Han\", \"Tang\", \"Feng\", \"Yu\", \"Dong\", \"Xiao\", \"Cheng\",\n",
    "\n",
    "\"Cao\", \"Yuan\", \"Deng\", \"Xu\", \"Fu\", \"Shen\", \"Zeng\", \"Peng\", \"Lu\", \"Su\", \"Lu\", \"Jiang\", \"Cai\", \"Jia\", \"Ding\", \"Wei\", \"Xue\", \"Ye\", \"Yan\", \n",
    "\n",
    "\"Yu\", \"Pan\", \"Du\", \"Dai\", \"Xia\", \"Zhong\", \"Wang\", \"Tian\", \"Ren\", \"Jiang\", \"Fan\", \"Fang\", \"Shi\", \"Yao\", \"Tan\", \"Sheng\", \"Zou\", \"Xiong\", \"Jin\", \"Lu\", \"Hao\", \"Kong\", \"Bai\", \"Cui\",\n",
    "\n",
    "\"Kang\", \"Mao\", \"Qio\", \"Qin\", \"Jiang\", \"Shu\", \"Shi\", \"Gu\", \"Hou\", \"Shao\", \"Meng\", \"Long\", \"Wan\", \"Duan\", \"Zhang\", \"Qian\", \"Tang\", \"Yin\", \"Li\", \"Yi\", \"Chang\", \"Wu\", \n",
    "    \n",
    "\"Qiao\", \"He\", \"Lao\", \"Gong\", \"Wen\"]\n",
    "\n",
    "chinese_last_names = [chinese_last_name.lower() for chinese_last_name in chinese_last_names]\n",
    "indian_last_names = [indian_last_name.lower() for indian_last_name in indian_last_names]\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "    'University'\n",
    "]\n",
    "\n",
    "def tokenize(i, person=[], education=[], graduation_year=[], phone_nbr=[], emails=[]):\n",
    "    ###Extract text from files\n",
    "    try:\n",
    "        with pdfplumber.open(files1[i]) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            txt1 = first_page.extract_text()\n",
    "    except:\n",
    "        txt1=\"\"\n",
    "        person.append(files1[i])\n",
    "        education.append(\"Err\")\n",
    "        graduation_year.append(extract_graduation_date(\"Err\"))\n",
    "        phone_nbr.append(extract_phone_number(\"Err\"))\n",
    "        emails.append(extract_emails(\"Err\"))\n",
    "        i+=1\n",
    "        tokenize(i)\n",
    "    txt = extract_text(files1[i], codec='utf-8')\n",
    "    #print(txt)\n",
    "    words = nltk.word_tokenize(txt) \n",
    "    return words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails\n",
    "\n",
    "\n",
    "def preprocessing(i=0, person=[],education=[],graduation_year=[], phone_nbr=[],emails=[]):\n",
    "    ##Here i is for iterating over the files and doing the process for each file\n",
    "    flag_success = 1 # Trying to check if try was successful for appending person record\n",
    "    words,i,txt,txt1,person,education,graduation_year,phone_nbr,emails = tokenize(i)\n",
    "    #print(txt)\n",
    "    ##removing stopwords and punctuations\n",
    "    simple_strings = [word for word in words if word not in stopwords if word not in string.punctuation]\n",
    "    res = [re.sub(r'[^\\w\\s]', '', word) for word in simple_strings]\n",
    "    res1 = [re.sub(r'^abc(.*?)=[A-Z0-9]+(.*)', r'\\1\\2', word) for word in res]\n",
    "    res2 = [str(res) for res in res1]\n",
    "    res3 = [re.sub(r'/^[A-Za-z]+$/', '', res) for res in res2]\n",
    "    \n",
    "    ##removing some ascii character\n",
    "    res3 = [res.replace(\"ï\",\"i\") if \"ï\" in res else res for res in res3]\n",
    "    \n",
    "    ##removing numbers\n",
    "    res3 = [re.sub('\\d', '', res) for res in res3]\n",
    "    \n",
    "    ##trying to remove acii characters\n",
    "    res3 = [res.encode('ascii',\"ignore\").decode() for res in res3]\n",
    "    \n",
    "    ##using the tagger object of StanfordNER for tagging the entities of the words \n",
    "    tagged = tagger.tag(res3)\n",
    "    \n",
    "    ## appending the names extracted from name_person\n",
    "    ##Since 2 files are of weird format ( 1 is image file converted into .pdf)\n",
    "    try:\n",
    "        person.append(name_person(tagged,words,res3))\n",
    "    except:\n",
    "        flag_success = 0\n",
    "        #person.append(files1[i])\n",
    "        pass\n",
    "    #tagged_list.append(tagged) \n",
    "    #flag_success = 1\n",
    "    if flag_success == 1:\n",
    "        education.append(extract_education(txt))\n",
    "        graduation_year.append(extract_graduation_date(txt1))\n",
    "        phone_nbr.append(extract_phone_number(txt))\n",
    "        emails.append(extract_emails(txt))\n",
    "    else:\n",
    "        flag_success == 1\n",
    "    if i+1 < len(files1):\n",
    "        i+=1\n",
    "        preprocessing(i, person)\n",
    "    \n",
    "    #len(person), len(education), len(graduation_year), len(phone_nbr), len(emails)#\n",
    "    return pd.DataFrame({'name':person,'education':education, 'graduation_year':graduation_year, 'Contact':phone_nbr, 'email':emails})\n",
    "from nltk.tag import pos_tag\n",
    "def name_person(tagged, words,res3):\n",
    "    #tagged_list = preprocessing(0,[],[])\n",
    "    #print(tagged_list)\n",
    "    person = []\n",
    "    temp_person = []\n",
    "    nltk_tagged = pos_tag(res3[:10])\n",
    "    \n",
    "    for k in range(10):\n",
    "        if k<=9:\n",
    "            if nltk_tagged[k][1] == 'NNP' and nltk_tagged[k+1][1] == 'NNP':\n",
    "                nltk_name = nltk_tagged[k][0] +' '+ nltk_tagged[k+1][0] \n",
    "            return nltk_name\n",
    "        if (res3[k].lower() in indian_last_names) or (res3[k].lower() in chinese_last_names):\n",
    "            j = k-1\n",
    "            return res3[j]+\" \" +res3[k]#+\"(Extracted using 1st approach)\"\n",
    "    for tuple_ele in tagged:\n",
    "        if \"PERSON\" in tuple_ele[1]:\n",
    "            temp_person.append(tuple_ele[0])\n",
    "    ## loop through the original words so we can extract from the first words and get the first PERSON\n",
    "    for word in words:\n",
    "        if word in temp_person:\n",
    "            return word\n",
    "\n",
    "def extract_education(txt):\n",
    "    edu=set()\n",
    "    p = re.compile('(EDUCATION)?\\n?(.*?),\\s+(.*?),(.*?)') \n",
    "    for m in re.finditer(p,txt):\n",
    "        try:\n",
    "            if any(x in m.group(1) for x in RESERVED_WORDS):\n",
    "                edu.append(m.group(1))\n",
    "        except:\n",
    "            pass\n",
    "        for word in RESERVED_WORDS:\n",
    "            if word in m.group(2):\n",
    "                edu.add(m.group(2))\n",
    "    return edu\n",
    "\n",
    "def extract_graduation_date(txt1):\n",
    "    dates=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    #/^(?=^abc)(?=.*xyz$)(?=.*123)(?=^(?:(?!456).)*$).*$/\n",
    "    # Working to extract Months :x=\"(?=(\"+'|'.join(dates)+r\"))\"\n",
    "    x=\"(?is)education.*?(\\d{4})\"\n",
    "    # Working to extract year after education: x=\"(?is)education.*?(\\d{4})\"\n",
    "    if len(re.findall(x,txt1))==0:\n",
    "        return None\n",
    "    return max(re.findall(x,txt1))\n",
    "    ## for dates\n",
    "    #for dt in dates:\n",
    "    #    if dt in txt:\n",
    "    #        return dt\n",
    "\n",
    "def extract_phone_number(txt):\n",
    "    PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG, txt)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if txt.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "def extract_emails(txt):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, txt)\n",
    "\n",
    "StanfordNER = preprocessing()\n",
    "StanfordNER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>graduation_year</th>\n",
       "      <th>Contact</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KarthikRamanarayana</td>\n",
       "      <td>{University of Maryland}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(410)-292-1151</td>\n",
       "      <td>[karthikr2194@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSHPUNDIR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[hpundir@umd.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRTHPATEL</td>\n",
       "      <td>{The University of Texas at Dallas, Nirma Univ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[tirth2410@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AkankshaBapna</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MythriPartha</td>\n",
       "      <td>{University of Houston}</td>\n",
       "      <td>2015</td>\n",
       "      <td>(281) 725-7080</td>\n",
       "      <td>[mythripartha8@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QizheWang</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "      <td>[ziyaotingyu@gmail.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RACHANVAMSI</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>(475) 685 0166</td>\n",
       "      <td>[rachan_vamsi.bhooshi@uconn.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAGHUVEERAKARTHIK</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>240-713-8296</td>\n",
       "      <td>[rmadireddy1@student.gsu.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAIDAMUKTAR</td>\n",
       "      <td>{}</td>\n",
       "      <td>2021</td>\n",
       "      <td>443-833-6344</td>\n",
       "      <td>[saidam1@umbc.edu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AllentownPA</td>\n",
       "      <td>{}</td>\n",
       "      <td>2020</td>\n",
       "      <td>+1 4845387112</td>\n",
       "      <td>[adp178@scarletmail.rutgers.edu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                          education  \\\n",
       "0  KarthikRamanarayana                           {University of Maryland}   \n",
       "1          HARSHPUNDIR                                                 {}   \n",
       "2           TIRTHPATEL  {The University of Texas at Dallas, Nirma Univ...   \n",
       "3        AkankshaBapna                                                 {}   \n",
       "4         MythriPartha                            {University of Houston}   \n",
       "5            QizheWang                                                 {}   \n",
       "6          RACHANVAMSI                                                 {}   \n",
       "7    RAGHUVEERAKARTHIK                                                 {}   \n",
       "8          SAIDAMUKTAR                                                 {}   \n",
       "9          AllentownPA                                                 {}   \n",
       "\n",
       "  graduation_year         Contact                             email  \n",
       "0            2021  (410)-292-1151          [karthikr2194@gmail.com]  \n",
       "1            2020            None                 [hpundir@umd.edu]  \n",
       "2            2021    469-370-9437             [tirth2410@gmail.com]  \n",
       "3            2021            None                                []  \n",
       "4            2015  (281) 725-7080         [mythripartha8@gmail.com]  \n",
       "5            2020            None           [ziyaotingyu@gmail.com]  \n",
       "6            2021  (475) 685 0166  [rachan_vamsi.bhooshi@uconn.edu]  \n",
       "7            2021    240-713-8296     [rmadireddy1@student.gsu.edu]  \n",
       "8            2021    443-833-6344                [saidam1@umbc.edu]  \n",
       "9            2020   +1 4845387112  [adp178@scarletmail.rutgers.edu]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StanfordNER.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 182\n"
     ]
    }
   ],
   "source": [
    "print(len(StanfordNER),len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data\\\\ground_truth.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-526432ee95cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data\\ground_truth.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcsv_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mline_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data\\\\ground_truth.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Data\\ground_truth.csv',encoding = 'latin1') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def parse_email(email_str):\\n    email = re.findall(r'[\\\\w.+-]+@[\\\\w-]+\\\\.[\\\\w.-]+', email_str)\\n    if len(email)>0:\\n        return email[0]\\n    else:\\n        return ''\\n\\n\\nground_truth['email'] = ground_truth['email'].apply(lambda x: parse_email(x))\\nground_truth.head()\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ground_truth = pd.read_csv(r\"C:\\Users\\rtd91\\Data\\resume_samples\\output data\\ground_truth.csv\", encoding = 'latin1')\n",
    "# ground_truth = ground_truth.drop(\"Unnamed: 0\",axis=1)\n",
    "# ground_truth[\"graduation_year\"] = ground_truth[\"graduation_year\"].fillna(-1).astype(int)\n",
    "\"\"\"def parse_email(email_str):\n",
    "    email = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', email_str)\n",
    "    if len(email)>0:\n",
    "        return email[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "ground_truth['email'] = ground_truth['email'].apply(lambda x: parse_email(x))\n",
    "ground_truth.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>skills</th>\n",
       "      <th>college_name</th>\n",
       "      <th>degree</th>\n",
       "      <th>designation</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_names</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>total_experience</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karthik Ramanarayana</td>\n",
       "      <td>karthikr2194@gmail.com</td>\n",
       "      <td>292-1151</td>\n",
       "      <td>[Communication, Hbase, Statistics, Matrix, Tab...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Data Intern, Index Analytics LLC, Baltimore, MD]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HARSH PUNDIR</td>\n",
       "      <td>hpundir@umd.edu</td>\n",
       "      <td>240.423.5453</td>\n",
       "      <td>[Algorithms, Six sigma, Tableau, Big data, Pay...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Business Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[SMSA (  VP, Operations), ■  Help with the pla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX Master</td>\n",
       "      <td>tirth2410@gmail.com</td>\n",
       "      <td>469-370-9437</td>\n",
       "      <td>[Statistics, Algorithms, Tableau, Statistical ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Web Analyst, The University of Texas at Dalla...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akanksha Bapna</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Retail, Health, Strategy, Ibm, Budget, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelors in Technology, Chemical Engineering</td>\n",
       "      <td>None</td>\n",
       "      <td>[The Editorial Board, Head of Logistics, Oct 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Akanksha Bapna\\nabapna.com ● Email ● Personal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mythri Partha</td>\n",
       "      <td>mythripartha8@gmail.com</td>\n",
       "      <td>725-7080</td>\n",
       "      <td>[Health, Numpy, Email, Access, Pandas, Enginee...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[InventXYZ, Data Engineering Intern, Aug. 2015...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Mythri Partha \\nHouston, TX | Phone: (281) 725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>yd2578@columbia.edu·(319)</td>\n",
       "      <td>512-0421</td>\n",
       "      <td>[Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...</td>\n",
       "      <td>None</td>\n",
       "      <td>M.S. in Computer and Information Science</td>\n",
       "      <td>None</td>\n",
       "      <td>[Streaming Data Retrieval System              ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yd2578@columbia.edu·(319) 512-0421·www.zoedong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Jersey City</td>\n",
       "      <td>zzhan67@stevens.edu</td>\n",
       "      <td>531-8942</td>\n",
       "      <td>[Communication, Algorithms, Statistics, Tablea...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[Utegration LLC, Houston, TX, Data Analytics I...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jersey City, NJ                               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>zz2751@columbia.edu</td>\n",
       "      <td>763-4795</td>\n",
       "      <td>[Health, Statistics, Data analytics, Tableau, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Bachelor of Arts, Mathematics (Statistics), Ec...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Project on Covid Social Media Data           ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Zixiao Huang</td>\n",
       "      <td>zixiao.huang.74@gmail.com</td>\n",
       "      <td>615-977-2160</td>\n",
       "      <td>[Algorithms, Statistics, Tableau, Startup, Big...</td>\n",
       "      <td>None</td>\n",
       "      <td>Master of Science in Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>[Stealth Mode Startup, Deep Learning Innovatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Stream Bank</td>\n",
       "      <td>zwang160@terpmail.umd.edu</td>\n",
       "      <td>216-6448</td>\n",
       "      <td>[Communication, Strategy, Statistics, Tableau,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>[Investment Manager Assistant]</td>\n",
       "      <td>[FRM designation. CFA level one exam passed., ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                      email mobile_number  \\\n",
       "0    Karthik Ramanarayana     karthikr2194@gmail.com      292-1151   \n",
       "1            HARSH PUNDIR            hpundir@umd.edu  240.423.5453   \n",
       "2               TX Master        tirth2410@gmail.com  469-370-9437   \n",
       "3          Akanksha Bapna                       None          None   \n",
       "4           Mythri Partha    mythripartha8@gmail.com      725-7080   \n",
       "..                    ...                        ...           ...   \n",
       "177   Columbia University  yd2578@columbia.edu·(319)      512-0421   \n",
       "178           Jersey City        zzhan67@stevens.edu      531-8942   \n",
       "179   Columbia University        zz2751@columbia.edu      763-4795   \n",
       "180          Zixiao Huang  zixiao.huang.74@gmail.com  615-977-2160   \n",
       "181           Stream Bank  zwang160@terpmail.umd.edu      216-6448   \n",
       "\n",
       "                                                skills college_name  \\\n",
       "0    [Communication, Hbase, Statistics, Matrix, Tab...         None   \n",
       "1    [Algorithms, Six sigma, Tableau, Big data, Pay...         None   \n",
       "2    [Statistics, Algorithms, Tableau, Statistical ...         None   \n",
       "3    [Retail, Health, Strategy, Ibm, Budget, Tablea...         None   \n",
       "4    [Health, Numpy, Email, Access, Pandas, Enginee...         None   \n",
       "..                                                 ...          ...   \n",
       "177  [Sqlalchemy, Ibm, Workflow, Tableau, P, Nosql,...         None   \n",
       "178  [Communication, Algorithms, Statistics, Tablea...         None   \n",
       "179  [Health, Statistics, Data analytics, Tableau, ...         None   \n",
       "180  [Algorithms, Statistics, Tableau, Startup, Big...         None   \n",
       "181  [Communication, Strategy, Statistics, Tableau,...         None   \n",
       "\n",
       "                                                degree  \\\n",
       "0                                                        \n",
       "1              Master of Science in Business Analytics   \n",
       "2                                                        \n",
       "3        Bachelors in Technology, Chemical Engineering   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "177           M.S. in Computer and Information Science   \n",
       "178                                                      \n",
       "179  Bachelor of Arts, Mathematics (Statistics), Ec...   \n",
       "180                     Master of Science in Analytics   \n",
       "181                                                      \n",
       "\n",
       "                        designation  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "..                              ...   \n",
       "177                            None   \n",
       "178                            None   \n",
       "179                            None   \n",
       "180                            None   \n",
       "181  [Investment Manager Assistant]   \n",
       "\n",
       "                                            experience company_names  \\\n",
       "0    [Data Intern, Index Analytics LLC, Baltimore, MD]          None   \n",
       "1    [SMSA (  VP, Operations), ■  Help with the pla...          None   \n",
       "2    [Web Analyst, The University of Texas at Dalla...          None   \n",
       "3    [The Editorial Board, Head of Logistics, Oct 2...          None   \n",
       "4    [InventXYZ, Data Engineering Intern, Aug. 2015...          None   \n",
       "..                                                 ...           ...   \n",
       "177  [Streaming Data Retrieval System              ...          None   \n",
       "178  [Utegration LLC, Houston, TX, Data Analytics I...          None   \n",
       "179  [Project on Covid Social Media Data           ...          None   \n",
       "180  [Stealth Mode Startup, Deep Learning Innovatio...          None   \n",
       "181  [FRM designation. CFA level one exam passed., ...          None   \n",
       "\n",
       "     no_of_pages  total_experience  \\\n",
       "0              1              0.00   \n",
       "1              1              0.00   \n",
       "2              1              2.83   \n",
       "3              2              3.92   \n",
       "4              1              0.75   \n",
       "..           ...               ...   \n",
       "177            1              0.00   \n",
       "178            1              0.00   \n",
       "179            1              0.75   \n",
       "180            1              1.25   \n",
       "181            1              0.00   \n",
       "\n",
       "                                                  text  \n",
       "0    Karthik Ramanarayana \\n2001 Eastern Avenue, Ba...  \n",
       "1    HARSH PUNDIR \\n3425 Tulane Dr.    Hyattsville,...  \n",
       "2    TIRTH PATEL \\nHolbrook, NY | tirth2410@gmail.c...  \n",
       "3    Akanksha Bapna\\nabapna.com ● Email ● Personal ...  \n",
       "4    Mythri Partha \\nHouston, TX | Phone: (281) 725...  \n",
       "..                                                 ...  \n",
       "177  yd2578@columbia.edu·(319) 512-0421·www.zoedong...  \n",
       "178  Jersey City, NJ                               ...  \n",
       "179  Zhuohan Zhang \\n (617) 763-4795 | zz2751@colum...  \n",
       "180  Zixiao Huang \\n\\nzixiao.huang.74@gmail.com | 6...  \n",
       "181  Ziyong “Willis” Wang \\n(667) 216-6448 ● 5504 S...  \n",
       "\n",
       "[182 rows x 12 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Karthik\n",
       "1      Robert\n",
       "2       TIRTH\n",
       "3    Akanksha\n",
       "4      Mythri\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StanfordNER['name'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(StanfordNER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jose',\n",
       " 'David',\n",
       " 'Michael',\n",
       " 'John',\n",
       " 'Juan',\n",
       " 'Carlos',\n",
       " 'Luis',\n",
       " 'Chris',\n",
       " 'Alex',\n",
       " 'Daniel']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karthik Ramanarayana Karthik Ramanarayana\n",
      "HARSH PUNDIR HARSH PUNDIR\n",
      "TIRTH PATEL TIRTH PATEL\n",
      "Akanksha Bapna Akanksha Bapna\n",
      "Mythri Partha Mythri Partha\n",
      "Qizhe Wang Qizhe Wang\n",
      "RACHAN BHOOSHI RACHAN VAMSI\n",
      "RAGHUVEERA MADIREDDY RAGHUVEERA KARTHIK\n",
      "SAIDA MUKTAR SAIDA MUKTAR\n",
      "Akash Patel Allentown PA\n",
      "AKSHARA ARCOT AKSHARA ARCOT\n",
      "Alexandra Manetas Alexandra Manetas\n",
      "NO MATCH Pranav Gulghane, MOHIT MANJARIA. Score = 28\n",
      "NO MATCH Wanting Lu, Yuchen Xie. Score = 10\n",
      "NO MATCH Manaswini Nagaraj, Indupriya Kompi. Score = 25\n",
      "MOHIT MANJARIA James Renier\n",
      "NO MATCH Yuchen Xie, ANYA PATEL. Score = 30\n",
      "NO MATCH Dimple Mehra, Ryan Martin. Score = 17\n",
      "NO MATCH Indupriya Kompi, Mayank Dubey. Score = 22\n",
      "NO MATCH James Domingo, ABHINAY NAGULAPALLI. Score = 25\n",
      "ANYA PATEL Noah Mattheis\n",
      "NO MATCH Ryan Goodwin, Abhilash Rajaram. Score = 14\n",
      "Mayank Dubey Adam Dinser\n",
      "ABHINAY NAGULAPALLI AJAY PRASATH\n",
      "NO MATCH Noah Mattheis, Akshay Kajale. Score = 15\n",
      "Abhilash Rajaram ALEXANDER FRIEND\n",
      "NO MATCH Adam Dinser, Amar Nath. Score = 30\n",
      "ADEOLA ADESOBA Amol Agrawal\n",
      "NO MATCH AJAY MANOHARAN, Andrew Yuchen. Score = 7\n",
      "Akshay Kajale ANIRUDH REDDY\n",
      "NO MATCH Alexander Friend, ANNAPOORNA M. Score = 21\n",
      "NO MATCH Amar Jha, Antton Wilbanks. Score = 26\n",
      "Amol Agrawal ANYA PATEL\n",
      "NO MATCH Andrew Lin, Archit Prem. Score = 19\n",
      "NO MATCH ANIRUDH SURAM, Washington DC. Score = 8\n",
      "NO MATCH ANNAPOORNA GOPALAKRISHNA, BAIYU CHEN. Score = 6\n",
      "NO MATCH Antton Wilbanks, BHAVIKA CHAVDA. Score = 21\n",
      "NO MATCH ANYA PATEL, BOKYEUNG KIM. Score = 9\n",
      "NO MATCH Archit Prem, B O. Score = 14\n",
      "NO MATCH Ashvi Soni, Buka Cakrawala. Score = 8\n",
      "NO MATCH BAIYU CHEN, Andrew Decker. Score = 9\n",
      "BHAVIKA CHAVDA Vienna VA\n",
      "NO MATCH BOKYEUNG KIM, Carlos Perez. Score = 8\n",
      "BON TRINH Cassin Thangam\n",
      "NO MATCH Buka Cakrawala, CHENGWEI CHEN. Score = 7\n",
      "NO MATCH Andrew Decker, CHETNA KHANNA. Score = 15\n",
      "NO MATCH C:\\Users\\rtd91\\Data\\resume_samples\\C01-21120201_Jiayue_Fei.pdf, Dhruv Parikh. Score = 16\n",
      "Cahyarini (Crystal) Hariga Dhwani Gandhi\n",
      "Carlos Perez Divya Reddy\n",
      "NO MATCH Cassin Edwin, Elijah James. Score = 17\n",
      "CHENGWEI CHEN EVAN W\n",
      "CHETNA KHANNA EVAN M\n",
      "NO MATCH Dhruv Parikh, Gali Sabyr. Score = 18\n",
      "Dhwani Gandhi EDUCATION GRISHMA\n",
      "NO MATCH Dimple Mehra, Hamza Abdelghani. Score = 14\n",
      "Divya Manku Indupriya Kompi\n",
      "Elijah Hall Jairo Carreon\n",
      "NO MATCH Evan JONES, James Renier. Score = 18\n",
      "NO MATCH EVAN JOYCE, Sep . Score = 14\n",
      "NO MATCH Gali Sabyr, Jiehong Liu. Score = 19\n",
      "NO MATCH GRISHMA PARAJULI, KRISTIN JIATING. Score = 6\n",
      "NO MATCH Hamza Abdelghani, KUNJ MITHAPARA. Score = 20\n",
      "Indupriya Kompi Manisha Patel\n",
      "NO MATCH Jairo Carreon, MASEN BACHLEDA. Score = 22\n",
      "James Domingo Matthew Holcombe\n",
      "NO MATCH Stellar(Jialu) Xia, MAYURI LALWANI. Score = 25\n",
      "NO MATCH Jianbo Gu, Mikhail Tokarev. Score = 25\n",
      "NO MATCH Jiehong Liu, San Diego. Score = 30\n",
      "Jim Liu Mona Eslamijam\n",
      "NO MATCH JULIA XIA, NIDHI CHOVATIYA. Score = 8\n",
      "KRISTIN CHEN  NIKSON PANIGRAHI\n",
      "NO MATCH KUNJ MITHAPARA, C:\\Users\\rtd91\\Data\\resume_samples\\output data. Score = 3\n",
      "NO MATCH Lejian He, PRATIK SATPUTE. Score = 26\n",
      "NO MATCH Manaswini Nagaraj, PRAVEEN PANDEY. Score = 13\n",
      "NO MATCH Manisha Patel, Adedamola . Score = 26\n",
      "NO MATCH MASEN BACHLEDA, Aditya Deshpande. Score = 7\n",
      "Matthew Holcombe ANKIT HEMANT\n",
      "NO MATCH MAYURI LALWANI, Anqi Cheng. Score = 8\n",
      "NO MATCH Mikhail Tokarev, Greensboro NC. Score = 14\n",
      "NO MATCH Mistere Abate, Arunava Ray. Score = 8\n",
      "NO MATCH Mohammad Zarei, ASHUTOSH SHINDE. Score = 21\n",
      "Mona Eslamijam BAH YANNICK\n",
      "Nahian Siddique Bala Harimani\n",
      "NO MATCH NIDHI CHOVATIYA, BONFACE NJUGUNA. Score = 7\n",
      "NO MATCH NIKSON PANIGRAHI, Chaeeun . Score = 8\n",
      "NO MATCH PRATIK SATPUTE, Chantelle Lim. Score = 7\n",
      "PRAVEEN PANDEY CHITRANJAN JOSHI\n",
      "NO MATCH Adedamola Olawoye, ETHAN LAURENCEAU. Score = 30\n",
      "Aditya Deshpande GIRIJA BANDARU\n",
      "NO MATCH Ali Majidian, Harshitha Prasad. Score = 21\n",
      "NO MATCH ANKIT LADE, HARSH PUNDIR. Score = 9\n",
      "NO MATCH Anqi Cheng, Hemanth Reddy. Score = 26\n",
      "NO MATCH Arbaaz Mohideen, Isioma . Score = 9\n",
      "Arunava Ray Jishan Ahmed\n",
      "ASHUTOSH SHINDE KALEB SHIKUR\n",
      "NO MATCH BAH KONAN, Kanth Juvadi. Score = 10\n",
      "NO MATCH Bala Harimani, KRUTI GUPTA. Score = 25\n",
      "NO MATCH BONFACE NJUGUNA, Kunjan Devendra. Score = 7\n",
      "NO MATCH Chaeeun Lim, L E. Score = 14\n",
      "NO MATCH Chantelle Lim, MAAZ ZAHID. Score = 9\n",
      "NO MATCH CHITRANJAN JOSHI, Naina Grover. Score = 7\n",
      "NO MATCH Diego Burgos, Nasir U. Score = 11\n",
      "ETHAN LAURENCEAU Nate Tsegaw\n",
      "GIRIJA BANDARU Navina Kaur\n",
      "Harshitha Prasad Rakshit Sinha\n",
      "NO MATCH HARSH PUNDIR, Rishitha Muddana. Score = 7\n",
      "NO MATCH Hemanth Kodakandla, Rohin Bhagavatula. Score = 23\n",
      "Isioma Ochia SAIDA MUKTAR\n",
      "NO MATCH JAY PANDYA, Sai Sri. Score = 12\n",
      "Jessica Rega Saumil Sudhir\n",
      "Jishan Ahmed Saumil Sudhir\n",
      "NO MATCH KALEB SHIKUR, SOBANAA JAYAKUMAR. Score = 7\n",
      "NO MATCH Kanth Juvadi, C:\\Users\\rtd91\\Data\\resume_samples\\Resume_Srinath_Narayanan.pdf. Score = 16\n",
      "NO MATCH Katherine Pearson, Srushti Shah. Score = 7\n",
      "NO MATCH KRUTI ALLENKI, TANISHK PARIHAR. Score = 7\n",
      "NO MATCH Kunjan Khatri, Tej Gottapu. Score = 8\n",
      "NO MATCH LEONEL FLORES, Tezeswi Madarasu. Score = 7\n",
      "NO MATCH MAAZ SHAIKH , TIA MOODY. Score = 10\n",
      "NO MATCH Naina Grover , VIJAYALAKSHMI GIRIJALA. Score = 11\n",
      "NO MATCH Nasir Sarkar, WEICHEN LU. Score = 18\n",
      "Nate Tsegaw Yann Tamraz\n",
      "NO MATCH Navina Sethi , YENI PEREZ. Score = 26\n",
      "NO MATCH Rakshit Sinha , Yingkun Wang. Score = 23\n",
      "REETIKA CHATURVEDI  Greenbelt MD\n",
      "Rishitha Muddana  Richie Lahoti\n",
      "NO MATCH Rohin Bhagavatula , Ricky Donnell. Score = 19\n",
      "NO MATCH SAIDA MUKTAR , ROHITH MALLULA. Score = 7\n",
      "NO MATCH Saivalini Durvasula , Ruoshi Zhao. Score = 19\n",
      "Sai Malempati Rutvik Patel\n",
      "Sanat Lal Ryan Martin\n",
      "Saumil Jariwala  San Jose\n",
      "Shalin Shanghavi  SAI KUMAR\n",
      "Siyu Tao Sam Song\n",
      "NO MATCH SOBANAA JAYAKUMAR , Senthil Nathan. Score = 6\n",
      "Srishti Piplani  Sharmista Vemulapalli\n",
      "NO MATCH Srushti Shah, Aug . Score = 12\n",
      "NO MATCH Taegeun Ohe, Location US. Score = 27\n",
      "NO MATCH TANISHK PARIHAR , Steven H. Score = 8\n",
      "NO MATCH Tej Gottapu, Tanvi Tembhurne. Score = 23\n",
      "NO MATCH Tezeswi Madarasu, TZUYAO LIN. Score = 15\n",
      "TIA MOODY  UDAY M\n",
      "NO MATCH VIJAYALAKSHMI GIRIJALA , VANISA ACHAKULVISUT. Score = 5\n",
      "WEI-CHEN, LU    Wenmo Sun\n",
      "NO MATCH   Yann Tamraz, New York. Score = 29\n",
      "YENI PEREZ Yichi Oliver\n",
      "NO MATCH YING LU , RELEVANT EXPERIENCEPROJECTS. Score = 6\n",
      "Yingkun Wang Yunhan Zoe\n",
      "NO MATCH Yutong Tang , Jersey City. Score = 9\n",
      "NO MATCH Richie Lahoti  , Zhuohan Zhang. Score = 21\n",
      "NO MATCH Ricky Lindsey, Zixiao Huang. Score = 24\n",
      "NO MATCH ROHITH MALLULA , Ziyong . Score = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtd91\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-bf4708fb3f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mStanfordNER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"degree\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m### Scoring for pyres parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m# email = metrics(StanfordNER['email'],ground_truth_tail['email'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# mobile = metrics(StanfordNER['Contact'],ground_truth_tail['Contact'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-bf4708fb3f0b>\u001b[0m in \u001b[0;36mmetrics\u001b[1;34m(parse_col, ground_truth_col)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#         print(fuzz.ratio(str.lower(list2[i]), list1[i]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlist2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ground_truth_tail = ground_truth\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "def metrics(parse_col,ground_truth_col):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    list1 = ground_truth_col.to_list()\n",
    "    list2 = parse_col.to_list()\n",
    "\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "#         print(fuzz.ratio(str.lower(list2[i]), list1[i]))\n",
    "        if list2[i]==None:\n",
    "            fp+=1\n",
    "            continue\n",
    "        try:\n",
    "           \n",
    "            if fuzz.ratio(str.lower(list2[i]), str.lower(list1[i]))>30:\n",
    "                print(list1[i],list2[i])\n",
    "                tp+=1\n",
    "            else:\n",
    "                print(f'NO MATCH {list1[i]}, {list2[i]}. Score = {fuzz.ratio(str.lower(list2[i]), list1[i])}')\n",
    "                fp+=1\n",
    "        except:\n",
    "            \n",
    "            fp+=1\n",
    "    print(i)\n",
    "    i+=1\n",
    "    return tp,fp\n",
    "\n",
    "metric_list = []\n",
    "StanfordNER[\"mobile\"] = 0\n",
    "StanfordNER[\"degree\"] = \"\"\n",
    "### Scoring for pyres parser\n",
    "name = metrics(StanfordNER['name'],ground_truth_tail['name'])\n",
    "# email = metrics(StanfordNER['email'],ground_truth_tail['email'])\n",
    "# mobile = metrics(StanfordNER['Contact'],ground_truth_tail['Contact'])\n",
    "# university = metrics(StanfordNER['education'],ground_truth_tail['education'])\n",
    "# degree = metrics(StanfordNER['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "# metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "name\n",
    "#print(name,email,mobile,university,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Karthik Ramanarayana', 'HARSH PUNDIR', 'TIRTH PATEL', 'Akanksha Bapna', 'Mythri Partha', 'Qizhe Wang', 'RACHAN BHOOSHI', 'RAGHUVEERA MADIREDDY', 'SAIDA MUKTAR', 'Akash Patel', 'AKSHARA ARCOT', 'Alexandra Manetas', 'Pranav Gulghane', 'Wanting Lu', 'Manaswini Nagaraj', 'MOHIT MANJARIA', 'Yuchen Xie', 'Dimple Mehra', 'Indupriya Kompi', 'James Domingo', 'ANYA PATEL', 'Ryan Goodwin', 'Mayank Dubey', 'ABHINAY NAGULAPALLI', 'Noah Mattheis', 'Abhilash Rajaram', 'Adam Dinser', 'ADEOLA ADESOBA', 'AJAY MANOHARAN', 'Akshay Kajale', 'Alexander Friend', 'Amar Jha', 'Amol Agrawal', 'Andrew Lin', 'ANIRUDH SURAM', 'ANNAPOORNA GOPALAKRISHNA', 'Antton Wilbanks', 'ANYA PATEL', 'Archit Prem', 'Ashvi Soni', 'BAIYU CHEN', 'BHAVIKA CHAVDA', 'BOKYEUNG KIM', 'BON TRINH', 'Buka Cakrawala', 'Andrew Decker', 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\C01-21120201_Jiayue_Fei.pdf', 'Cahyarini (Crystal) Hariga', 'Carlos Perez', 'Cassin Edwin', 'CHENGWEI CHEN', 'CHETNA KHANNA', 'Dhruv Parikh', 'Dhwani Gandhi', 'Dimple Mehra', 'Divya Manku', 'Elijah Hall', 'Evan JONES', 'EVAN JOYCE', 'Gali Sabyr', 'GRISHMA PARAJULI', 'Hamza Abdelghani', 'Indupriya Kompi', 'Jairo Carreon', 'James Domingo', 'Stellar(Jialu) Xia', 'Jianbo Gu', 'Jiehong Liu', 'Jim Liu', 'JULIA XIA', 'KRISTIN CHEN ', 'KUNJ MITHAPARA', 'Lejian He', 'Manaswini Nagaraj', 'Manisha Patel', 'MASEN BACHLEDA', 'Matthew Holcombe', 'MAYURI LALWANI', 'Mikhail Tokarev', 'Mistere Abate', 'Mohammad Zarei', 'Mona Eslamijam', 'Nahian Siddique', 'NIDHI CHOVATIYA', 'NIKSON PANIGRAHI', 'PRATIK SATPUTE', 'PRAVEEN PANDEY', 'Adedamola Olawoye', 'Aditya Deshpande', 'Ali Majidian', 'ANKIT LADE', 'Anqi Cheng', 'Arbaaz Mohideen', 'Arunava Ray', 'ASHUTOSH SHINDE', 'BAH KONAN', 'Bala Harimani', 'BONFACE NJUGUNA', 'Chaeeun Lim', 'Chantelle Lim', 'CHITRANJAN JOSHI', 'Diego Burgos', 'ETHAN LAURENCEAU', 'GIRIJA BANDARU', 'Harshitha Prasad', 'HARSH PUNDIR', 'Hemanth Kodakandla', 'Isioma Ochia', 'JAY PANDYA', 'Jessica Rega', 'Jishan Ahmed', nan, 'KALEB SHIKUR', 'Kanth Juvadi', 'Katherine Pearson', 'KRUTI ALLENKI', 'Kunjan Khatri', 'LEONEL FLORES', 'MAAZ SHAIKH ', 'Naina Grover ', 'Nasir Sarkar', 'Nate Tsegaw', 'Navina Sethi ', 'Rakshit Sinha ', 'REETIKA CHATURVEDI ', 'Rishitha Muddana ', 'Rohin Bhagavatula ', 'SAIDA MUKTAR ', 'Saivalini Durvasula ', 'Sai Malempati', 'Sanat Lal', 'Saumil Jariwala ', 'Shalin Shanghavi ', 'Siyu Tao', 'SOBANAA JAYAKUMAR ', 'Srishti Piplani ', 'Srushti Shah', nan, 'Taegeun Ohe', 'TANISHK PARIHAR ', 'Tej Gottapu', 'Tezeswi Madarasu', 'TIA MOODY ', 'VIJAYALAKSHMI GIRIJALA ', 'WEI-CHEN, LU   ', '  Yann Tamraz', 'YENI PEREZ', 'YING LU ', 'Yingkun Wang', 'Yutong Tang ', 'Richie Lahoti  ', 'Ricky Lindsey', 'ROHITH MALLULA ', 'Ruoshi Zhao ', 'Rutvik Patel', 'Ryan Goodwin', 'Sahana Halady ', 'SAI KUMAR', 'Sam Song', 'Senthil Nathan', 'Sharmista Vemulapalli  ', 'Shian Chen ', 'Shreya Dharmarajan ', 'SHWETANK ROKADE  ', 'Steven Zhang ', 'Sushant Patankar', 'Tanvi Tembhurne', 'TZUYAO Lin', 'UDAY hiremath', 'Vanisa ACHAKULVISUT', 'VICTOR JOHNSON, JR', 'Wenmo Sun', 'Xiaolan Li', 'Yichi Qian', 'Yichuan Hong', 'Yifan Liu', 'Yiru Wen', 'Yanhan Dong', 'Zhengyang Zhang', 'Zhuohan Zhang', 'Zixiao Huang', 'Ziyong Wang'] ['Karthik', 'Robert', 'TIRTH', 'Akanksha', 'Mythri', 'Qizhe', 'Python', 'Python', None, 'Akash', 'Sharpe', 'Alexandra', 'Python', 'Wanting', 'Boston', 'Boston', 'Yuchen', 'JOHNSON', 'Indupriya', 'James', 'ANYA', 'Ryan', 'Mayank', None, 'Noah', 'Python', 'Adam', 'Python', 'Jira', 'Akshay', 'Python', 'Nath', 'Amol', 'Yuchen', 'ANIRUDH', 'Middough', 'Antton', 'ANYA', None, 'Django', 'BAIYU', 'Priyanka', 'Dunbar', 'Python', 'Swift', 'Andrew', 'Python', 'Carlos', 'Cassin', 'CHENGWEI', 'CHETNA', 'Mario', 'Dhwani', 'JOHNSON', 'Divya', 'Elijah', 'Evan', 'EVAN', 'Gali', 'Caldwell', 'Hamza', 'Indupriya', 'Jairo', 'James', 'Hong', 'Jianbo', 'Jiehong', 'Jim', None, 'JIATING', 'Python', 'Lejian', 'Boston', 'Manisha', 'MASEN', 'Matthew', 'Bayes', 'Mikhail', 'Pepys', 'Kaplan', 'Mona', 'Siddique', None, 'Boston', 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\output data', None, 'PRAVEEN', None, 'Aditya', 'Ali', 'Django', 'Anqi', None, 'Arunava', 'Python', 'Paul', 'Bala', None, 'Chaeeun', 'Chantelle', 'CHITRANJAN', 'P', 'ETHAN', 'Python', 'Harshitha', 'Robert', 'Hemanth', None, 'Vidyavardhini', None, 'Ahmed', 'Kaleb', 'Maria', 'Katherine', 'KRUTI', 'Devendra', 'Python', 'Bayes', 'Naina', 'Nasir', None, 'Navina', 'Rakshit', 'Jayshree', 'Rishitha', 'Rohin', None, 'Monte', 'Hadoop', 'Python', 'Tesla', 'Tesla', None, 'Siyu', None, 'C:\\\\Users\\\\rtd91\\\\Data\\\\resume_samples\\\\Resume_Srinath_Narayanan.pdf', 'Python', 'Srushti', 'Python', 'Python', 'Tableau', 'Tezeswi', 'Python', 'Python', 'WEICHEN', 'Yann', 'Glen', 'Yingkun', 'YING', 'Yutong', 'Richie', 'Ricky', 'Boston', 'Ruoshi', 'Rutvik', 'Ryan', 'Sahana', 'SAI', 'Sam', 'Senthil', 'Sharmista', 'Shian', 'Shreya', None, 'H', 'Sushant', 'Boston', 'TZUYAO', 'UDAY', None, 'VICTOR', 'Wenmo', 'Python', 'Oliver', 'Python', None, 'Yiru', 'Zoe', 'Zhengyang', 'Zhuohan', 'Zixiao', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 182)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = metrics(StanfordNER['name'],ground_truth_tail['name'])\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyres_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b7307270e759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m### Scoring for pyres parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0memail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mobile_number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Contact'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyres_df' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth_tail = ground_truth\n",
    "from fuzzywuzzy import fuzz\n",
    "def metrics(parse_col,ground_truth_col):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    list1 = ground_truth_col.to_list()\n",
    "    list2 = parse_col.to_list()\n",
    "    print(list1,list2)\n",
    "    for i in range(len(list1)):\n",
    "        if list2[i]==None:\n",
    "            fp+=1\n",
    "            continue\n",
    "        try:\n",
    "            if fuzz.ratio(list2[i],list1[i])>50:\n",
    "                tp+=1\n",
    "            else:\n",
    "                \n",
    "                fp+=1\n",
    "        except:\n",
    "            fp+=1\n",
    "    return tp,fp\n",
    "\n",
    "metric_list = []\n",
    "\n",
    "### Scoring for pyres parser\n",
    "\n",
    "name = metrics(pyres_df['name'],ground_truth_tail['name'])\n",
    "# email = metrics(pyres_df['email'],ground_truth_tail['email'])\n",
    "# mobile = metrics(pyres_df['mobile_number'],ground_truth_tail['Contact'])\n",
    "# university = metrics(pyres_df['college_name'],ground_truth_tail['education'])\n",
    "# degree = metrics(pyres_df['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "# metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "\n",
    "# print(name,email,mobile,university,degree)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ee20b93c802b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0memail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phone'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Contact'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muniversity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'university'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'education'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Major'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resume_df' is not defined"
     ]
    }
   ],
   "source": [
    "name = metrics(resume_df['name'],ground_truth_tail['name'])\n",
    "email = metrics(resume_df['email'],ground_truth_tail['email'])\n",
    "mobile = metrics(resume_df['phone'],ground_truth_tail['Contact'])\n",
    "university = metrics(resume_df['university'],ground_truth_tail['education'])\n",
    "degree = metrics(pyres_df['degree'],ground_truth_tail['Major'])\n",
    "\n",
    "metric_list.append({'name':name,'email':email,'mobile':mobile,'university':university,'degree':degree})\n",
    "\n",
    "\n",
    "print(name,email,mobile,university,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>university</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pyres_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resumeparser_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name     email    mobile university    degree\n",
       "pyres_model         (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)\n",
       "resumeparser_model  (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_list,index=['pyres_model','resumeparser_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>university</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stanford_model</th>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>(0, 182)</td>\n",
       "      <td>(0, 182)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name     email    mobile university    degree\n",
       "stanford_model  (0, 182)  (0, 182)  (87, 95)   (0, 182)  (0, 182)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_list,index=['stanford_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Things to work on \n",
    "## 1. Resume Parser extracts many universities & stores them in a list. But I only took the 1st item in list to compare against groundtruth\n",
    "## 2. Raj's NER model needs to be compiled & scored\n",
    "## 3. Degree scoring evaluation should be a fuzzy match\n",
    "## 4. Univeristy scoring evaluation should be a fuzzy match \n",
    "## 5. Need to work on extracting Degree like.. M.S , PhD..etc \n",
    "## 6. Both resume parser model & pyres_model has functionalities to extract the technical skills \n",
    "## 7. Can work on feature to extract years of experiance aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Questions\n",
    "1. Will this be used in the front-end(candidate facing) to facilitate their ease of application process? or Is it going to be used by us to shortlist?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75e802e3c978afb93b9b8a4fcdbb12c74d1245b9e37bc1cfa064b95f94c1eab1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
