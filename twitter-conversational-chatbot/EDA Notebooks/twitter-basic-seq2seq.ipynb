{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "39717eda-bc31-4143-a87f-bac4eea63678",
    "_uuid": "e30b174032d05dd0048b02ca268f3b31cf5b183e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "keras:2.4.3\n",
      "pandas:1.4.1\n",
      "sklearn:1.0.2\n",
      "nltk:3.6.7\n",
      "numpy:1.22.3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "print('Library versions:')\n",
    "\n",
    "import keras\n",
    "print(f'keras:{keras.__version__}')\n",
    "import pandas as pd\n",
    "print(f'pandas:{pd.__version__}')\n",
    "import sklearn\n",
    "print(f'sklearn:{sklearn.__version__}')\n",
    "import nltk\n",
    "print(f'nltk:{nltk.__version__}')\n",
    "import numpy as np\n",
    "print(f'numpy:{np.__version__}')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm # Special jupyter notebook progress bar ðŸ’«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24fb3871-ea0e-4b9e-b21c-ed76ccefc823",
    "_uuid": "ec0199c0dff22aeed45b9dbe1e7bf659e9449fe9"
   },
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3217b16f-bf7c-4fed-bbea-d74e09a537e4",
    "_uuid": "6509f5443163b0d11f1a003bb8a28b79a340f192",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8192 - large enough for demonstration, larger values make network training slower\n",
    "MAX_VOCAB_SIZE = 2**13\n",
    "# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n",
    "# but result in slower training and larger networks\n",
    "MAX_MESSAGE_LEN = 30  \n",
    "# Embedding size for words - gives a trade off between expressivity of words and network size\n",
    "EMBEDDING_SIZE = 100\n",
    "# Embedding size for whole messages, same trade off as word embeddings\n",
    "CONTEXT_SIZE = 100\n",
    "# Larger batch sizes generally reach the average response faster, but small batch sizes are\n",
    "# required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\n",
    "BATCH_SIZE = 4\n",
    "# Helps regularize network and prevent overfitting.\n",
    "DROPOUT = 0.2\n",
    "# High learning rate helps model reach average response faster, but can make it hard to \n",
    "# converge on nuanced responses\n",
    "LEARNING_RATE=0.005\n",
    "\n",
    "# Tokens needed for seq2seq\n",
    "UNK = 0  # words that aren't found in the vocab\n",
    "PAD = 1  # after message has finished, this fills all remaining vector positions\n",
    "START = 2  # provided to the model at position 0 for every response predicted\n",
    "\n",
    "# Implementaiton detail for allowing this to be run in Kaggle's notebook hardware\n",
    "SUB_BATCH_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea67c840-2d02-45df-be0b-ec5746e72028",
    "_uuid": "9f99990233daf26e624d6e0a735b8a3559eb54c9"
   },
   "source": [
    "## Data Prep\n",
    "Here, we'll prepare the data for training our seq2seq model, including:\n",
    "\n",
    "- Replace screen names with `@__sn__` token to show model the commonality between them\n",
    "- Build a vocab to turn tokens into integers suitable for our seq2seq model\n",
    "- Tokenize input and target text into fixed size vectors\n",
    "- Partition our dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e83096b-f411-494a-962b-80a76122e674",
    "_uuid": "8989bdc2a930380aa52af0ef6a296906d313715c"
   },
   "source": [
    "### Data Loading and Reshaping\n",
    "Pulled from [this kernel](https://www.kaggle.com/soaxelbrooke/first-inbound-and-response-tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9f2bbcb-e843-42d9-81ec-70be9465c4a8",
    "_uuid": "0c3d1708c29764aceb980a93bd9ac61fdf852a22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tweets = pd.read_csv('../input/twcs/twcs.csv')\n",
    "\n",
    "first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n",
    "\n",
    "inbounds_and_outbounds = pd.merge(first_inbound, tweets, left_on='tweet_id', \n",
    "                                  right_on='in_response_to_tweet_id').sample(frac=1)\n",
    "\n",
    "# Filter to only outbound replies (from companies)\n",
    "inbounds_and_outbounds = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n",
    "\n",
    "tqdm().pandas()  # Enable tracking of progress in dataframe `apply` calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1510aa86-71ce-41b6-bda9-6bec1206836b",
    "_uuid": "67186b1ace1e3fff0fed9ec9fe6fd0c59f009a5b"
   },
   "outputs": [],
   "source": [
    "print(f'Data shape: {inbounds_and_outbounds.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0afc1aa-a34f-4ea5-b34e-2fb21b5da62e",
    "_uuid": "2478590bbc0924cc8e8dd127ac4fea8019f8b3f3"
   },
   "source": [
    "### Tokenizing and Vocab Build\n",
    "\n",
    "We'll use NLTK's `casual_tokenize`, which handles a lot of corner cases found in social media data (\"casual\" text data) along with scitkit learn's `CountVectorizer`.  We won't use the actual `CountVectorizer`, just use it as a convenient vocabulary builder, which we'll apply with functions that turn text into \"word indexes\" - integers that represent each word - and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cab88942-373b-42fa-ac15-f341b19ced6f",
    "_uuid": "eccc4fdc8c0770ccb650c2fea30d7c4c5644ee4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inbounds_and_outbounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1efdae4f-8ff9-4242-be79-06916771da66",
    "_uuid": "44f124ae8898978cf5c1e571da976888f8229894"
   },
   "outputs": [],
   "source": [
    "# Replace anonymized screen names with common token @__sn__\n",
    "def sn_replace(match):\n",
    "    _sn = match.group(2).lower()\n",
    "    if not _sn.isnumeric():\n",
    "        # This is a company screen name\n",
    "        return match.group(1) + match.group(2)\n",
    "    return '@__sn__'\n",
    "\n",
    "sn_re = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\n",
    "print(\"Replacing anonymized screen names in X...\")\n",
    "x_text = inbounds_and_outbounds.text_x.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))\n",
    "print(\"Replacing anonymized screen names in Y...\")\n",
    "y_text = inbounds_and_outbounds.text_y.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eba2c6e6-f57e-438e-983f-d19c5156c817",
    "_uuid": "e3f3adeb6aebe8a90bdd7bfcd4fd6cff797d85ad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(tokenizer=casual_tokenize, max_features=MAX_VOCAB_SIZE - 3)\n",
    "print(\"Fitting CountVectorizer on X and Y text data...\")\n",
    "count_vec.fit(tqdm(x_text + y_text))\n",
    "analyzer = count_vec.build_analyzer()\n",
    "vocab = {k: v + 3 for k, v in count_vec.vocabulary_.items()}\n",
    "vocab['__unk__'] = UNK\n",
    "vocab['__pad__'] = PAD\n",
    "vocab['__start__'] = START\n",
    "# Used to turn seq2seq predictions into human readable strings\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "print(f\"Learned vocab of {len(vocab)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c01e85bb-974d-4bf3-8eb1-b5893ce8f284",
    "_uuid": "893cae536132c7477699350574f5de0fd7a60a3f"
   },
   "source": [
    "### Vocab Helper Functions\n",
    "These helper functions take strings and turn them into word indexes used by the actual seq2seq models.  This turns something like \"This is how we do it.\" into a padded array of integers, like [153, 4, 643, 48, 94, 54, 8, 0, 0, 0].  We'll apply the `to_word_idx` function to our text data to get our `N x MESSAGE_LEN` training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d04887da-5ca9-4380-949e-7f7923a85df8",
    "_uuid": "b32c76052be9f5493b7eeee1cf40435e438eb3ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_word_idx(sentence):\n",
    "    full_length = [vocab.get(tok, UNK) for tok in analyzer(sentence)] + [PAD] * MAX_MESSAGE_LEN\n",
    "    return full_length[:MAX_MESSAGE_LEN]\n",
    "\n",
    "def from_word_idx(word_idxs):\n",
    "    return ' '.join(reverse_vocab[idx] for idx in word_idxs if idx != PAD).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4b791c1-48a3-4cff-895f-a9140a93c831",
    "_uuid": "3ac8190e6d5985151dbfd5a11a502a389279bad7"
   },
   "outputs": [],
   "source": [
    "# Make sure our helpers work as expected...\n",
    "x_text.head().apply(to_word_idx).apply(from_word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1977e30-2cc7-4786-91b2-10dbaa63f17d",
    "_uuid": "74951a1b74353c31e5e64a68d0f329f1efa9543d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Calculating word indexes for X...\")\n",
    "x = pd.np.vstack(x_text.progress_apply(to_word_idx).values)\n",
    "print(\"Calculating word indexes for Y...\")\n",
    "y = pd.np.vstack(y_text.progress_apply(to_word_idx).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edebf7ab-0495-4e38-9900-7a9652cdb2d0",
    "_uuid": "f9f842020ecb321805c9032d20b01f1ffe6f5885"
   },
   "source": [
    "### Train / Test Split\n",
    "Here, we split our data into training and test sets.  For simplicity, we use a random split, which may result in different distributions between the training and test set, but we won't worry about that for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "67b9f6e4-2619-48c9-9339-da6637e646d3",
    "_uuid": "d4f3b7134ac395a1952f8dc816cedad201cdf0a4"
   },
   "outputs": [],
   "source": [
    "all_idx = list(range(len(x)))\n",
    "train_idx = set(random.sample(all_idx, int(0.8 * len(all_idx))))\n",
    "test_idx = {idx for idx in all_idx if idx not in train_idx}\n",
    "\n",
    "train_x = x[list(train_idx)]\n",
    "test_x = x[list(test_idx)]\n",
    "train_y = y[list(train_idx)]\n",
    "test_y = y[list(test_idx)]\n",
    "\n",
    "assert train_x.shape == train_y.shape\n",
    "assert test_x.shape == test_y.shape\n",
    "\n",
    "print(f'Training data of shape {train_x.shape} and test data of shape {test_x.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9764b6aa-13e3-409e-939e-64f7f7083350",
    "_uuid": "89c827fa1a1aabc990e6b04b4a4839bd04b9b42c"
   },
   "source": [
    "## Model Creation\n",
    "We'll create and compile the model here.  It will consist of the following components:\n",
    "\n",
    "- Shared word embeddings\n",
    "  - A shared embedding layer that turns word indexes (a sparse representation) into a dense/compressed representation.  This embeds both the request from the customer, and also the last words uttered by the model that are fed back into the model.\n",
    "- Encoder RNN\n",
    "  - In this case, a single LSTM layer.  This encodes the whole input sentence into a context vector (or thought vector) that represents completely what the customer is saying, and produces a single output.\n",
    "- Decoder RNN\n",
    "  - This RNN (also an LSTM in this case) decodes the context vector into a string of tokens/utterances.  For each time step, it takes the context vector and the embedded last utterance and produces the next utterance, which is fed back into the model.  More complex and effective models copy the encoder state into the decoder, add more layers of LSTMs, and apply attention mechanisms - but these are out of the scope of this simple example.\n",
    "- Next Word Dense+Softmax\n",
    "  - These two layers take the decoder output and turn it into the next word to be uttered.  The dense layer allows the decoder to not map directly to words uttered, and the softmax turns the dense layer output into a probability distribution, from which we pick the most likely next word.\n",
    "\n",
    "![seq2seq model structure](https://i.imgur.com/JmuryKu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "ee656365-3b04-4dfc-a4dc-a4081ceea82d",
    "_uuid": "46bea6af349b1b67b870eb5977c2bd0c798e2b86"
   },
   "outputs": [],
   "source": [
    "# keras imports, because there are like... A million of them.\n",
    "from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Embedding, RepeatVector, concatenate, \\\n",
    "    TimeDistributed\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d0a87457-573a-43b8-9a0a-d869a75b86ba",
    "_uuid": "420e809e7f3c9986bf4d3e3804198b8205e4a772",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    shared_embedding = Embedding(\n",
    "        output_dim=EMBEDDING_SIZE,\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        input_length=MAX_MESSAGE_LEN,\n",
    "        name='embedding',\n",
    "    )\n",
    "    \n",
    "    # ENCODER\n",
    "    \n",
    "    encoder_input = Input(\n",
    "        shape=(MAX_MESSAGE_LEN,),\n",
    "        dtype='int32',\n",
    "        name='encoder_input',\n",
    "    )\n",
    "    \n",
    "    embedded_input = shared_embedding(encoder_input)\n",
    "    \n",
    "    # No return_sequences - since the encoder here only produces a single value for the\n",
    "    # input sequence provided.\n",
    "    encoder_rnn = LSTM(\n",
    "        CONTEXT_SIZE,\n",
    "        name='encoder',\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    context = RepeatVector(MAX_MESSAGE_LEN)(encoder_rnn(embedded_input))\n",
    "    \n",
    "    # DECODER\n",
    "    \n",
    "    last_word_input = Input(\n",
    "        shape=(MAX_MESSAGE_LEN, ),\n",
    "        dtype='int32',\n",
    "        name='last_word_input',\n",
    "    )\n",
    "    \n",
    "    embedded_last_word = shared_embedding(last_word_input)\n",
    "    # Combines the context produced by the encoder and the last word uttered as inputs\n",
    "    # to the decoder.\n",
    "    decoder_input = concatenate([embedded_last_word, context], axis=2)\n",
    "    \n",
    "    # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
    "    # end of the intput, which is important for sequence producing models.\n",
    "    decoder_rnn = LSTM(\n",
    "        CONTEXT_SIZE,\n",
    "        name='decoder',\n",
    "        return_sequences=True,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    decoder_output = decoder_rnn(decoder_input)\n",
    "    \n",
    "    # TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
    "    next_word_dense = TimeDistributed(\n",
    "        Dense(int(MAX_VOCAB_SIZE / 2), activation='relu'),\n",
    "        name='next_word_dense',\n",
    "    )(decoder_output)\n",
    "    \n",
    "    next_word = TimeDistributed(\n",
    "        Dense(MAX_VOCAB_SIZE, activation='softmax'),\n",
    "        name='next_word_softmax'\n",
    "    )(next_word_dense)\n",
    "    \n",
    "    return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n",
    "\n",
    "s2s_model = create_model()\n",
    "optimizer = Adam(lr=LEARNING_RATE, clipvalue=5.0)\n",
    "s2s_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37dc7acc-ed74-4933-9182-b4329258d31f",
    "_uuid": "261fdad98295fa395592a5a6bf60806151a51c74"
   },
   "source": [
    "## Model Training\n",
    "We'll train the model here.  After each sub-batch of the dataset, we'll test with static input strings to see how the model is progressing in human readable terms.  Its important to have these tests along with traditional model evaluation to provide a better understanding of how well the model is training.\n",
    "\n",
    "It's important to pull test strings from the real distribution of the data, also.  It can be hard to really put yourself in customers' shoes when writing test messages, and you will get non-representative results when you provide test examples that don't fit the true distribution of the input data (when your input text doesn't sound like real customer requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db2eee93-8070-4572-962e-c7225b20d2e4",
    "_uuid": "464355ec34af2c192723e66222c036826ac323f7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_start_token(y_array):\n",
    "    \"\"\" Adds the start token to vectors.  Used for training data. \"\"\"\n",
    "    return np.hstack([\n",
    "        START * np.ones((len(y_array), 1)),\n",
    "        y_array[:, :-1],\n",
    "    ])\n",
    "\n",
    "def binarize_labels(labels):\n",
    "    \"\"\" Helper function that turns integer word indexes into sparse binary matrices for \n",
    "        the expected model output.\n",
    "    \"\"\"\n",
    "    return np.array([np_utils.to_categorical(row, num_classes=MAX_VOCAB_SIZE)\n",
    "                     for row in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ce33ef3c-90df-4d3f-aad7-82db7b8cbf38",
    "_uuid": "982ca948fcf372ba6d29cc1872b9667c8e7c678f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def respond_to(model, text):\n",
    "    \"\"\" Helper function that takes a text input and provides a text output. \"\"\"\n",
    "    input_y = add_start_token(PAD * np.ones((1, MAX_MESSAGE_LEN)))\n",
    "    idxs = np.array(to_word_idx(text)).reshape((1, MAX_MESSAGE_LEN))\n",
    "    for position in range(MAX_MESSAGE_LEN - 1):\n",
    "        prediction = model.predict([idxs, input_y]).argmax(axis=2)[0]\n",
    "        input_y[:,position + 1] = prediction[position]\n",
    "    return from_word_idx(model.predict([idxs, input_y]).argmax(axis=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3fb77b82-c4c3-4328-89a9-4c679744018c",
    "_uuid": "134fb65418234dd849a6d0147f95728373ea80a0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mini_epoch(model, start_idx, end_idx):\n",
    "    \"\"\" Batching seems necessary in Kaggle Jupyter Notebook environments, since\n",
    "        `model.fit` seems to freeze on larger batches (somewhere 1k-10k).\n",
    "    \"\"\"\n",
    "    b_train_y = binarize_labels(train_y[start_idx:end_idx])\n",
    "    input_train_y = add_start_token(train_y[start_idx:end_idx])\n",
    "    \n",
    "    model.fit(\n",
    "        [train_x[start_idx:end_idx], input_train_y], \n",
    "        b_train_y,\n",
    "        epochs=1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    rand_idx = random.sample(list(range(len(test_x))), SUB_BATCH_SIZE)\n",
    "    print('Test results:', model.evaluate(\n",
    "        [test_x[rand_idx], add_start_token(test_y[rand_idx])],\n",
    "        binarize_labels(test_y[rand_idx])\n",
    "    ))\n",
    "    \n",
    "    input_strings = [\n",
    "        \"@AppleSupport I fix I this I stupid I problem I\",\n",
    "        \"@AmazonHelp I hadnt expected that such a big brand like amazon would have such a poor customer service.\",\n",
    "    ]\n",
    "    \n",
    "    for input_string in input_strings:\n",
    "        output_string = respond_to(model, input_string)\n",
    "        print(f'> \"{input_string}\"\\n< \"{output_string}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a481fec-429a-45c5-bbb7-5f8f7e0e2487",
    "_uuid": "00e0850fe55d06f6d86c71c055ee45d8a0086c21"
   },
   "source": [
    "### Train the model!\n",
    "\n",
    "You can stop training by pressing the stop button - the training code is configured to watch for the `KeyboardInterrupt` exception triggered that way.  Also, it will run until the configured stopping point below.\n",
    "\n",
    "\n",
    "Let's start the training! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "450d8fd0-25b2-41cb-ad3b-dbb266568b72",
    "_uuid": "790bb2978f320f7988f0e850026f264a434b5c3e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_time_limit = 360 * 60  # seconds (notebooks terminate after 1 hour)\n",
    "start_time = time.time()\n",
    "stop_after = start_time + training_time_limit\n",
    "\n",
    "class TimesUpInterrupt(Exception):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for epoch in range(100):\n",
    "        print(f'Training in epoch {epoch}...')\n",
    "        for start_idx in range(0, len(train_x), SUB_BATCH_SIZE):\n",
    "            train_mini_epoch(s2s_model, start_idx, start_idx + SUB_BATCH_SIZE)\n",
    "            if time.time() > stop_after:\n",
    "                raise TimesUpInterrupt\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Halting training from keyboard interrupt.\")\n",
    "except TimesUpInterrupt:\n",
    "    print(f\"Halting after {time.time() - start_time} seconds spent training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18a421a1-41d7-4c87-988d-74fa8dc85992",
    "_uuid": "8ebd6c01cd3a7e75810f9fc48b4d2ad913f9f410",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "respond_to(s2s_model, '''@AppleSupport iPhone 8 touchID doesnt unlock while charging on \n",
    "    110v w/ 61w laptop charger to usbc lightning cable just uh.. so you guys know''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eef56338-516e-46ed-94d5-6c7d21279d34",
    "_uuid": "e5f0a5acf6611334291cea8c3440c5dd3badf2f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "respond_to(s2s_model, '''@sprintcare I can't make calls... wtf''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1445794c-c26f-4dc9-a38e-90b98ee206cb",
    "_uuid": "f7e4303f8ee7d2def95c95b166b6fac8727217f5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_renv",
   "language": "python",
   "name": "twitter_renv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
