{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import demoji\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AishwaryaBhangale\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_complete = pd.read_table('conv.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>text_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 Hello! We never like our customers to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 I would love the chance to review the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text_x  \\\n",
       "0  @sprintcare is the worst customer service   \n",
       "1  @sprintcare is the worst customer service   \n",
       "\n",
       "                                              text_y  \n",
       "0  @115712 Hello! We never like our customers to ...  \n",
       "1  @115712 I would love the chance to review the ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_complete[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>text_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 Hello! We never like our customers to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 I would love the chance to review the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 Can you please send us a private messa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
       "      <td>@115713 H there! We'd definitely like to work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115714 whenever I contact customer support, t...</td>\n",
       "      <td>@115715 Please send me a private message so th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>actually that's a broken link you sent me and ...</td>\n",
       "      <td>@115716 The information pertaining to the acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yo @Ask_Spectrum, your customer service reps a...</td>\n",
       "      <td>@115717 Hello, My apologies for any frustratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My picture on @Ask_Spectrum pretty much every ...</td>\n",
       "      <td>@115718 I apologize for the inconvenience. I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>somebody from @VerizonSupport please help meee...</td>\n",
       "      <td>@115719 Help has arrived! We are sorry to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VerizonSupport My friend is without internet ...</td>\n",
       "      <td>@115720 Have your friend message us.\\r\\n^ACM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@115722 tried to pay a bill for 60 days. No se...</td>\n",
       "      <td>@115721 Are you referring to wireless or resid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@115722 is the worst ISP I’ve ever had</td>\n",
       "      <td>@115723 What did we do to make you feel this w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.@VerizonSupport @115725 @115726              ...</td>\n",
       "      <td>@115724 In what area are you located? All of y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@115725 fix your app it won't even open</td>\n",
       "      <td>@115727 Which app are you referring too? \\r\\n^ACM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ChipotleTweets @28 \\r\\nI don't fit in my Vegg...</td>\n",
       "      <td>@115728 I still think you look great! -Becky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@ChipotleTweets @28 \\r\\nI don't fit in my Vegg...</td>\n",
       "      <td>@ChipotleTweets @28 My baby https://t.co/dtiXx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@ChipotleTweets messed up today and didn’t giv...</td>\n",
       "      <td>@115729 I'm so sorry about that. Please tell u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hey @ChipotleTweets wanna come to Mammoth. I'l...</td>\n",
       "      <td>@115730 Hopefully we'll get there at some poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>When you're the only one in costume #boorito @...</td>\n",
       "      <td>@115731 It's because you're smart. -Tara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@ChipotleTweets no Diet Coke and a literal bon...</td>\n",
       "      <td>@115732 That's incredibly concerning. Please t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_x  \\\n",
       "0           @sprintcare is the worst customer service   \n",
       "1           @sprintcare is the worst customer service   \n",
       "2           @sprintcare is the worst customer service   \n",
       "3   @115714 y’all lie about your “great” connectio...   \n",
       "4   @115714 whenever I contact customer support, t...   \n",
       "5   actually that's a broken link you sent me and ...   \n",
       "6   Yo @Ask_Spectrum, your customer service reps a...   \n",
       "7   My picture on @Ask_Spectrum pretty much every ...   \n",
       "8   somebody from @VerizonSupport please help meee...   \n",
       "9   @VerizonSupport My friend is without internet ...   \n",
       "10  @115722 tried to pay a bill for 60 days. No se...   \n",
       "11             @115722 is the worst ISP I’ve ever had   \n",
       "12  .@VerizonSupport @115725 @115726              ...   \n",
       "13            @115725 fix your app it won't even open   \n",
       "14  @ChipotleTweets @28 \\r\\nI don't fit in my Vegg...   \n",
       "15  @ChipotleTweets @28 \\r\\nI don't fit in my Vegg...   \n",
       "16  @ChipotleTweets messed up today and didn’t giv...   \n",
       "17  hey @ChipotleTweets wanna come to Mammoth. I'l...   \n",
       "18  When you're the only one in costume #boorito @...   \n",
       "19  @ChipotleTweets no Diet Coke and a literal bon...   \n",
       "\n",
       "                                               text_y  \n",
       "0   @115712 Hello! We never like our customers to ...  \n",
       "1   @115712 I would love the chance to review the ...  \n",
       "2   @115712 Can you please send us a private messa...  \n",
       "3   @115713 H there! We'd definitely like to work ...  \n",
       "4   @115715 Please send me a private message so th...  \n",
       "5   @115716 The information pertaining to the acco...  \n",
       "6   @115717 Hello, My apologies for any frustratio...  \n",
       "7   @115718 I apologize for the inconvenience. I w...  \n",
       "8   @115719 Help has arrived! We are sorry to see ...  \n",
       "9        @115720 Have your friend message us.\\r\\n^ACM  \n",
       "10  @115721 Are you referring to wireless or resid...  \n",
       "11  @115723 What did we do to make you feel this w...  \n",
       "12  @115724 In what area are you located? All of y...  \n",
       "13  @115727 Which app are you referring too? \\r\\n^ACM  \n",
       "14       @115728 I still think you look great! -Becky  \n",
       "15  @ChipotleTweets @28 My baby https://t.co/dtiXx...  \n",
       "16  @115729 I'm so sorry about that. Please tell u...  \n",
       "17  @115730 Hopefully we'll get there at some poin...  \n",
       "18           @115731 It's because you're smart. -Tara  \n",
       "19  @115732 That's incredibly concerning. Please t...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets_complete.head(20)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\AishwaryaBhangale\\Documents\\GitHub\\nlp-coe\\twitter-conversational-chatbot\\Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        text = re.sub(r'(' +emot+ ')', \"\".join(Emoji_Dict[emot].replace(\":\",\" \")),text)\n",
    "    return text\n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-58ac437121b3>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'] = \"\"\n",
      "<ipython-input-78-58ac437121b3>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_y_clean'] = \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@115712 hello! we never like our customers to feel like they are not valued.\n",
      "@115712 i would love the chance to review the account and provide assistance.\n",
      "@115712 can you please send us a private message, so that i can gain further details about your account?\n",
      "@115713 h there! we'd definitely like to work with you on this, how long have you been experiencing this issue? -aa\n",
      "@115715 please send me a private message so that i can send you the link to access your account. -fr\n",
      "@115716 the information pertaining to the account assumption is correct.  this does need to be done at a local outlet wit... \n",
      "@115717 hello, my apologies for any frustrations or inconvenience. i’d be happy to look into this for you? ^mg\n",
      "@115718 i apologize for the inconvenience. i will be glad to assist you. can you dm me your name and acct # or phone #? -jb\n",
      "@115719 help has arrived! we are sorry to see that you are having trouble. how can we help?\n",
      "^hsb\n",
      "@115720 have your friend message us.\n",
      "^acm\n",
      "@115721 are you referring to wireless or residential service?\n",
      "^jay\n",
      "@115723 what did we do to make you feel this way and how can we fix things between us? ^kmg\n",
      "@115724 in what area are you located? all of your services are down at this time?\n",
      "^hsb\n",
      "@115727 which app are you referring too? \n",
      "^acm\n",
      "@115728 i still think you look great! -becky\n",
      "@chipotletweets @28 my baby \n",
      "@115729 i'm so sorry about that. please tell us more so we can help:  -becky\n",
      "@115730 hopefully we'll get there at some point! -becky\n",
      "@115731 it's because you're smart. -tara\n",
      "@115732 that's incredibly concerning. please tell us more here:  -becky\n"
     ]
    }
   ],
   "source": [
    "#Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#define special characters list\n",
    "special_characters = ['!','#','$','%', '&','@','[',']',' ',']','_']\n",
    "\n",
    "tweets['text_x_clean'] = \"\"\n",
    "tweets['text_y_clean'] = \"\"\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "\n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    tweets['text_x_clean'][i] = tweets['text_x'][i].lower()\n",
    "    \n",
    "    #Conversion of Emojis to Text\n",
    "    tweets['text_x_clean'][i] = convert_emojis_to_word(tweets['text_x_clean'][i])\n",
    "    \n",
    "    #Removal of URLS\n",
    "    pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n",
    "    match = re.findall(pattern, tweets['text_x_clean'][i])\n",
    "    for m in match:\n",
    "        url = m[0]\n",
    "        tweets['text_x_clean'][i] = tweets['text_x_clean'][i].replace(url, '')\n",
    "    \n",
    "    #Removal of HTTPS\n",
    "    tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n",
    "    \n",
    "    \n",
    "    #We don't need the following steps of text data preprocessing for deep learning models \n",
    "    '''\n",
    "    #Remove Punctuation --remove\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    unpunctuated_text = tokenizer.tokenize(tweets['text_x_clean'][i])\n",
    "    tweets['text_x_clean'][i] = \" \".join(unpunctuated_text)\n",
    "    \n",
    "    #Special Character Replacement -- remove\n",
    "    for j in range(len(special_characters)):\n",
    "        tweets['text_x_clean'][i] = tweets['text_x_clean'][i].replace(special_characters[j],' ')\n",
    "    \n",
    "    #Lemmatization -- remove\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = tweets['text_x_clean'][i]\n",
    "    lemma_text = nltk.word_tokenize(text)\n",
    "    tweets['text_x_clean'][i] = \" \".join(lemma_text)\n",
    "    \n",
    "   \n",
    "    #Removing Stop Words --remove\n",
    "    word_tokens = word_tokenize(tweets['text_x_clean'][i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    tweets['text_x_clean'][i] = filtered_sentence\n",
    "    print(tweets['text_x_clean'][i])\n",
    "'''\n",
    "    #################################################################################################################\n",
    "    \n",
    "    tweets['text_y_clean'][i] = tweets['text_y'][i].lower()\n",
    "    \n",
    "    #Conversion of Emojis to Text\n",
    "    tweets['text_y_clean'][i] = convert_emojis_to_word(tweets['text_y_clean'][i])\n",
    "    \n",
    "    #Removal of URLS\n",
    "    pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n",
    "    match = re.findall(pattern, tweets['text_y_clean'][i])\n",
    "    for m in match:\n",
    "        url = m[0]\n",
    "        tweets['text_y_clean'][i] = tweets['text_y_clean'][i].replace(url, '')\n",
    "    \n",
    "    #Removal of HTTPS\n",
    "    tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n",
    "\n",
    "    #We don't need the following steps of text data preprocessing for deep learning models \n",
    "    '''\n",
    "    #Remove Punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    unpunctuated_text = tokenizer.tokenize(tweets['text_y_clean'][i])\n",
    "    tweets['text_y_clean'][i] = \" \".join(unpunctuated_text)\n",
    "    \n",
    "    #Special Character Replacement\n",
    "    for j in range(len(special_characters)):\n",
    "        tweets['text_y_clean'][i] = tweets['text_y_clean'][i].replace(special_characters[j],' ')\n",
    "    \n",
    "    #Lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = tweets['text_y_clean'][i]\n",
    "    lemma_text = nltk.word_tokenize(text)\n",
    "    tweets['text_y_clean'][i] = \" \".join(lemma_text)\n",
    "    \n",
    "    \n",
    "    #Removing Stop Words\n",
    "    word_tokens = word_tokenize(tweets['text_y_clean'][i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    tweets['text_y_clean'][i] = filtered_sentence\n",
    "    '''\n",
    "    \n",
    "    print(tweets['text_y_clean'][i])\n",
    "\n",
    "    #################################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Steps:\n",
    "#1) Replace customer id with _sn_\n",
    "#2) split into train and test\n",
    "#3)Save to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
