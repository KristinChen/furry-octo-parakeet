{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "keras:2.8.0\n",
      "pandas:1.3.4\n",
      "sklearn:0.24.2\n",
      "nltk:3.6.7\n",
      "numpy:1.20.3\n",
      "tensofrlow:2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JiatingChen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "print('Library versions:')\n",
    "\n",
    "import keras\n",
    "print(f'keras:{keras.__version__}')\n",
    "import pandas as pd\n",
    "print(f'pandas:{pd.__version__}')\n",
    "import sklearn\n",
    "print(f'sklearn:{sklearn.__version__}')\n",
    "import nltk\n",
    "print(f'nltk:{nltk.__version__}')\n",
    "import numpy as np\n",
    "print(f'numpy:{np.__version__}')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "import demoji\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import tensorflow as tf\n",
    "print(f'tensofrlow:{tf.__version__}')\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "demoji.__version__\n",
    "with open(r'../data/Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "    Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "    \n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JiatingChen\\\\Documents\\\\nlp-coe\\\\twitter-conversational-chatbot\\\\modeling'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 787346 first inbound messages.\n",
      "Found 875292 responses.\n",
      "Found 794299 responses from companies.\n",
      "Tweets Preview:\n",
      "        tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n",
      "0                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "1                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "2                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "3               18      115713       True  Tue Oct 31 19:56:01 +0000 2017   \n",
      "4               20      115715       True  Tue Oct 31 22:03:34 +0000 2017   \n",
      "...            ...         ...        ...                             ...   \n",
      "875287     2987942      823867       True  Wed Nov 22 07:30:39 +0000 2017   \n",
      "875288     2987944      823868       True  Wed Nov 22 07:43:36 +0000 2017   \n",
      "875289     2987946      524544       True  Wed Nov 22 08:25:48 +0000 2017   \n",
      "875290     2987948      823869       True  Wed Nov 22 08:35:16 +0000 2017   \n",
      "875291     2987950      823870       True  Tue Nov 21 22:01:04 +0000 2017   \n",
      "\n",
      "                                                   text_x response_tweet_id_x  \\\n",
      "0               @sprintcare is the worst customer service              9,6,10   \n",
      "1               @sprintcare is the worst customer service              9,6,10   \n",
      "2               @sprintcare is the worst customer service              9,6,10   \n",
      "3       @115714 y‚Äôall lie about your ‚Äúgreat‚Äù connectio...                  17   \n",
      "4       @115714 whenever I contact customer support, t...                  19   \n",
      "...                                                   ...                 ...   \n",
      "875287  Hai @AirAsiaSupport #asking how many days need...             2987941   \n",
      "875288  @AirAsiaSupport \\n\\nI am unable to do web chec...             2987943   \n",
      "875289  @VirginTrains Hope you are well? Does the 9.30...             2987945   \n",
      "875290  @115714 wtf!? I‚Äôve been having really shitty s...             2987947   \n",
      "875291  @AldiUK  warm sloe gin mince pies with ice cre...     2987951,2987949   \n",
      "\n",
      "        in_response_to_tweet_id_x  tweet_id_y     author_id_y  inbound_y  \\\n",
      "0                             NaN           6      sprintcare      False   \n",
      "1                             NaN           9      sprintcare      False   \n",
      "2                             NaN          10      sprintcare      False   \n",
      "3                             NaN          17      sprintcare      False   \n",
      "4                             NaN          19      sprintcare      False   \n",
      "...                           ...         ...             ...        ...   \n",
      "875287                        NaN     2987941  AirAsiaSupport      False   \n",
      "875288                        NaN     2987943  AirAsiaSupport      False   \n",
      "875289                        NaN     2987945    VirginTrains      False   \n",
      "875290                        NaN     2987947      sprintcare      False   \n",
      "875291                        NaN     2987949          AldiUK      False   \n",
      "\n",
      "                          created_at_y  \\\n",
      "0       Tue Oct 31 21:46:24 +0000 2017   \n",
      "1       Tue Oct 31 21:46:14 +0000 2017   \n",
      "2       Tue Oct 31 21:45:59 +0000 2017   \n",
      "3       Tue Oct 31 19:59:13 +0000 2017   \n",
      "4       Tue Oct 31 22:10:10 +0000 2017   \n",
      "...                                ...   \n",
      "875287  Wed Nov 22 07:55:05 +0000 2017   \n",
      "875288  Wed Nov 22 07:54:57 +0000 2017   \n",
      "875289  Wed Nov 22 08:27:34 +0000 2017   \n",
      "875290  Wed Nov 22 08:43:51 +0000 2017   \n",
      "875291  Wed Nov 22 08:31:24 +0000 2017   \n",
      "\n",
      "                                                   text_y response_tweet_id_y  \\\n",
      "0       @115712 Can you please send us a private messa...                 5,7   \n",
      "1       @115712 I would love the chance to review the ...                 NaN   \n",
      "2       @115712 Hello! We never like our customers to ...                 NaN   \n",
      "3       @115713 H there! We'd definitely like to work ...                  16   \n",
      "4       @115715 Please send me a private message so th...                 NaN   \n",
      "...                                                   ...                 ...   \n",
      "875287     @823867 we have replied you via DM.Thanks-Emir                 NaN   \n",
      "875288  @823868 Sorry but kindly try to clear browser,...                 NaN   \n",
      "875289  @524544 That's a Peak service. The 09:56 is th...                 NaN   \n",
      "875290  @823869 Hey, we'd be happy to look into this f...                 NaN   \n",
      "875291  @823870 Sounds delicious, Sarah! üòã https://t.c...                 NaN   \n",
      "\n",
      "        in_response_to_tweet_id_y  \n",
      "0                             8.0  \n",
      "1                             8.0  \n",
      "2                             8.0  \n",
      "3                            18.0  \n",
      "4                            20.0  \n",
      "...                           ...  \n",
      "875287                  2987942.0  \n",
      "875288                  2987944.0  \n",
      "875289                  2987946.0  \n",
      "875290                  2987948.0  \n",
      "875291                  2987950.0  \n",
      "\n",
      "[794299 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A kernel posted on Kaggle that shows how to pull just the first consumer request and\n",
    "    company response from the dataset.\n",
    "\"\"\"\n",
    "\n",
    "tweets = pd.read_csv('../data/twcs/twcs.csv')\n",
    "\n",
    "\n",
    "# Pick only inbound tweets that aren't in reply to anything...\n",
    "first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n",
    "print('Found {} first inbound messages.'.format(len(first_inbound)))\n",
    "\n",
    "# Merge in all tweets in response\n",
    "inbounds_and_outbounds = pd.merge(first_inbound, tweets, left_on='tweet_id', \n",
    "                                  right_on='in_response_to_tweet_id')\n",
    "print(\"Found {} responses.\".format(len(inbounds_and_outbounds)))\n",
    "\n",
    "# Filter out cases where reply tweet isn't from company\n",
    "inbounds_and_outbounds = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n",
    "\n",
    "# Et voila!\n",
    "print(\"Found {} responses from companies.\".format(len(inbounds_and_outbounds)))\n",
    "print(\"Tweets Preview:\")\n",
    "print(inbounds_and_outbounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (794299, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Data shape: {inbounds_and_outbounds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbounds_and_outbounds.to_csv('../data/inbounds_and_outbounds.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace anonymized screen names with common token @__sn__\n",
    "def sn_replace(match):\n",
    "    _sn = match.group(2).lower()\n",
    "    if not _sn.isnumeric():\n",
    "        # This is a company screen name\n",
    "        return match.group(1) + match.group(2)\n",
    "    return ' @__sn__'\n",
    "\n",
    "\n",
    "def replace_ID(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            sn_re = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\n",
    "            return sn_re.sub(sn_replace, text)\n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "def convert_emojis_to_word(text, emoji_dict):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            for emot in Emoji_Dict:\n",
    "                text = re.sub(r'(' +emot+ ')', \"\".join(Emoji_Dict[emot].replace(\":\",\" \")),text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def replace_URL(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            pattern = r\"http\\S+|https\\S+|www\\S+\"\n",
    "            return re.sub(pattern, \"__URL__\", str(text))\n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def clean_text(text, emoji_dict = Emoji_Dict):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            temp1 = replace_ID(text)\n",
    "            temp2 = convert_emojis_to_word(temp1, emoji_dict)\n",
    "            temp3 = replace_URL(temp2)\n",
    "            return temp3   \n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testiing case 2: @__sn__ Please refer to either this link __URL__ or this link: __URL__ Have a great day smiling_face_with_smiling_eyes !'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"testiing case 2: @23456 Please refer to either this link www:/somethinghelpful or this link: http:/somethinghelpful/somethingevermorehelpful. Have a great dayüòä!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "sample = inbounds_and_outbounds.loc[: , ['text_x', 'text_y']].sample(frac = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54662d5e42940dd998e9a3d642fa064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_text = sample.text_x.progress_apply(lambda txt: clean_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da59ae60b9754b4b957eb85d1af652b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_text = sample.text_y.progress_apply(lambda txt: clean_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([x_text, y_text], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df.sample(frac = 0.8, random_state = 1234)\n",
    "testing_data = df.loc[~df.index.isin(training_data.index.to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(training_data, handle)\n",
    "with open('testing_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(testing_data, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_venv_testing",
   "language": "python",
   "name": "chatbot_venv_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
