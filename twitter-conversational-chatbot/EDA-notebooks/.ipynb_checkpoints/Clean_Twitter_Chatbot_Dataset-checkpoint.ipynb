{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "keras:2.8.0\n",
      "pandas:1.3.4\n",
      "sklearn:0.24.2\n",
      "nltk:3.6.7\n",
      "numpy:1.22.3\n",
      "tensofrlow:2.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JiatingChen\\\\Documents\\\\nlp-coe\\\\twitter-conversational-chatbot\\\\EDA-notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print('Library versions:')\n",
    "\n",
    "import keras\n",
    "print(f'keras:{keras.__version__}')\n",
    "import pandas as pd\n",
    "print(f'pandas:{pd.__version__}')\n",
    "import sklearn\n",
    "print(f'sklearn:{sklearn.__version__}')\n",
    "import nltk\n",
    "print(f'nltk:{nltk.__version__}')\n",
    "import numpy as np\n",
    "print(f'numpy:{np.__version__}')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "import demoji\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import tensorflow as tf\n",
    "print(f'tensofrlow:{tf.__version__}')\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "demoji.__version__\n",
    "with open(r'./Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "    Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "    \n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 787346 first inbound messages.\n",
      "Found 875292 responses.\n",
      "Found 794299 responses from companies.\n",
      "Tweets Preview:\n",
      "        tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n",
      "0                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "1                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "2                8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
      "3               18      115713       True  Tue Oct 31 19:56:01 +0000 2017   \n",
      "4               20      115715       True  Tue Oct 31 22:03:34 +0000 2017   \n",
      "...            ...         ...        ...                             ...   \n",
      "875287     2987942      823867       True  Wed Nov 22 07:30:39 +0000 2017   \n",
      "875288     2987944      823868       True  Wed Nov 22 07:43:36 +0000 2017   \n",
      "875289     2987946      524544       True  Wed Nov 22 08:25:48 +0000 2017   \n",
      "875290     2987948      823869       True  Wed Nov 22 08:35:16 +0000 2017   \n",
      "875291     2987950      823870       True  Tue Nov 21 22:01:04 +0000 2017   \n",
      "\n",
      "                                                   text_x response_tweet_id_x  \\\n",
      "0               @sprintcare is the worst customer service              9,6,10   \n",
      "1               @sprintcare is the worst customer service              9,6,10   \n",
      "2               @sprintcare is the worst customer service              9,6,10   \n",
      "3       @115714 y‚Äôall lie about your ‚Äúgreat‚Äù connectio...                  17   \n",
      "4       @115714 whenever I contact customer support, t...                  19   \n",
      "...                                                   ...                 ...   \n",
      "875287  Hai @AirAsiaSupport #asking how many days need...             2987941   \n",
      "875288  @AirAsiaSupport \\n\\nI am unable to do web chec...             2987943   \n",
      "875289  @VirginTrains Hope you are well? Does the 9.30...             2987945   \n",
      "875290  @115714 wtf!? I‚Äôve been having really shitty s...             2987947   \n",
      "875291  @AldiUK  warm sloe gin mince pies with ice cre...     2987951,2987949   \n",
      "\n",
      "        in_response_to_tweet_id_x  tweet_id_y     author_id_y  inbound_y  \\\n",
      "0                             NaN           6      sprintcare      False   \n",
      "1                             NaN           9      sprintcare      False   \n",
      "2                             NaN          10      sprintcare      False   \n",
      "3                             NaN          17      sprintcare      False   \n",
      "4                             NaN          19      sprintcare      False   \n",
      "...                           ...         ...             ...        ...   \n",
      "875287                        NaN     2987941  AirAsiaSupport      False   \n",
      "875288                        NaN     2987943  AirAsiaSupport      False   \n",
      "875289                        NaN     2987945    VirginTrains      False   \n",
      "875290                        NaN     2987947      sprintcare      False   \n",
      "875291                        NaN     2987949          AldiUK      False   \n",
      "\n",
      "                          created_at_y  \\\n",
      "0       Tue Oct 31 21:46:24 +0000 2017   \n",
      "1       Tue Oct 31 21:46:14 +0000 2017   \n",
      "2       Tue Oct 31 21:45:59 +0000 2017   \n",
      "3       Tue Oct 31 19:59:13 +0000 2017   \n",
      "4       Tue Oct 31 22:10:10 +0000 2017   \n",
      "...                                ...   \n",
      "875287  Wed Nov 22 07:55:05 +0000 2017   \n",
      "875288  Wed Nov 22 07:54:57 +0000 2017   \n",
      "875289  Wed Nov 22 08:27:34 +0000 2017   \n",
      "875290  Wed Nov 22 08:43:51 +0000 2017   \n",
      "875291  Wed Nov 22 08:31:24 +0000 2017   \n",
      "\n",
      "                                                   text_y response_tweet_id_y  \\\n",
      "0       @115712 Can you please send us a private messa...                 5,7   \n",
      "1       @115712 I would love the chance to review the ...                 NaN   \n",
      "2       @115712 Hello! We never like our customers to ...                 NaN   \n",
      "3       @115713 H there! We'd definitely like to work ...                  16   \n",
      "4       @115715 Please send me a private message so th...                 NaN   \n",
      "...                                                   ...                 ...   \n",
      "875287     @823867 we have replied you via DM.Thanks-Emir                 NaN   \n",
      "875288  @823868 Sorry but kindly try to clear browser,...                 NaN   \n",
      "875289  @524544 That's a Peak service. The 09:56 is th...                 NaN   \n",
      "875290  @823869 Hey, we'd be happy to look into this f...                 NaN   \n",
      "875291  @823870 Sounds delicious, Sarah! üòã https://t.c...                 NaN   \n",
      "\n",
      "        in_response_to_tweet_id_y  \n",
      "0                             8.0  \n",
      "1                             8.0  \n",
      "2                             8.0  \n",
      "3                            18.0  \n",
      "4                            20.0  \n",
      "...                           ...  \n",
      "875287                  2987942.0  \n",
      "875288                  2987944.0  \n",
      "875289                  2987946.0  \n",
      "875290                  2987948.0  \n",
      "875291                  2987950.0  \n",
      "\n",
      "[794299 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A kernel posted on Kaggle that shows how to pull just the first consumer request and\n",
    "    company response from the dataset.\n",
    "\"\"\"\n",
    "\n",
    "tweets = pd.read_csv('../data/twcs/twcs.csv')\n",
    "\n",
    "\n",
    "# Pick only inbound tweets that aren't in reply to anything...\n",
    "first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n",
    "print('Found {} first inbound messages.'.format(len(first_inbound)))\n",
    "\n",
    "# Merge in all tweets in response\n",
    "inbounds_and_outbounds = pd.merge(first_inbound, tweets, left_on='tweet_id', \n",
    "                                  right_on='in_response_to_tweet_id')\n",
    "print(\"Found {} responses.\".format(len(inbounds_and_outbounds)))\n",
    "\n",
    "# Filter out cases where reply tweet isn't from company\n",
    "inbounds_and_outbounds = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n",
    "\n",
    "# Et voila!\n",
    "print(\"Found {} responses from companies.\".format(len(inbounds_and_outbounds)))\n",
    "print(\"Tweets Preview:\")\n",
    "print(inbounds_and_outbounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (794299, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Data shape: {inbounds_and_outbounds.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace anonymized screen names with common token @__sn__\n",
    "with open(r'./Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "    Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "    def sn_replace(match):\n",
    "    _sn = match.group(2).lower()\n",
    "    if not _sn.isnumeric():\n",
    "        # This is a company screen name\n",
    "        return match.group(1) + match.group(2)\n",
    "    return ' @__sn__'\n",
    "\n",
    "\n",
    "def replace_ID(text):\n",
    "    sn_re = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\n",
    "    return sn_re.sub(sn_replace, text)\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        if \n",
    "        \n",
    "        if isinstance(text, str):\n",
    "        for emot in Emoji_Dict:\n",
    "            text = re.sub(r'(' +emot+ ')', \"\".join(Emoji_Dict[emot].replace(\":\",\" \")),text)\n",
    "        return text\n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def replace_URL(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "        pattern = r\"http\\S+|https\\S+|www\\S+\"\n",
    "        return re.sub(pattern, \"__URL__\", str(text))\n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "\n",
    " def clean_text(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            temp1 = replace_ID(text)\n",
    "            temp2 = convert_emojis_to_word(temp1)\n",
    "            temp3 = replace_URL(temp2)\n",
    "            return temp3   \n",
    "        else:\n",
    "            raise ValueError()      \n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testiing case 2: @__sn__ Please refer to either this link __URL__ or this link: __URL__ Have a great day smiling_face_with_smiling_eyes !'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"testiing case 2: @23456 Please refer to either this link www:/somethinghelpful or this link: http:/somethinghelpful/somethingevermorehelpful. Have a great dayüòä!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa871969e944a3bb3237328be747ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/794299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_text \u001b[38;5;241m=\u001b[39m \u001b[43minbounds_and_outbounds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_text \u001b[38;5;241m=\u001b[39m inbounds_and_outbounds\u001b[38;5;241m.\u001b[39mtext_y\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m txt: clean_text(txt))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\tqdm\\std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\site-packages\\tqdm\\std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(txt)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_text \u001b[38;5;241m=\u001b[39m inbounds_and_outbounds\u001b[38;5;241m.\u001b[39mtext_x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m txt: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m y_text \u001b[38;5;241m=\u001b[39m inbounds_and_outbounds\u001b[38;5;241m.\u001b[39mtext_y\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m txt: clean_text(txt))\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m     32\u001b[0m     temp1 \u001b[38;5;241m=\u001b[39m replace_ID(text)\n\u001b[1;32m---> 33\u001b[0m     temp2 \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_emojis_to_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     temp3 \u001b[38;5;241m=\u001b[39m replace_URL(temp2)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m temp3\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mconvert_emojis_to_word\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m     Emoji_Dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m Emoji_Dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emot \u001b[38;5;129;01min\u001b[39;00m Emoji_Dict:\n\u001b[1;32m---> 20\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43memot\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEmoji_Dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43memot\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 304\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\sre_compile.py:768\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43m_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_DEBUG:\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\sre_compile.py:607\u001b[0m, in \u001b[0;36m_code\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    604\u001b[0m _compile_info(code, p, flags)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# compile the pattern\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m code\u001b[38;5;241m.\u001b[39mappend(SUCCESS)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\sre_compile.py:168\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, pattern, flags)\u001b[0m\n\u001b[0;32m    166\u001b[0m     emit((group\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# _compile_info(code, p, _combine_flags(flags, add_flags, del_flags))\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_combine_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdel_flags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group:\n\u001b[0;32m    170\u001b[0m     emit(MARK)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\sre_compile.py:90\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, pattern, flags)\u001b[0m\n\u001b[0;32m     88\u001b[0m         iscased \u001b[38;5;241m=\u001b[39m _sre\u001b[38;5;241m.\u001b[39mascii_iscased\n\u001b[0;32m     89\u001b[0m         tolower \u001b[38;5;241m=\u001b[39m _sre\u001b[38;5;241m.\u001b[39mascii_tolower\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, av \u001b[38;5;129;01min\u001b[39;00m pattern:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m LITERAL_CODES:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_IGNORECASE:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chatbot_venv\\lib\\sre_parse.py:167\u001b[0m, in \u001b[0;36mSubPattern.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SubPattern(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index])\n\u001b[1;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m[index]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_text = inbounds_and_outbounds.text_x.progress_apply(lambda txt: clean_text(txt))\n",
    "y_text = inbounds_and_outbounds.text_y.progress_apply(lambda txt: clean_text(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when split training and testing set, please keep `random_state` = 1234. Example is as follows.\n",
    "```python\n",
    "import random\n",
    "all_idx = list(range(len(x)))\n",
    "random.seed(1234)\n",
    "train_idx = set(random.sample(all_idx, int(0.8 * len(all_idx))))\n",
    "test_idx = {idx for idx in all_idx if idx not in train_idx}\n",
    "\n",
    "train_x = x[list(train_idx)]\n",
    "test_x = x[list(test_idx)]\n",
    "train_y = y[list(train_idx)]\n",
    "test_y = y[list(test_idx)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'] = \"\"\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_y_clean'] = \"\"\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'][i] = tweets['text_x'][i].lower()\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'][i] = convert_emojis_to_word(tweets['text_x_clean'][i])\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_y_clean'][i] = tweets['text_y'][i].lower()\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_y_clean'][i] = convert_emojis_to_word(tweets['text_y_clean'][i])\n",
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@115712 hello! we never like our customers to feel like they are not valued.\n",
      "@115712 i would love the chance to review the account and provide assistance.\n",
      "@115712 can you please send us a private message, so that i can gain further details about your account?\n",
      "@115713 h there! we'd definitely like to work with you on this, how long have you been experiencing this issue? -aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_x_clean'][i] = tweets['text_x_clean'][i].replace(url, '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@115715 please send me a private message so that i can send you the link to access your account. -fr\n",
      "@115716 the information pertaining to the account assumption is correct.  this does need to be done at a local outlet wit... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiatingChen\\AppData\\Local\\Temp\\ipykernel_1828\\3381722036.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text_y_clean'][i] = tweets['text_y_clean'][i].replace(url, '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@115717 hello, my apologies for any frustrations or inconvenience. i‚Äôd be happy to look into this for you? ^mg\n",
      "@115718 i apologize for the inconvenience. i will be glad to assist you. can you dm me your name and acct # or phone #? -jb\n",
      "@115719 help has arrived! we are sorry to see that you are having trouble. how can we help?\n",
      "^hsb\n",
      "@115720 have your friend message us.\n",
      "^acm\n",
      "@115721 are you referring to wireless or residential service?\n",
      "^jay\n",
      "@115723 what did we do to make you feel this way and how can we fix things between us? ^kmg\n",
      "@115724 in what area are you located? all of your services are down at this time?\n",
      "^hsb\n",
      "@115727 which app are you referring too? \n",
      "^acm\n",
      "@115728 i still think you look great! -becky\n",
      "@chipotletweets @28 my baby \n",
      "@115729 i'm so sorry about that. please tell us more so we can help:  -becky\n",
      "@115730 hopefully we'll get there at some point! -becky\n",
      "@115731 it's because you're smart. -tara\n",
      "@115732 that's incredibly concerning. please tell us more here:  -becky\n"
     ]
    }
   ],
   "source": [
    "#Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#define special characters list\n",
    "special_characters = ['!','#','$','%', '&','@','[',']',' ',']','_']\n",
    "\n",
    "tweets['text_x_clean'] = \"\"\n",
    "tweets['text_y_clean'] = \"\"\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "\n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    tweets['text_x_clean'][i] = tweets['text_x'][i].lower()\n",
    "    \n",
    "    #Conversion of Emojis to Text\n",
    "    tweets['text_x_clean'][i] = convert_emojis_to_word(tweets['text_x_clean'][i])\n",
    "    \n",
    "    #Removal of URLS\n",
    "    pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))';\n",
    "    match = re.findall(pattern, tweets['text_x_clean'][i])\n",
    "    for m in match:\n",
    "        url = m[0]\n",
    "        tweets['text_x_clean'][i] = tweets['text_x_clean'][i].replace(url, '')\n",
    "    \n",
    "    #Removal of HTTPS\n",
    "    tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n",
    "    \n",
    "    \n",
    "    #We don't need the following steps of text data preprocessing for deep learning models \n",
    "    '''\n",
    "    #Remove Punctuation --remove\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    unpunctuated_text = tokenizer.tokenize(tweets['text_x_clean'][i])\n",
    "    tweets['text_x_clean'][i] = \" \".join(unpunctuated_text)\n",
    "    \n",
    "    #Special Character Replacement -- remove\n",
    "    for j in range(len(special_characters)):\n",
    "        tweets['text_x_clean'][i] = tweets['text_x_clean'][i].replace(special_characters[j],' ')\n",
    "    \n",
    "    #Lemmatization -- remove\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = tweets['text_x_clean'][i]\n",
    "    lemma_text = nltk.word_tokenize(text)\n",
    "    tweets['text_x_clean'][i] = \" \".join(lemma_text)\n",
    "    \n",
    "   \n",
    "    #Removing Stop Words --remove\n",
    "    word_tokens = word_tokenize(tweets['text_x_clean'][i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    tweets['text_x_clean'][i] = filtered_sentence\n",
    "    print(tweets['text_x_clean'][i])\n",
    "'''\n",
    "    #################################################################################################################\n",
    "    \n",
    "    tweets['text_y_clean'][i] = tweets['text_y'][i].lower()\n",
    "    \n",
    "    #Conversion of Emojis to Text\n",
    "    tweets['text_y_clean'][i] = convert_emojis_to_word(tweets['text_y_clean'][i])\n",
    "    \n",
    "    #Removal of URLS\n",
    "    pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))';\n",
    "    match = re.findall(pattern, tweets['text_y_clean'][i])\n",
    "    for m in match:\n",
    "        url = m[0]\n",
    "        tweets['text_y_clean'][i] = tweets['text_y_clean'][i].replace(url, '')\n",
    "    \n",
    "    #Removal of HTTPS\n",
    "    tweets['text_x_clean'][i] = remove_URL(tweets['text_x_clean'][i])\n",
    "\n",
    "    #We don't need the following steps of text data preprocessing for deep learning models \n",
    "    '''\n",
    "    #Remove Punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    unpunctuated_text = tokenizer.tokenize(tweets['text_y_clean'][i])\n",
    "    tweets['text_y_clean'][i] = \" \".join(unpunctuated_text)\n",
    "    \n",
    "    #Special Character Replacement\n",
    "    for j in range(len(special_characters)):\n",
    "        tweets['text_y_clean'][i] = tweets['text_y_clean'][i].replace(special_characters[j],' ')\n",
    "    \n",
    "    #Lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = tweets['text_y_clean'][i]\n",
    "    lemma_text = nltk.word_tokenize(text)\n",
    "    tweets['text_y_clean'][i] = \" \".join(lemma_text)\n",
    "    \n",
    "    \n",
    "    #Removing Stop Words\n",
    "    word_tokens = word_tokenize(tweets['text_y_clean'][i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    tweets['text_y_clean'][i] = filtered_sentence\n",
    "    '''\n",
    "    \n",
    "    print(tweets['text_y_clean'][i])\n",
    "\n",
    "    #################################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Steps:\n",
    "#1) Replace customer id with _sn_\n",
    "#2) split into train and test\n",
    "#3)Save to github"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_venv",
   "language": "python",
   "name": "chatbot_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
