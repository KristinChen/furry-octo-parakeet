{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223245c0",
   "metadata": {},
   "source": [
    "# 0: recap of Neural Network\n",
    "- reference: \n",
    "[A Visual and Interactive Guide to the Basics of Neural Networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e6dc3",
   "metadata": {},
   "source": [
    "# Question 1: Why is feed-forward NN usually not used in NLP problem? What is the solution?\n",
    "\n",
    "- Cannot handle sequential data\n",
    "- Considers only the current input\n",
    "- Cannot memorize previous inputs\n",
    "\n",
    "The solution to these issues is using neural networks structured in the way to handle sequential data – such as RNN (Recurrent Nueral Networks).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040ae6a",
   "metadata": {},
   "source": [
    "## Bonus Question:\n",
    "\n",
    "- What is sequential data?\n",
    "- Why do we expect our model to memorize previous inputs in dealing with text data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a3218",
   "metadata": {},
   "source": [
    "# Qustion 2: Before we dive into RNN, what is a seq2seq model?\n",
    "\n",
    "Depending on the output of the sequence model, RNN could be divided into 4 types: \n",
    "    \n",
    "- One to One\n",
    "- One to Many\n",
    "- Many to One\n",
    "- Many to Many\n",
    "\n",
    "Many to many is also call sequence to sequence (seq2seq) model. In other words, a sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.\n",
    "\n",
    "        \n",
    "- refernece 1: [4 types of RNN](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn?source=sl_frs_nav_playlist_video_clicked#types_of_recurrent_neural_networks)\n",
    "       \n",
    "- reference 2: \n",
    "[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "(**Please stop reading the materials when it talks about attention – we will build this section of knowledge later on when it comes to building more complex models!!!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e21693",
   "metadata": {},
   "source": [
    "## Bonus Question:\n",
    "\n",
    "- Among four of RNN, what is the correct structure to solve our chatbot problem? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfa071",
   "metadata": {},
   "source": [
    "# Question 3: Why RNN could handle sequence data? \n",
    "- refernece: [A friendly introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a19ec4",
   "metadata": {},
   "source": [
    "# Question 4: What are the drawbacks of RNN? How does LSTM, the special kind of RNN, handle the drawbacks of RNN? \n",
    "- reference: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f582a",
   "metadata": {},
   "source": [
    "## Bonus Question: how does GRU, another special kind of RNN, handle the drawbacks of RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2145f3",
   "metadata": {},
   "source": [
    "# Question 5: how to implement a simple seq2seq using LSTM in keras? \n",
    "- reference: [How to implement Seq2Seq LSTM Model in Keras](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa222c83",
   "metadata": {},
   "source": [
    "## Bonus question: \n",
    "- Anyone could interpret what's the structure of the model implemented by below code snippet?\n",
    "\n",
    "```python\n",
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    encoder_embedding = embed_layer(encoder_inputs)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    decoder_embedding = embed_layer(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "- Anyone could interpret what's the structure of the model implemented by below code snippet?\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(train_data, epochs=10)\n",
    "```\n",
    "\n",
    "- Hint/Reference: [Keras for Beginners: Implementing a Recurrent Neural Network](https://victorzhou.com/blog/keras-rnn-tutorial/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_test",
   "language": "python",
   "name": "tf_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
